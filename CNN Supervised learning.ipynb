{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment2 3.23.ipynb","provenance":[],"authorship_tag":"ABX9TyP+D4FqJbUM2CmbwEBAQxKq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sFUYhI2Qc28G","colab_type":"text"},"source":["# Keypoint Detection on Human Body Silhouette Images"]},{"cell_type":"markdown","metadata":{"id":"A391aU7WcOuE","colab_type":"text"},"source":["Name: HE XIANTAO\n","\n","Student number: 1155132173"]},{"cell_type":"markdown","metadata":{"id":"oULlzOq_dE12","colab_type":"text"},"source":["# Introduction\n","Detect human body keypoints on silhouette images.\n","\n","Deep learning with PyTorch.\n","\n","Side view detection is demonstrated here.\n","\n","Assignment: front view keypoint detection.\n"]},{"cell_type":"code","metadata":{"id":"n456xgFgZTVD","colab_type":"code","outputId":"b595cc58-b2ef-4eab-fed6-7799578b33dc","executionInfo":{"status":"ok","timestamp":1584941308331,"user_tz":-480,"elapsed":21367,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","ROOT_FOLDER = './gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/'\n","\n","import glob\n","print('\\nContents in the data folder:')\n","for x in glob.glob(ROOT_FOLDER+'data/*'):\n","    print(x)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n","\n","Contents in the data folder:\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/train_kpt_side.npy\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/test_kpt_side.npy\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/train_kpt_front.npy\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/test_img_front.npy\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/test_img_side.npy\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/train_img_front.npy\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/train_img_side.npy\n","./gdrive/My Drive/Colab Notebooks/MAEG5735-2020-Assignment2/data/README.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HSrXt5mCZYDI","colab_type":"code","outputId":"1df68c02-fb47-4737-a08a-8d57b35e4ed4","executionInfo":{"status":"ok","timestamp":1584941316439,"user_tz":-480,"elapsed":2644,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def draw_points(image, kpts):\n","    plt.figure()\n","    plt.imshow(image, cmap='gray')\n","    keypoints = (kpts+0.5)*IMG_SIZE\n","    plt.scatter(keypoints[:, 0], keypoints[:, 1], s=50, marker='.', c='r')\n","\n","# load side view data\n","IMG_SIZE = 200\n","IMG_S_TRAIN = np.load(ROOT_FOLDER+'data/train_img_side.npy')\n","IMG_S_TRAIN = np.unpackbits(IMG_S_TRAIN).reshape((-1,IMG_SIZE,IMG_SIZE))\n","IMG_S_TEST = np.load(ROOT_FOLDER+'data/test_img_side.npy')\n","IMG_S_TEST = np.unpackbits(IMG_S_TEST).reshape((-1,IMG_SIZE,IMG_SIZE))\n","KPT_S_TRAIN = np.load(ROOT_FOLDER+'data/train_kpt_side.npy')/IMG_SIZE - 0.5\n","KPT_S_TEST = np.load(ROOT_FOLDER+'data/test_kpt_side.npy')/IMG_SIZE - 0.5\n","\n","# show one\n","idx = 0\n","draw_points(IMG_S_TRAIN[idx,:,:], KPT_S_TRAIN[idx,:,:])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSgIiKackIJJySgIiKff/AW53RdNnGMEsAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"_bRd2CDKQQVn","colab_type":"text"},"source":["# Dataset loader"]},{"cell_type":"code","metadata":{"id":"-YsK387BZgzx","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class SideKeypointsDataset(Dataset):\n","    '''Side View Keypoints Dataset'''\n","    def __init__(self, img, kpt, train=True, transform=None):\n","        self.img = img\n","        self.kpt = kpt\n","        self.train = train\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return self.img.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        image = self.img[idx,:,:].astype(np.float32)\n","        if self.train:\n","            keypoints = self.kpt[idx,:,:].ravel().astype(np.float32)\n","        else:\n","            keypoints = None\n","        sample = {'image': image, 'keypoints': keypoints}\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvQSTNl2ZoD5","colab_type":"code","colab":{}},"source":["from torch.utils.data.sampler import SubsetRandomSampler\n","\n","def prepare_train_valid_loaders(trainset, valid_size=0.2, \n","                                batch_size=128):\n","    '''\n","    Split trainset data and prepare DataLoader for training and validation\n","    \n","    Args:\n","        trainset (Dataset): data \n","        valid_size (float): validation size, defalut=0.2\n","        batch_size (int) : batch size, default=128\n","    ''' \n","    \n","    # obtain training indices that will be used for validation\n","    num_train = len(trainset)\n","    indices = list(range(num_train))\n","    np.random.shuffle(indices)\n","    split = int(np.floor(valid_size * num_train))\n","    train_idx, valid_idx = indices[split:], indices[:split]\n","    \n","    # define samplers for obtaining training and validation batches\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","    \n","    # prepare data loaders\n","    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                               sampler=train_sampler)\n","    valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                               sampler=valid_sampler)\n","    \n","    return train_loader, valid_loader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQkuhmHIZrmI","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","import cv2\n","\n","class Rescale(object):\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, key_pts = sample['image'], sample['keypoints']\n","        h, w = image.shape[:2]\n","        new_w = np.random.randint(w, self.output_size)\n","        new_h = new_w\n","        new_h, new_w = int(new_h), int(new_w)\n","        img = cv2.resize(image, (new_w, new_h))    \n","        if key_pts is not None:\n","            return {'image': img, 'keypoints': key_pts}\n","        else:\n","            return {'image': img}\n","\n","class RandomCrop(object):\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, key_pts = sample['image'], sample['keypoints']\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","        if h == new_h:\n","            return sample\n","        top = np.random.randint(0, h - new_h)\n","        left = np.random.randint(0, w - new_w)\n","        #left = top # temp\n","        image = image[top: top + new_h,\n","                      left: left + new_w]\n","        if key_pts is not None:\n","            #key_pts = key_pts - [left/output_size, top/output_size]\n","            key_pts[0::2] = ((key_pts[0::2]+0.5)*w-left)/new_w-0.5\n","            key_pts[1::2] = ((key_pts[1::2]+0.5)*h-top)/new_h-0.5\n","            return {'image': image, 'keypoints': key_pts}\n","        else:\n","            return {'image': image}\n","\n","class ToTensor(object):\n","    '''Convert ndarrays in sample to Tensors.'''\n","    def __call__(self, sample):\n","        image, keypoints = sample['image'], sample['keypoints']\n","        # swap color axis because\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.reshape(1, IMG_SIZE, IMG_SIZE)\n","        image = torch.from_numpy(image)\n","        if keypoints is not None:\n","            keypoints = torch.from_numpy(keypoints)\n","            return {'image': image, 'keypoints': keypoints}\n","        else:\n","            return {'image': image}\n","\n","batch_size = 32\n","valid_size = 0.2 # percentage of training set to use as validation\n","\n","# Define a transform to normalize the data\n","tsfm_train = transforms.Compose([Rescale(205), RandomCrop(200), ToTensor()])\n","tsfm_test = transforms.Compose([ToTensor()])\n","\n","# Load the training data and test data\n","trainset = SideKeypointsDataset(IMG_S_TRAIN, KPT_S_TRAIN, transform=tsfm_train)\n","testset = SideKeypointsDataset(IMG_S_TEST, None, train=False, transform=tsfm_test)\n","\n","# prepare data loaders\n","train_loader, valid_loader = prepare_train_valid_loaders(trainset, \n","                                                         valid_size,\n","                                                         batch_size)\n","\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30f1WaeUQX6G","colab_type":"text"},"source":["# Neural network structure"]},{"cell_type":"code","metadata":{"id":"Uk9pbhFpZvkw","colab_type":"code","colab":{}},"source":["from torch import nn, optim\n","import torch.nn.functional as F\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_layers, drop_p =0.5):\n","        '''\n","        Buid a forward network with arbitrary hidden layers.\n","        Arguments\n","            ---------\n","            input_size (integer): size of the input layer\n","            output_size (integer): size of the output layer\n","            hidden_layers (list of integers):, the sizes of each hidden layers\n","        '''\n","        super(MLP, self).__init__()\n","        # hidden layers\n","        layer_sizes = [(input_size, hidden_layers[0])] \\\n","                      + list(zip(hidden_layers[:-1], hidden_layers[1:]))\n","        self.hidden_layers = nn.ModuleList([nn.Linear(h1, h2) \n","                                            for h1, h2 in layer_sizes])\n","        self.output = nn.Linear(hidden_layers[-1], output_size)\n","        self.dropout = nn.Dropout(drop_p)\n","        \n","    def forward(self, x):\n","        ''' Forward pass through the network, returns the output logits '''\n","        # flatten inputs\n","        x = x.view(x.shape[0], -1)\n","        for layer in self.hidden_layers:\n","            x = F.relu(layer(x))\n","            x = self.dropout(x)\n","        x = self.output(x)    \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QML65oRFZyp5","colab_type":"code","colab":{}},"source":["def train(train_loader, valid_loader, model, criterion, optimizer, \n","          n_epochs=50, saved_model='model.pt'):\n","    '''\n","    Train the model\n","    \n","    Args:\n","        train_loader (DataLoader): DataLoader for train Dataset\n","        valid_loader (DataLoader): DataLoader for valid Dataset\n","        model (nn.Module): model to be trained on\n","        criterion (torch.nn): loss funtion\n","        optimizer (torch.optim): optimization algorithms\n","        n_epochs (int): number of epochs to train the model\n","        saved_model (str): file path for saving model\n","    \n","    Return:\n","        tuple of train_losses, valid_losses\n","    '''\n","\n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = np.Inf # set initial \"min\" to infinity\n","\n","    train_losses = []\n","    valid_losses = []\n","\n","    for epoch in range(n_epochs):\n","        # monitor training loss\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train() # prep model for training\n","        for batch in train_loader:\n","            # clear the gradients of all optimized variables\n","            optimizer.zero_grad()\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(batch['image'].to(device))\n","            # calculate the loss\n","            loss = criterion(output, batch['keypoints'].to(device))\n","            # backward pass: compute gradient of the loss with respect to model parameters\n","            loss.backward()\n","            # perform a single optimization step (parameter update)\n","            optimizer.step()\n","            # update running training loss\n","            train_loss += loss.item()*batch['image'].size(0)\n","\n","        ######################    \n","        # validate the model #\n","        ######################\n","        model.eval() # prep model for evaluation\n","        for batch in valid_loader:\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(batch['image'].to(device))\n","            # calculate the loss\n","            loss = criterion(output, batch['keypoints'].to(device))\n","            # update running validation loss \n","            valid_loss += loss.item()*batch['image'].size(0)\n","\n","        # print training/validation statistics \n","        # calculate average Root Mean Square loss over an epoch\n","        train_loss = np.sqrt(train_loss/len(train_loader.sampler.indices))\n","        valid_loss = np.sqrt(valid_loss/len(valid_loader.sampler.indices))\n","\n","        train_losses.append(train_loss)\n","        valid_losses.append(valid_loss)\n","\n","        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'\n","              .format(epoch+1, train_loss, valid_loss))\n","\n","        # save model if validation loss has decreased\n","        if valid_loss <= valid_loss_min:\n","            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n","                  .format(valid_loss_min, valid_loss))\n","            torch.save(model.state_dict(), saved_model)\n","            valid_loss_min = valid_loss\n","            \n","    return train_losses, valid_losses "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZWQ7LHRNQedJ","colab_type":"text"},"source":["# Criterion and Optimizer\n","I change learning rate to 0.0001 and set n_epoches to 500 to train MLP"]},{"cell_type":"code","metadata":{"id":"e0_JoPxsZ5bO","colab_type":"code","outputId":"449c2d92-297f-4c6b-f192-3ea29df1735a","executionInfo":{"status":"ok","timestamp":1584941644471,"user_tz":-480,"elapsed":166270,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from torch import optim\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# output_size ?\n","# Side view keypoints: 6x2 = 12\n","\n","model = MLP(input_size=IMG_SIZE*IMG_SIZE, output_size=12, \n","            hidden_layers=[128, 64], drop_p=0.1)\n","model = model.to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","train_losses, valid_losses = train(train_loader, valid_loader,\n","                                   model, criterion, optimizer,\n","                                   n_epochs=500,\n","                                   saved_model=ROOT_FOLDER+'model.pt')                          "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.087811 \tValidation Loss: 0.049523\n","Validation loss decreased (inf --> 0.049523).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.063000 \tValidation Loss: 0.046372\n","Validation loss decreased (0.049523 --> 0.046372).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.057992 \tValidation Loss: 0.039217\n","Validation loss decreased (0.046372 --> 0.039217).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.054298 \tValidation Loss: 0.040090\n","Epoch: 5 \tTraining Loss: 0.050316 \tValidation Loss: 0.039432\n","Epoch: 6 \tTraining Loss: 0.048721 \tValidation Loss: 0.037670\n","Validation loss decreased (0.039217 --> 0.037670).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.047248 \tValidation Loss: 0.035284\n","Validation loss decreased (0.037670 --> 0.035284).  Saving model ...\n","Epoch: 8 \tTraining Loss: 0.044964 \tValidation Loss: 0.037337\n","Epoch: 9 \tTraining Loss: 0.043108 \tValidation Loss: 0.035208\n","Validation loss decreased (0.035284 --> 0.035208).  Saving model ...\n","Epoch: 10 \tTraining Loss: 0.042503 \tValidation Loss: 0.035264\n","Epoch: 11 \tTraining Loss: 0.040780 \tValidation Loss: 0.033769\n","Validation loss decreased (0.035208 --> 0.033769).  Saving model ...\n","Epoch: 12 \tTraining Loss: 0.040662 \tValidation Loss: 0.033973\n","Epoch: 13 \tTraining Loss: 0.039729 \tValidation Loss: 0.034794\n","Epoch: 14 \tTraining Loss: 0.039020 \tValidation Loss: 0.033669\n","Validation loss decreased (0.033769 --> 0.033669).  Saving model ...\n","Epoch: 15 \tTraining Loss: 0.038167 \tValidation Loss: 0.033802\n","Epoch: 16 \tTraining Loss: 0.038797 \tValidation Loss: 0.033459\n","Validation loss decreased (0.033669 --> 0.033459).  Saving model ...\n","Epoch: 17 \tTraining Loss: 0.037163 \tValidation Loss: 0.035243\n","Epoch: 18 \tTraining Loss: 0.037050 \tValidation Loss: 0.033410\n","Validation loss decreased (0.033459 --> 0.033410).  Saving model ...\n","Epoch: 19 \tTraining Loss: 0.035986 \tValidation Loss: 0.035290\n","Epoch: 20 \tTraining Loss: 0.035559 \tValidation Loss: 0.034072\n","Epoch: 21 \tTraining Loss: 0.035183 \tValidation Loss: 0.033131\n","Validation loss decreased (0.033410 --> 0.033131).  Saving model ...\n","Epoch: 22 \tTraining Loss: 0.034887 \tValidation Loss: 0.033690\n","Epoch: 23 \tTraining Loss: 0.034644 \tValidation Loss: 0.034380\n","Epoch: 24 \tTraining Loss: 0.033505 \tValidation Loss: 0.033244\n","Epoch: 25 \tTraining Loss: 0.034319 \tValidation Loss: 0.033115\n","Validation loss decreased (0.033131 --> 0.033115).  Saving model ...\n","Epoch: 26 \tTraining Loss: 0.033812 \tValidation Loss: 0.034047\n","Epoch: 27 \tTraining Loss: 0.032920 \tValidation Loss: 0.035756\n","Epoch: 28 \tTraining Loss: 0.034086 \tValidation Loss: 0.032855\n","Validation loss decreased (0.033115 --> 0.032855).  Saving model ...\n","Epoch: 29 \tTraining Loss: 0.032925 \tValidation Loss: 0.032968\n","Epoch: 30 \tTraining Loss: 0.033433 \tValidation Loss: 0.034366\n","Epoch: 31 \tTraining Loss: 0.033253 \tValidation Loss: 0.033120\n","Epoch: 32 \tTraining Loss: 0.033555 \tValidation Loss: 0.034898\n","Epoch: 33 \tTraining Loss: 0.033537 \tValidation Loss: 0.035592\n","Epoch: 34 \tTraining Loss: 0.032502 \tValidation Loss: 0.033613\n","Epoch: 35 \tTraining Loss: 0.032071 \tValidation Loss: 0.033698\n","Epoch: 36 \tTraining Loss: 0.031360 \tValidation Loss: 0.034768\n","Epoch: 37 \tTraining Loss: 0.031316 \tValidation Loss: 0.033459\n","Epoch: 38 \tTraining Loss: 0.032717 \tValidation Loss: 0.034000\n","Epoch: 39 \tTraining Loss: 0.032067 \tValidation Loss: 0.035659\n","Epoch: 40 \tTraining Loss: 0.030678 \tValidation Loss: 0.032494\n","Validation loss decreased (0.032855 --> 0.032494).  Saving model ...\n","Epoch: 41 \tTraining Loss: 0.029876 \tValidation Loss: 0.033841\n","Epoch: 42 \tTraining Loss: 0.030977 \tValidation Loss: 0.033295\n","Epoch: 43 \tTraining Loss: 0.029700 \tValidation Loss: 0.032955\n","Epoch: 44 \tTraining Loss: 0.029180 \tValidation Loss: 0.032930\n","Epoch: 45 \tTraining Loss: 0.029549 \tValidation Loss: 0.032833\n","Epoch: 46 \tTraining Loss: 0.028861 \tValidation Loss: 0.032737\n","Epoch: 47 \tTraining Loss: 0.028433 \tValidation Loss: 0.033238\n","Epoch: 48 \tTraining Loss: 0.028543 \tValidation Loss: 0.033561\n","Epoch: 49 \tTraining Loss: 0.028461 \tValidation Loss: 0.033550\n","Epoch: 50 \tTraining Loss: 0.029265 \tValidation Loss: 0.032688\n","Epoch: 51 \tTraining Loss: 0.029103 \tValidation Loss: 0.033483\n","Epoch: 52 \tTraining Loss: 0.028558 \tValidation Loss: 0.032822\n","Epoch: 53 \tTraining Loss: 0.028123 \tValidation Loss: 0.033077\n","Epoch: 54 \tTraining Loss: 0.028863 \tValidation Loss: 0.032805\n","Epoch: 55 \tTraining Loss: 0.027778 \tValidation Loss: 0.036070\n","Epoch: 56 \tTraining Loss: 0.028760 \tValidation Loss: 0.034072\n","Epoch: 57 \tTraining Loss: 0.027116 \tValidation Loss: 0.033336\n","Epoch: 58 \tTraining Loss: 0.027248 \tValidation Loss: 0.033289\n","Epoch: 59 \tTraining Loss: 0.027915 \tValidation Loss: 0.032946\n","Epoch: 60 \tTraining Loss: 0.027635 \tValidation Loss: 0.033235\n","Epoch: 61 \tTraining Loss: 0.027434 \tValidation Loss: 0.033339\n","Epoch: 62 \tTraining Loss: 0.027319 \tValidation Loss: 0.034379\n","Epoch: 63 \tTraining Loss: 0.026464 \tValidation Loss: 0.033225\n","Epoch: 64 \tTraining Loss: 0.026522 \tValidation Loss: 0.032970\n","Epoch: 65 \tTraining Loss: 0.028255 \tValidation Loss: 0.033566\n","Epoch: 66 \tTraining Loss: 0.027688 \tValidation Loss: 0.032195\n","Validation loss decreased (0.032494 --> 0.032195).  Saving model ...\n","Epoch: 67 \tTraining Loss: 0.025883 \tValidation Loss: 0.032666\n","Epoch: 68 \tTraining Loss: 0.026495 \tValidation Loss: 0.032976\n","Epoch: 69 \tTraining Loss: 0.025367 \tValidation Loss: 0.032592\n","Epoch: 70 \tTraining Loss: 0.025336 \tValidation Loss: 0.032558\n","Epoch: 71 \tTraining Loss: 0.025951 \tValidation Loss: 0.031832\n","Validation loss decreased (0.032195 --> 0.031832).  Saving model ...\n","Epoch: 72 \tTraining Loss: 0.026125 \tValidation Loss: 0.032836\n","Epoch: 73 \tTraining Loss: 0.025504 \tValidation Loss: 0.032205\n","Epoch: 74 \tTraining Loss: 0.024740 \tValidation Loss: 0.032022\n","Epoch: 75 \tTraining Loss: 0.024173 \tValidation Loss: 0.032690\n","Epoch: 76 \tTraining Loss: 0.025255 \tValidation Loss: 0.034228\n","Epoch: 77 \tTraining Loss: 0.025986 \tValidation Loss: 0.032263\n","Epoch: 78 \tTraining Loss: 0.025316 \tValidation Loss: 0.032079\n","Epoch: 79 \tTraining Loss: 0.025360 \tValidation Loss: 0.034339\n","Epoch: 80 \tTraining Loss: 0.025523 \tValidation Loss: 0.033537\n","Epoch: 81 \tTraining Loss: 0.024808 \tValidation Loss: 0.032881\n","Epoch: 82 \tTraining Loss: 0.024269 \tValidation Loss: 0.032163\n","Epoch: 83 \tTraining Loss: 0.023964 \tValidation Loss: 0.032286\n","Epoch: 84 \tTraining Loss: 0.023786 \tValidation Loss: 0.033057\n","Epoch: 85 \tTraining Loss: 0.023790 \tValidation Loss: 0.032243\n","Epoch: 86 \tTraining Loss: 0.024282 \tValidation Loss: 0.032430\n","Epoch: 87 \tTraining Loss: 0.023340 \tValidation Loss: 0.031960\n","Epoch: 88 \tTraining Loss: 0.023915 \tValidation Loss: 0.032623\n","Epoch: 89 \tTraining Loss: 0.023427 \tValidation Loss: 0.032453\n","Epoch: 90 \tTraining Loss: 0.023901 \tValidation Loss: 0.031791\n","Validation loss decreased (0.031832 --> 0.031791).  Saving model ...\n","Epoch: 91 \tTraining Loss: 0.023518 \tValidation Loss: 0.031983\n","Epoch: 92 \tTraining Loss: 0.023337 \tValidation Loss: 0.032957\n","Epoch: 93 \tTraining Loss: 0.023831 \tValidation Loss: 0.032683\n","Epoch: 94 \tTraining Loss: 0.023417 \tValidation Loss: 0.032218\n","Epoch: 95 \tTraining Loss: 0.023168 \tValidation Loss: 0.032509\n","Epoch: 96 \tTraining Loss: 0.023011 \tValidation Loss: 0.032232\n","Epoch: 97 \tTraining Loss: 0.022541 \tValidation Loss: 0.031938\n","Epoch: 98 \tTraining Loss: 0.022842 \tValidation Loss: 0.032323\n","Epoch: 99 \tTraining Loss: 0.022954 \tValidation Loss: 0.032653\n","Epoch: 100 \tTraining Loss: 0.022619 \tValidation Loss: 0.032752\n","Epoch: 101 \tTraining Loss: 0.022758 \tValidation Loss: 0.032011\n","Epoch: 102 \tTraining Loss: 0.022537 \tValidation Loss: 0.032449\n","Epoch: 103 \tTraining Loss: 0.022143 \tValidation Loss: 0.032343\n","Epoch: 104 \tTraining Loss: 0.022247 \tValidation Loss: 0.032021\n","Epoch: 105 \tTraining Loss: 0.022725 \tValidation Loss: 0.032760\n","Epoch: 106 \tTraining Loss: 0.022263 \tValidation Loss: 0.032704\n","Epoch: 107 \tTraining Loss: 0.022220 \tValidation Loss: 0.032523\n","Epoch: 108 \tTraining Loss: 0.022107 \tValidation Loss: 0.032369\n","Epoch: 109 \tTraining Loss: 0.022076 \tValidation Loss: 0.032397\n","Epoch: 110 \tTraining Loss: 0.022112 \tValidation Loss: 0.031919\n","Epoch: 111 \tTraining Loss: 0.021966 \tValidation Loss: 0.031813\n","Epoch: 112 \tTraining Loss: 0.021668 \tValidation Loss: 0.032278\n","Epoch: 113 \tTraining Loss: 0.021455 \tValidation Loss: 0.032193\n","Epoch: 114 \tTraining Loss: 0.021369 \tValidation Loss: 0.031738\n","Validation loss decreased (0.031791 --> 0.031738).  Saving model ...\n","Epoch: 115 \tTraining Loss: 0.021789 \tValidation Loss: 0.032193\n","Epoch: 116 \tTraining Loss: 0.021762 \tValidation Loss: 0.032629\n","Epoch: 117 \tTraining Loss: 0.021815 \tValidation Loss: 0.031544\n","Validation loss decreased (0.031738 --> 0.031544).  Saving model ...\n","Epoch: 118 \tTraining Loss: 0.021203 \tValidation Loss: 0.031420\n","Validation loss decreased (0.031544 --> 0.031420).  Saving model ...\n","Epoch: 119 \tTraining Loss: 0.021181 \tValidation Loss: 0.031620\n","Epoch: 120 \tTraining Loss: 0.021462 \tValidation Loss: 0.031758\n","Epoch: 121 \tTraining Loss: 0.021108 \tValidation Loss: 0.032182\n","Epoch: 122 \tTraining Loss: 0.021231 \tValidation Loss: 0.031554\n","Epoch: 123 \tTraining Loss: 0.021156 \tValidation Loss: 0.031700\n","Epoch: 124 \tTraining Loss: 0.022036 \tValidation Loss: 0.032197\n","Epoch: 125 \tTraining Loss: 0.021699 \tValidation Loss: 0.032690\n","Epoch: 126 \tTraining Loss: 0.021233 \tValidation Loss: 0.031643\n","Epoch: 127 \tTraining Loss: 0.020542 \tValidation Loss: 0.031319\n","Validation loss decreased (0.031420 --> 0.031319).  Saving model ...\n","Epoch: 128 \tTraining Loss: 0.021044 \tValidation Loss: 0.032489\n","Epoch: 129 \tTraining Loss: 0.020419 \tValidation Loss: 0.032388\n","Epoch: 130 \tTraining Loss: 0.020802 \tValidation Loss: 0.031818\n","Epoch: 131 \tTraining Loss: 0.020161 \tValidation Loss: 0.032130\n","Epoch: 132 \tTraining Loss: 0.020108 \tValidation Loss: 0.032174\n","Epoch: 133 \tTraining Loss: 0.020407 \tValidation Loss: 0.032113\n","Epoch: 134 \tTraining Loss: 0.020886 \tValidation Loss: 0.031902\n","Epoch: 135 \tTraining Loss: 0.020439 \tValidation Loss: 0.032570\n","Epoch: 136 \tTraining Loss: 0.020210 \tValidation Loss: 0.032124\n","Epoch: 137 \tTraining Loss: 0.019666 \tValidation Loss: 0.032077\n","Epoch: 138 \tTraining Loss: 0.020411 \tValidation Loss: 0.031883\n","Epoch: 139 \tTraining Loss: 0.019918 \tValidation Loss: 0.031818\n","Epoch: 140 \tTraining Loss: 0.020003 \tValidation Loss: 0.031454\n","Epoch: 141 \tTraining Loss: 0.020837 \tValidation Loss: 0.032272\n","Epoch: 142 \tTraining Loss: 0.020773 \tValidation Loss: 0.031613\n","Epoch: 143 \tTraining Loss: 0.020038 \tValidation Loss: 0.031836\n","Epoch: 144 \tTraining Loss: 0.019628 \tValidation Loss: 0.031880\n","Epoch: 145 \tTraining Loss: 0.020143 \tValidation Loss: 0.031390\n","Epoch: 146 \tTraining Loss: 0.020400 \tValidation Loss: 0.032451\n","Epoch: 147 \tTraining Loss: 0.020586 \tValidation Loss: 0.031881\n","Epoch: 148 \tTraining Loss: 0.020093 \tValidation Loss: 0.032266\n","Epoch: 149 \tTraining Loss: 0.020693 \tValidation Loss: 0.032346\n","Epoch: 150 \tTraining Loss: 0.020672 \tValidation Loss: 0.031648\n","Epoch: 151 \tTraining Loss: 0.019825 \tValidation Loss: 0.032216\n","Epoch: 152 \tTraining Loss: 0.019955 \tValidation Loss: 0.032152\n","Epoch: 153 \tTraining Loss: 0.019728 \tValidation Loss: 0.031906\n","Epoch: 154 \tTraining Loss: 0.019799 \tValidation Loss: 0.031368\n","Epoch: 155 \tTraining Loss: 0.019656 \tValidation Loss: 0.031793\n","Epoch: 156 \tTraining Loss: 0.019724 \tValidation Loss: 0.031352\n","Epoch: 157 \tTraining Loss: 0.019871 \tValidation Loss: 0.031857\n","Epoch: 158 \tTraining Loss: 0.019954 \tValidation Loss: 0.032394\n","Epoch: 159 \tTraining Loss: 0.019699 \tValidation Loss: 0.031918\n","Epoch: 160 \tTraining Loss: 0.018937 \tValidation Loss: 0.031718\n","Epoch: 161 \tTraining Loss: 0.018801 \tValidation Loss: 0.031854\n","Epoch: 162 \tTraining Loss: 0.019982 \tValidation Loss: 0.032384\n","Epoch: 163 \tTraining Loss: 0.019618 \tValidation Loss: 0.031945\n","Epoch: 164 \tTraining Loss: 0.019771 \tValidation Loss: 0.031978\n","Epoch: 165 \tTraining Loss: 0.019232 \tValidation Loss: 0.032014\n","Epoch: 166 \tTraining Loss: 0.019729 \tValidation Loss: 0.031743\n","Epoch: 167 \tTraining Loss: 0.019942 \tValidation Loss: 0.032406\n","Epoch: 168 \tTraining Loss: 0.019436 \tValidation Loss: 0.032231\n","Epoch: 169 \tTraining Loss: 0.019163 \tValidation Loss: 0.031778\n","Epoch: 170 \tTraining Loss: 0.019246 \tValidation Loss: 0.031660\n","Epoch: 171 \tTraining Loss: 0.019161 \tValidation Loss: 0.032506\n","Epoch: 172 \tTraining Loss: 0.019108 \tValidation Loss: 0.032157\n","Epoch: 173 \tTraining Loss: 0.019148 \tValidation Loss: 0.031230\n","Validation loss decreased (0.031319 --> 0.031230).  Saving model ...\n","Epoch: 174 \tTraining Loss: 0.018984 \tValidation Loss: 0.032347\n","Epoch: 175 \tTraining Loss: 0.019181 \tValidation Loss: 0.031620\n","Epoch: 176 \tTraining Loss: 0.019180 \tValidation Loss: 0.031755\n","Epoch: 177 \tTraining Loss: 0.019725 \tValidation Loss: 0.031774\n","Epoch: 178 \tTraining Loss: 0.019130 \tValidation Loss: 0.031405\n","Epoch: 179 \tTraining Loss: 0.018759 \tValidation Loss: 0.031785\n","Epoch: 180 \tTraining Loss: 0.018937 \tValidation Loss: 0.032099\n","Epoch: 181 \tTraining Loss: 0.018508 \tValidation Loss: 0.031713\n","Epoch: 182 \tTraining Loss: 0.018853 \tValidation Loss: 0.032262\n","Epoch: 183 \tTraining Loss: 0.018783 \tValidation Loss: 0.032028\n","Epoch: 184 \tTraining Loss: 0.019164 \tValidation Loss: 0.032044\n","Epoch: 185 \tTraining Loss: 0.019241 \tValidation Loss: 0.031913\n","Epoch: 186 \tTraining Loss: 0.019064 \tValidation Loss: 0.031829\n","Epoch: 187 \tTraining Loss: 0.018777 \tValidation Loss: 0.032659\n","Epoch: 188 \tTraining Loss: 0.018366 \tValidation Loss: 0.031860\n","Epoch: 189 \tTraining Loss: 0.018571 \tValidation Loss: 0.032541\n","Epoch: 190 \tTraining Loss: 0.018744 \tValidation Loss: 0.031457\n","Epoch: 191 \tTraining Loss: 0.018227 \tValidation Loss: 0.031767\n","Epoch: 192 \tTraining Loss: 0.018622 \tValidation Loss: 0.031798\n","Epoch: 193 \tTraining Loss: 0.018511 \tValidation Loss: 0.031962\n","Epoch: 194 \tTraining Loss: 0.018596 \tValidation Loss: 0.031180\n","Validation loss decreased (0.031230 --> 0.031180).  Saving model ...\n","Epoch: 195 \tTraining Loss: 0.018874 \tValidation Loss: 0.032494\n","Epoch: 196 \tTraining Loss: 0.018136 \tValidation Loss: 0.031841\n","Epoch: 197 \tTraining Loss: 0.018124 \tValidation Loss: 0.031984\n","Epoch: 198 \tTraining Loss: 0.019066 \tValidation Loss: 0.032007\n","Epoch: 199 \tTraining Loss: 0.018596 \tValidation Loss: 0.032931\n","Epoch: 200 \tTraining Loss: 0.018720 \tValidation Loss: 0.032120\n","Epoch: 201 \tTraining Loss: 0.018862 \tValidation Loss: 0.031879\n","Epoch: 202 \tTraining Loss: 0.018047 \tValidation Loss: 0.031770\n","Epoch: 203 \tTraining Loss: 0.018704 \tValidation Loss: 0.031795\n","Epoch: 204 \tTraining Loss: 0.018262 \tValidation Loss: 0.031994\n","Epoch: 205 \tTraining Loss: 0.018195 \tValidation Loss: 0.031652\n","Epoch: 206 \tTraining Loss: 0.018846 \tValidation Loss: 0.032068\n","Epoch: 207 \tTraining Loss: 0.018843 \tValidation Loss: 0.032463\n","Epoch: 208 \tTraining Loss: 0.018329 \tValidation Loss: 0.031892\n","Epoch: 209 \tTraining Loss: 0.017537 \tValidation Loss: 0.031831\n","Epoch: 210 \tTraining Loss: 0.017984 \tValidation Loss: 0.032310\n","Epoch: 211 \tTraining Loss: 0.018522 \tValidation Loss: 0.032384\n","Epoch: 212 \tTraining Loss: 0.018103 \tValidation Loss: 0.032345\n","Epoch: 213 \tTraining Loss: 0.018478 \tValidation Loss: 0.031696\n","Epoch: 214 \tTraining Loss: 0.018439 \tValidation Loss: 0.032140\n","Epoch: 215 \tTraining Loss: 0.018257 \tValidation Loss: 0.032069\n","Epoch: 216 \tTraining Loss: 0.018375 \tValidation Loss: 0.031752\n","Epoch: 217 \tTraining Loss: 0.018237 \tValidation Loss: 0.031920\n","Epoch: 218 \tTraining Loss: 0.017938 \tValidation Loss: 0.032339\n","Epoch: 219 \tTraining Loss: 0.019024 \tValidation Loss: 0.032104\n","Epoch: 220 \tTraining Loss: 0.018862 \tValidation Loss: 0.031729\n","Epoch: 221 \tTraining Loss: 0.018343 \tValidation Loss: 0.031739\n","Epoch: 222 \tTraining Loss: 0.017698 \tValidation Loss: 0.032394\n","Epoch: 223 \tTraining Loss: 0.018308 \tValidation Loss: 0.032281\n","Epoch: 224 \tTraining Loss: 0.017943 \tValidation Loss: 0.032405\n","Epoch: 225 \tTraining Loss: 0.017451 \tValidation Loss: 0.031761\n","Epoch: 226 \tTraining Loss: 0.017430 \tValidation Loss: 0.032029\n","Epoch: 227 \tTraining Loss: 0.017544 \tValidation Loss: 0.031748\n","Epoch: 228 \tTraining Loss: 0.018027 \tValidation Loss: 0.031857\n","Epoch: 229 \tTraining Loss: 0.017770 \tValidation Loss: 0.032909\n","Epoch: 230 \tTraining Loss: 0.018126 \tValidation Loss: 0.032339\n","Epoch: 231 \tTraining Loss: 0.017958 \tValidation Loss: 0.032288\n","Epoch: 232 \tTraining Loss: 0.017566 \tValidation Loss: 0.031957\n","Epoch: 233 \tTraining Loss: 0.017659 \tValidation Loss: 0.032134\n","Epoch: 234 \tTraining Loss: 0.017952 \tValidation Loss: 0.032421\n","Epoch: 235 \tTraining Loss: 0.018306 \tValidation Loss: 0.032145\n","Epoch: 236 \tTraining Loss: 0.017470 \tValidation Loss: 0.031510\n","Epoch: 237 \tTraining Loss: 0.017593 \tValidation Loss: 0.031776\n","Epoch: 238 \tTraining Loss: 0.017510 \tValidation Loss: 0.031813\n","Epoch: 239 \tTraining Loss: 0.017477 \tValidation Loss: 0.031913\n","Epoch: 240 \tTraining Loss: 0.017528 \tValidation Loss: 0.031804\n","Epoch: 241 \tTraining Loss: 0.017642 \tValidation Loss: 0.032146\n","Epoch: 242 \tTraining Loss: 0.017932 \tValidation Loss: 0.032426\n","Epoch: 243 \tTraining Loss: 0.017452 \tValidation Loss: 0.031775\n","Epoch: 244 \tTraining Loss: 0.016859 \tValidation Loss: 0.032394\n","Epoch: 245 \tTraining Loss: 0.017009 \tValidation Loss: 0.032165\n","Epoch: 246 \tTraining Loss: 0.017395 \tValidation Loss: 0.032124\n","Epoch: 247 \tTraining Loss: 0.017275 \tValidation Loss: 0.032255\n","Epoch: 248 \tTraining Loss: 0.017197 \tValidation Loss: 0.032009\n","Epoch: 249 \tTraining Loss: 0.017336 \tValidation Loss: 0.032230\n","Epoch: 250 \tTraining Loss: 0.017816 \tValidation Loss: 0.032145\n","Epoch: 251 \tTraining Loss: 0.017468 \tValidation Loss: 0.032107\n","Epoch: 252 \tTraining Loss: 0.017438 \tValidation Loss: 0.031721\n","Epoch: 253 \tTraining Loss: 0.016814 \tValidation Loss: 0.032404\n","Epoch: 254 \tTraining Loss: 0.017093 \tValidation Loss: 0.031849\n","Epoch: 255 \tTraining Loss: 0.016496 \tValidation Loss: 0.031443\n","Epoch: 256 \tTraining Loss: 0.017083 \tValidation Loss: 0.031348\n","Epoch: 257 \tTraining Loss: 0.017377 \tValidation Loss: 0.031709\n","Epoch: 258 \tTraining Loss: 0.017346 \tValidation Loss: 0.031671\n","Epoch: 259 \tTraining Loss: 0.017363 \tValidation Loss: 0.032045\n","Epoch: 260 \tTraining Loss: 0.017138 \tValidation Loss: 0.031648\n","Epoch: 261 \tTraining Loss: 0.016929 \tValidation Loss: 0.031763\n","Epoch: 262 \tTraining Loss: 0.017504 \tValidation Loss: 0.032036\n","Epoch: 263 \tTraining Loss: 0.016994 \tValidation Loss: 0.032125\n","Epoch: 264 \tTraining Loss: 0.017687 \tValidation Loss: 0.032394\n","Epoch: 265 \tTraining Loss: 0.017208 \tValidation Loss: 0.031431\n","Epoch: 266 \tTraining Loss: 0.017387 \tValidation Loss: 0.032120\n","Epoch: 267 \tTraining Loss: 0.016712 \tValidation Loss: 0.031821\n","Epoch: 268 \tTraining Loss: 0.017121 \tValidation Loss: 0.032334\n","Epoch: 269 \tTraining Loss: 0.017038 \tValidation Loss: 0.031951\n","Epoch: 270 \tTraining Loss: 0.017158 \tValidation Loss: 0.032207\n","Epoch: 271 \tTraining Loss: 0.017257 \tValidation Loss: 0.032078\n","Epoch: 272 \tTraining Loss: 0.016697 \tValidation Loss: 0.032174\n","Epoch: 273 \tTraining Loss: 0.016942 \tValidation Loss: 0.031970\n","Epoch: 274 \tTraining Loss: 0.016621 \tValidation Loss: 0.031447\n","Epoch: 275 \tTraining Loss: 0.017223 \tValidation Loss: 0.032019\n","Epoch: 276 \tTraining Loss: 0.016742 \tValidation Loss: 0.032375\n","Epoch: 277 \tTraining Loss: 0.017509 \tValidation Loss: 0.031673\n","Epoch: 278 \tTraining Loss: 0.017642 \tValidation Loss: 0.032478\n","Epoch: 279 \tTraining Loss: 0.017009 \tValidation Loss: 0.032226\n","Epoch: 280 \tTraining Loss: 0.016723 \tValidation Loss: 0.032270\n","Epoch: 281 \tTraining Loss: 0.016553 \tValidation Loss: 0.031530\n","Epoch: 282 \tTraining Loss: 0.016316 \tValidation Loss: 0.032395\n","Epoch: 283 \tTraining Loss: 0.016755 \tValidation Loss: 0.031602\n","Epoch: 284 \tTraining Loss: 0.017106 \tValidation Loss: 0.032580\n","Epoch: 285 \tTraining Loss: 0.016817 \tValidation Loss: 0.031807\n","Epoch: 286 \tTraining Loss: 0.016112 \tValidation Loss: 0.031922\n","Epoch: 287 \tTraining Loss: 0.016454 \tValidation Loss: 0.032176\n","Epoch: 288 \tTraining Loss: 0.016705 \tValidation Loss: 0.032375\n","Epoch: 289 \tTraining Loss: 0.016517 \tValidation Loss: 0.031249\n","Epoch: 290 \tTraining Loss: 0.016784 \tValidation Loss: 0.031748\n","Epoch: 291 \tTraining Loss: 0.016577 \tValidation Loss: 0.032133\n","Epoch: 292 \tTraining Loss: 0.016564 \tValidation Loss: 0.031673\n","Epoch: 293 \tTraining Loss: 0.016199 \tValidation Loss: 0.031997\n","Epoch: 294 \tTraining Loss: 0.016496 \tValidation Loss: 0.032048\n","Epoch: 295 \tTraining Loss: 0.017042 \tValidation Loss: 0.031942\n","Epoch: 296 \tTraining Loss: 0.017079 \tValidation Loss: 0.032035\n","Epoch: 297 \tTraining Loss: 0.016616 \tValidation Loss: 0.032079\n","Epoch: 298 \tTraining Loss: 0.016213 \tValidation Loss: 0.031627\n","Epoch: 299 \tTraining Loss: 0.016558 \tValidation Loss: 0.031961\n","Epoch: 300 \tTraining Loss: 0.016324 \tValidation Loss: 0.031904\n","Epoch: 301 \tTraining Loss: 0.015995 \tValidation Loss: 0.032163\n","Epoch: 302 \tTraining Loss: 0.016126 \tValidation Loss: 0.032035\n","Epoch: 303 \tTraining Loss: 0.016253 \tValidation Loss: 0.032009\n","Epoch: 304 \tTraining Loss: 0.016030 \tValidation Loss: 0.031761\n","Epoch: 305 \tTraining Loss: 0.015811 \tValidation Loss: 0.031990\n","Epoch: 306 \tTraining Loss: 0.016529 \tValidation Loss: 0.031819\n","Epoch: 307 \tTraining Loss: 0.015989 \tValidation Loss: 0.031848\n","Epoch: 308 \tTraining Loss: 0.015858 \tValidation Loss: 0.031756\n","Epoch: 309 \tTraining Loss: 0.016820 \tValidation Loss: 0.032105\n","Epoch: 310 \tTraining Loss: 0.016419 \tValidation Loss: 0.031669\n","Epoch: 311 \tTraining Loss: 0.015641 \tValidation Loss: 0.032241\n","Epoch: 312 \tTraining Loss: 0.016159 \tValidation Loss: 0.031748\n","Epoch: 313 \tTraining Loss: 0.016444 \tValidation Loss: 0.031799\n","Epoch: 314 \tTraining Loss: 0.016646 \tValidation Loss: 0.031598\n","Epoch: 315 \tTraining Loss: 0.016256 \tValidation Loss: 0.032020\n","Epoch: 316 \tTraining Loss: 0.016341 \tValidation Loss: 0.031762\n","Epoch: 317 \tTraining Loss: 0.016623 \tValidation Loss: 0.031868\n","Epoch: 318 \tTraining Loss: 0.016405 \tValidation Loss: 0.031534\n","Epoch: 319 \tTraining Loss: 0.016150 \tValidation Loss: 0.031946\n","Epoch: 320 \tTraining Loss: 0.016058 \tValidation Loss: 0.031769\n","Epoch: 321 \tTraining Loss: 0.015853 \tValidation Loss: 0.032160\n","Epoch: 322 \tTraining Loss: 0.016045 \tValidation Loss: 0.031875\n","Epoch: 323 \tTraining Loss: 0.016025 \tValidation Loss: 0.032028\n","Epoch: 324 \tTraining Loss: 0.016490 \tValidation Loss: 0.032887\n","Epoch: 325 \tTraining Loss: 0.016687 \tValidation Loss: 0.032190\n","Epoch: 326 \tTraining Loss: 0.016580 \tValidation Loss: 0.031736\n","Epoch: 327 \tTraining Loss: 0.016307 \tValidation Loss: 0.032337\n","Epoch: 328 \tTraining Loss: 0.015800 \tValidation Loss: 0.031617\n","Epoch: 329 \tTraining Loss: 0.016018 \tValidation Loss: 0.031421\n","Epoch: 330 \tTraining Loss: 0.016118 \tValidation Loss: 0.031619\n","Epoch: 331 \tTraining Loss: 0.016681 \tValidation Loss: 0.032000\n","Epoch: 332 \tTraining Loss: 0.016643 \tValidation Loss: 0.032666\n","Epoch: 333 \tTraining Loss: 0.016115 \tValidation Loss: 0.031600\n","Epoch: 334 \tTraining Loss: 0.015836 \tValidation Loss: 0.032103\n","Epoch: 335 \tTraining Loss: 0.016558 \tValidation Loss: 0.031967\n","Epoch: 336 \tTraining Loss: 0.015995 \tValidation Loss: 0.031882\n","Epoch: 337 \tTraining Loss: 0.015757 \tValidation Loss: 0.031945\n","Epoch: 338 \tTraining Loss: 0.016560 \tValidation Loss: 0.032020\n","Epoch: 339 \tTraining Loss: 0.015618 \tValidation Loss: 0.031046\n","Validation loss decreased (0.031180 --> 0.031046).  Saving model ...\n","Epoch: 340 \tTraining Loss: 0.015828 \tValidation Loss: 0.031888\n","Epoch: 341 \tTraining Loss: 0.016068 \tValidation Loss: 0.031935\n","Epoch: 342 \tTraining Loss: 0.015840 \tValidation Loss: 0.031699\n","Epoch: 343 \tTraining Loss: 0.015700 \tValidation Loss: 0.032142\n","Epoch: 344 \tTraining Loss: 0.015732 \tValidation Loss: 0.031711\n","Epoch: 345 \tTraining Loss: 0.015336 \tValidation Loss: 0.032152\n","Epoch: 346 \tTraining Loss: 0.015844 \tValidation Loss: 0.032025\n","Epoch: 347 \tTraining Loss: 0.015431 \tValidation Loss: 0.032265\n","Epoch: 348 \tTraining Loss: 0.015517 \tValidation Loss: 0.032138\n","Epoch: 349 \tTraining Loss: 0.015557 \tValidation Loss: 0.032136\n","Epoch: 350 \tTraining Loss: 0.015088 \tValidation Loss: 0.031931\n","Epoch: 351 \tTraining Loss: 0.015604 \tValidation Loss: 0.031891\n","Epoch: 352 \tTraining Loss: 0.015595 \tValidation Loss: 0.032316\n","Epoch: 353 \tTraining Loss: 0.016094 \tValidation Loss: 0.031804\n","Epoch: 354 \tTraining Loss: 0.016023 \tValidation Loss: 0.031821\n","Epoch: 355 \tTraining Loss: 0.015327 \tValidation Loss: 0.031576\n","Epoch: 356 \tTraining Loss: 0.015737 \tValidation Loss: 0.031743\n","Epoch: 357 \tTraining Loss: 0.015462 \tValidation Loss: 0.031907\n","Epoch: 358 \tTraining Loss: 0.015432 \tValidation Loss: 0.031772\n","Epoch: 359 \tTraining Loss: 0.015773 \tValidation Loss: 0.031754\n","Epoch: 360 \tTraining Loss: 0.015405 \tValidation Loss: 0.031434\n","Epoch: 361 \tTraining Loss: 0.016124 \tValidation Loss: 0.032067\n","Epoch: 362 \tTraining Loss: 0.015606 \tValidation Loss: 0.032335\n","Epoch: 363 \tTraining Loss: 0.015737 \tValidation Loss: 0.031845\n","Epoch: 364 \tTraining Loss: 0.015989 \tValidation Loss: 0.031974\n","Epoch: 365 \tTraining Loss: 0.015476 \tValidation Loss: 0.031971\n","Epoch: 366 \tTraining Loss: 0.015466 \tValidation Loss: 0.031894\n","Epoch: 367 \tTraining Loss: 0.015615 \tValidation Loss: 0.032060\n","Epoch: 368 \tTraining Loss: 0.015660 \tValidation Loss: 0.031871\n","Epoch: 369 \tTraining Loss: 0.014988 \tValidation Loss: 0.031959\n","Epoch: 370 \tTraining Loss: 0.015297 \tValidation Loss: 0.031796\n","Epoch: 371 \tTraining Loss: 0.015351 \tValidation Loss: 0.032275\n","Epoch: 372 \tTraining Loss: 0.015281 \tValidation Loss: 0.032389\n","Epoch: 373 \tTraining Loss: 0.014967 \tValidation Loss: 0.031449\n","Epoch: 374 \tTraining Loss: 0.015137 \tValidation Loss: 0.031860\n","Epoch: 375 \tTraining Loss: 0.015259 \tValidation Loss: 0.031867\n","Epoch: 376 \tTraining Loss: 0.014891 \tValidation Loss: 0.032062\n","Epoch: 377 \tTraining Loss: 0.015713 \tValidation Loss: 0.032187\n","Epoch: 378 \tTraining Loss: 0.015108 \tValidation Loss: 0.031698\n","Epoch: 379 \tTraining Loss: 0.015031 \tValidation Loss: 0.031930\n","Epoch: 380 \tTraining Loss: 0.015803 \tValidation Loss: 0.031968\n","Epoch: 381 \tTraining Loss: 0.015076 \tValidation Loss: 0.031752\n","Epoch: 382 \tTraining Loss: 0.015088 \tValidation Loss: 0.032501\n","Epoch: 383 \tTraining Loss: 0.014969 \tValidation Loss: 0.031757\n","Epoch: 384 \tTraining Loss: 0.015432 \tValidation Loss: 0.031715\n","Epoch: 385 \tTraining Loss: 0.014992 \tValidation Loss: 0.031818\n","Epoch: 386 \tTraining Loss: 0.014927 \tValidation Loss: 0.031402\n","Epoch: 387 \tTraining Loss: 0.015160 \tValidation Loss: 0.031919\n","Epoch: 388 \tTraining Loss: 0.014878 \tValidation Loss: 0.031547\n","Epoch: 389 \tTraining Loss: 0.015082 \tValidation Loss: 0.031816\n","Epoch: 390 \tTraining Loss: 0.015060 \tValidation Loss: 0.031367\n","Epoch: 391 \tTraining Loss: 0.015375 \tValidation Loss: 0.031771\n","Epoch: 392 \tTraining Loss: 0.014782 \tValidation Loss: 0.031690\n","Epoch: 393 \tTraining Loss: 0.015028 \tValidation Loss: 0.031971\n","Epoch: 394 \tTraining Loss: 0.014708 \tValidation Loss: 0.031690\n","Epoch: 395 \tTraining Loss: 0.015100 \tValidation Loss: 0.031834\n","Epoch: 396 \tTraining Loss: 0.014975 \tValidation Loss: 0.032277\n","Epoch: 397 \tTraining Loss: 0.014990 \tValidation Loss: 0.031933\n","Epoch: 398 \tTraining Loss: 0.015170 \tValidation Loss: 0.032130\n","Epoch: 399 \tTraining Loss: 0.014729 \tValidation Loss: 0.032296\n","Epoch: 400 \tTraining Loss: 0.014847 \tValidation Loss: 0.032010\n","Epoch: 401 \tTraining Loss: 0.015164 \tValidation Loss: 0.032011\n","Epoch: 402 \tTraining Loss: 0.014659 \tValidation Loss: 0.031887\n","Epoch: 403 \tTraining Loss: 0.014956 \tValidation Loss: 0.031929\n","Epoch: 404 \tTraining Loss: 0.015097 \tValidation Loss: 0.031834\n","Epoch: 405 \tTraining Loss: 0.014727 \tValidation Loss: 0.031829\n","Epoch: 406 \tTraining Loss: 0.015232 \tValidation Loss: 0.032133\n","Epoch: 407 \tTraining Loss: 0.015567 \tValidation Loss: 0.031815\n","Epoch: 408 \tTraining Loss: 0.014951 \tValidation Loss: 0.031588\n","Epoch: 409 \tTraining Loss: 0.015069 \tValidation Loss: 0.032540\n","Epoch: 410 \tTraining Loss: 0.015250 \tValidation Loss: 0.031930\n","Epoch: 411 \tTraining Loss: 0.015678 \tValidation Loss: 0.031562\n","Epoch: 412 \tTraining Loss: 0.014991 \tValidation Loss: 0.031580\n","Epoch: 413 \tTraining Loss: 0.015007 \tValidation Loss: 0.031640\n","Epoch: 414 \tTraining Loss: 0.014589 \tValidation Loss: 0.031583\n","Epoch: 415 \tTraining Loss: 0.015146 \tValidation Loss: 0.031901\n","Epoch: 416 \tTraining Loss: 0.014934 \tValidation Loss: 0.031998\n","Epoch: 417 \tTraining Loss: 0.014817 \tValidation Loss: 0.032202\n","Epoch: 418 \tTraining Loss: 0.015412 \tValidation Loss: 0.032006\n","Epoch: 419 \tTraining Loss: 0.015077 \tValidation Loss: 0.031545\n","Epoch: 420 \tTraining Loss: 0.014533 \tValidation Loss: 0.031642\n","Epoch: 421 \tTraining Loss: 0.014374 \tValidation Loss: 0.031807\n","Epoch: 422 \tTraining Loss: 0.014985 \tValidation Loss: 0.032182\n","Epoch: 423 \tTraining Loss: 0.014482 \tValidation Loss: 0.031997\n","Epoch: 424 \tTraining Loss: 0.015506 \tValidation Loss: 0.031902\n","Epoch: 425 \tTraining Loss: 0.015119 \tValidation Loss: 0.031875\n","Epoch: 426 \tTraining Loss: 0.014692 \tValidation Loss: 0.032080\n","Epoch: 427 \tTraining Loss: 0.014883 \tValidation Loss: 0.031700\n","Epoch: 428 \tTraining Loss: 0.014400 \tValidation Loss: 0.031677\n","Epoch: 429 \tTraining Loss: 0.014731 \tValidation Loss: 0.031821\n","Epoch: 430 \tTraining Loss: 0.014443 \tValidation Loss: 0.031682\n","Epoch: 431 \tTraining Loss: 0.014293 \tValidation Loss: 0.031920\n","Epoch: 432 \tTraining Loss: 0.014250 \tValidation Loss: 0.031525\n","Epoch: 433 \tTraining Loss: 0.014283 \tValidation Loss: 0.032292\n","Epoch: 434 \tTraining Loss: 0.014825 \tValidation Loss: 0.031713\n","Epoch: 435 \tTraining Loss: 0.014667 \tValidation Loss: 0.031727\n","Epoch: 436 \tTraining Loss: 0.015191 \tValidation Loss: 0.031968\n","Epoch: 437 \tTraining Loss: 0.015122 \tValidation Loss: 0.031845\n","Epoch: 438 \tTraining Loss: 0.015616 \tValidation Loss: 0.032104\n","Epoch: 439 \tTraining Loss: 0.014979 \tValidation Loss: 0.031627\n","Epoch: 440 \tTraining Loss: 0.014802 \tValidation Loss: 0.031696\n","Epoch: 441 \tTraining Loss: 0.014163 \tValidation Loss: 0.031754\n","Epoch: 442 \tTraining Loss: 0.014600 \tValidation Loss: 0.032062\n","Epoch: 443 \tTraining Loss: 0.014070 \tValidation Loss: 0.031553\n","Epoch: 444 \tTraining Loss: 0.014475 \tValidation Loss: 0.031719\n","Epoch: 445 \tTraining Loss: 0.014822 \tValidation Loss: 0.031306\n","Epoch: 446 \tTraining Loss: 0.014192 \tValidation Loss: 0.031914\n","Epoch: 447 \tTraining Loss: 0.014311 \tValidation Loss: 0.031444\n","Epoch: 448 \tTraining Loss: 0.014657 \tValidation Loss: 0.031630\n","Epoch: 449 \tTraining Loss: 0.014193 \tValidation Loss: 0.032012\n","Epoch: 450 \tTraining Loss: 0.014836 \tValidation Loss: 0.031396\n","Epoch: 451 \tTraining Loss: 0.013894 \tValidation Loss: 0.031757\n","Epoch: 452 \tTraining Loss: 0.014339 \tValidation Loss: 0.032084\n","Epoch: 453 \tTraining Loss: 0.015009 \tValidation Loss: 0.032149\n","Epoch: 454 \tTraining Loss: 0.013797 \tValidation Loss: 0.032466\n","Epoch: 455 \tTraining Loss: 0.014338 \tValidation Loss: 0.032333\n","Epoch: 456 \tTraining Loss: 0.014337 \tValidation Loss: 0.031668\n","Epoch: 457 \tTraining Loss: 0.014653 \tValidation Loss: 0.031835\n","Epoch: 458 \tTraining Loss: 0.014724 \tValidation Loss: 0.031689\n","Epoch: 459 \tTraining Loss: 0.014045 \tValidation Loss: 0.031958\n","Epoch: 460 \tTraining Loss: 0.014201 \tValidation Loss: 0.031641\n","Epoch: 461 \tTraining Loss: 0.014431 \tValidation Loss: 0.031480\n","Epoch: 462 \tTraining Loss: 0.014213 \tValidation Loss: 0.031426\n","Epoch: 463 \tTraining Loss: 0.014234 \tValidation Loss: 0.031455\n","Epoch: 464 \tTraining Loss: 0.014554 \tValidation Loss: 0.031767\n","Epoch: 465 \tTraining Loss: 0.014335 \tValidation Loss: 0.031572\n","Epoch: 466 \tTraining Loss: 0.014069 \tValidation Loss: 0.031827\n","Epoch: 467 \tTraining Loss: 0.014188 \tValidation Loss: 0.031999\n","Epoch: 468 \tTraining Loss: 0.014239 \tValidation Loss: 0.031441\n","Epoch: 469 \tTraining Loss: 0.014355 \tValidation Loss: 0.032092\n","Epoch: 470 \tTraining Loss: 0.014073 \tValidation Loss: 0.031746\n","Epoch: 471 \tTraining Loss: 0.014284 \tValidation Loss: 0.031737\n","Epoch: 472 \tTraining Loss: 0.014497 \tValidation Loss: 0.031646\n","Epoch: 473 \tTraining Loss: 0.014197 \tValidation Loss: 0.032157\n","Epoch: 474 \tTraining Loss: 0.014293 \tValidation Loss: 0.031745\n","Epoch: 475 \tTraining Loss: 0.013969 \tValidation Loss: 0.031878\n","Epoch: 476 \tTraining Loss: 0.014312 \tValidation Loss: 0.031472\n","Epoch: 477 \tTraining Loss: 0.013737 \tValidation Loss: 0.031684\n","Epoch: 478 \tTraining Loss: 0.014190 \tValidation Loss: 0.031618\n","Epoch: 479 \tTraining Loss: 0.014862 \tValidation Loss: 0.031359\n","Epoch: 480 \tTraining Loss: 0.014032 \tValidation Loss: 0.031693\n","Epoch: 481 \tTraining Loss: 0.014062 \tValidation Loss: 0.032081\n","Epoch: 482 \tTraining Loss: 0.013789 \tValidation Loss: 0.031707\n","Epoch: 483 \tTraining Loss: 0.014548 \tValidation Loss: 0.031692\n","Epoch: 484 \tTraining Loss: 0.014435 \tValidation Loss: 0.032111\n","Epoch: 485 \tTraining Loss: 0.014429 \tValidation Loss: 0.032065\n","Epoch: 486 \tTraining Loss: 0.014452 \tValidation Loss: 0.031999\n","Epoch: 487 \tTraining Loss: 0.014814 \tValidation Loss: 0.031352\n","Epoch: 488 \tTraining Loss: 0.014034 \tValidation Loss: 0.031735\n","Epoch: 489 \tTraining Loss: 0.014139 \tValidation Loss: 0.031939\n","Epoch: 490 \tTraining Loss: 0.014323 \tValidation Loss: 0.032203\n","Epoch: 491 \tTraining Loss: 0.013982 \tValidation Loss: 0.032209\n","Epoch: 492 \tTraining Loss: 0.014136 \tValidation Loss: 0.031851\n","Epoch: 493 \tTraining Loss: 0.014090 \tValidation Loss: 0.031706\n","Epoch: 494 \tTraining Loss: 0.013913 \tValidation Loss: 0.031381\n","Epoch: 495 \tTraining Loss: 0.014061 \tValidation Loss: 0.032074\n","Epoch: 496 \tTraining Loss: 0.014238 \tValidation Loss: 0.031825\n","Epoch: 497 \tTraining Loss: 0.014409 \tValidation Loss: 0.031465\n","Epoch: 498 \tTraining Loss: 0.014314 \tValidation Loss: 0.031630\n","Epoch: 499 \tTraining Loss: 0.014437 \tValidation Loss: 0.032406\n","Epoch: 500 \tTraining Loss: 0.014515 \tValidation Loss: 0.031705\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SJ475eIjQz-Z","colab_type":"text"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"uG-bns0saAdN","colab_type":"code","colab":{}},"source":["def predict(data_loader, model):\n","    '''\n","    Predict keypoints\n","    Args:\n","        data_loader (DataLoader): DataLoader for Dataset\n","        model (nn.Module): trained model for prediction.\n","    Return:\n","        predictions (array-like): keypoints in float (no. of images x keypoints).\n","    '''\n","    \n","    model.eval() # prep model for evaluation\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(data_loader):\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(batch['image'].to(device)).cpu().numpy()\n","            if i == 0:\n","                predictions = output\n","            else:\n","                predictions = np.vstack((predictions, output))\n","\n","    return predictions"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkTFU-T9Q8I2","colab_type":"text"},"source":["The error is 0.0671 and shows decreased comparing to formal one 0.105"]},{"cell_type":"code","metadata":{"id":"vZrHN2u7aOfx","colab_type":"code","outputId":"e2edb6f3-9479-4cd1-8be5-9f66d4eb596a","executionInfo":{"status":"ok","timestamp":1584941715749,"user_tz":-480,"elapsed":894,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["model.load_state_dict(torch.load(ROOT_FOLDER+'model.pt'))\n","predictions = predict(test_loader, model)\n","\n","print('Error: ', np.linalg.norm(predictions-KPT_S_TEST.reshape((predictions.shape[0],-1)), axis=1).mean())\n","\n","idx = 100\n","draw_points(IMG_S_TEST[idx,:,:], predictions[idx,:].reshape((-1,2)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Error:  0.06718845568535453\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAZhUlEQVR4nO3deZRU5Z3/8ffXFtpGQBYJIcqqmIOS\nSFi0E0XT+JuR5ARQEv1pklEm5ofGaIinMWpMMiGLiRnE5eASEwmYKHFt9eQ4LmNwjIlGGmVYMgZb\nphUEuwER2dIN3d/fH3XBgq6ml6pbT3Xdz+uc51Tdp27d53u76S/P3Z7H3B0RSa7DQgcgImEpCYgk\nnJKASMIpCYgknJKASMIpCYgkXGxJwMwmm9nfzazGzK6Nqx0RyY7FcZ+AmZUAa4B/AtYDS4EL3f1v\nOW9MRLISV0/gFKDG3de6eyPwe2BaTG2JSBYOj2m7xwDr0pbXA6e2trKZ6bbFTjr66KMZOnRou9ev\nr69n3bp1ba8oxWizuw84uDKuJNAmM5sJzAzVfjH48Y9/zDXXXEO3bt3a/Z358+dz5ZVXxhiVFLC3\nMlXGdU7g08AP3f3saPk6AHf/WSvrqyfQQXfddRdf+cpX6NmzZ4e+t2vXLrZt29bq52PHjuXdd9/N\nNjwpTMvcfXyLWnfPeSHVw1gLDAe6A/8NnHSI9V2l/WXRokW+detWj8MxxxzjgM+bN89fe+01v/rq\nq4Pvr0rOSnWmv79YTgy6+17gCuBp4H+AB919dRxtJc0999zD1KlT6dOnT6zt3H///bz99ttcfvnl\nfO9734u1LQkrtnMC7v4k8GRc20+qSZMmxZoAfvKTn7Bjxw4AqquraWxs5Gtf+xp79uzhxhtvjK1d\nCSeWcwIdDkLnBNrlyiuvZM6cOfTt2zcv7V111VVUV1dzzjnnsGfPHmpra/nlL3+Zl7YlFhnPCQS7\nOiAd99Of/pRevXrltc0XX3yRF198kREjRjBnzhzq6+sBqKqqymscEh8lAWnVkCFDGD/+w/847r//\nfp58MnWEN2HCBJYtW0Yh9CQlOzoc6EI++OCDvPcEDqVHjx7s3r07dBjSfhkPB/QUoUjCKQlIp5lZ\n6BAkB5QEpNN27tzJoEGDQochWVISEEk4JQHptKFDh1JXVxc6DMmSkoB02qZNm2hubg4dhmRJSUAk\n4ZQERBJOSUAk4XTbcBfy6KOPUlZWBsCnP/1pBg8enLe2t27dyrPPPntAXVNTU97alxjFMahIRwvh\nB1vocuWBBx6IY0yRVi1btiz4PqtkXfI3qIiIdB1KAiIJpyQg7XbYYfrnUow6/Vs1s8FmtsTM/mZm\nq81sVlT/QzN7x8yWR+XzuQtXQhk7dixvvZVxxGrp4rK5OrAXqHT3V82sF7DMzPadPr7Z3edmH56I\nxK3TPQF33+jur0bvt5MaVfiYXAUmh/b1r3+d+fPn5629FStWcPLJJ+etPcmfnNwnYGbDgE8BfwVO\nA64ws4uAalK9ha25aEc+tH37dnbt2hVrGz//+c/57W9/C6QmLXnvvfdibU8Cae+1/NYK0BNYBkyP\nlgcCJaR6GT8FFrTyvZmkkkQ14a+fdsly/PHH+y233BLbvQHf/va3g++jSk5L7u8TMLNuwCPAfe7+\nKIC717l7k7s3A78iNUNxC+5+t7uP90zTIkm71NTUcNddd3HrrbfG2s7JJ5+sCUiKWDZXBwy4B/gf\nd5+XVp8+1My5wKrOhydtef3115k/fz6//vWvY2tj2LBhnHvuubFtX8LKpidwGvAvwKSDLgf+wsxW\nmtkKoAK4KheBSutqampi6w2ccMIJTJw4kX79+jFt2rRY2pDAsj0nkItC+GOlLl9GjBjhf/3rX3N6\nTuDWW2894JzDpk2bvLy8PPi+qnS6ZDwnoHkHikj//v1ZunQpw4cPj62NXbt2ceSRR8a2fYmV5h0o\ndlu2bGHUqFFs27aNbdu2xdKGmXHUUUfFsm0JQz2BIrZt2zZ69+6d8+26u54j6JrUE0iao446irff\nfjt0GFLglASK3HHHHUf37t33l1mzZmW9TTOjsbGRI444IgcRSmg6HEiY0tJSevbsyejRo3n++eez\n2pYmJO1ydDgg0NDQwJYtW9p34rCxEWbPhnHjUq+NjQd8vHz5cgYMGBBTpJI3oe8R0H0C+S/l5eX+\nyiuvuLt7Q0ODV1RUeHNzc8sbBSor3cvK3CH1WlnZYpU//elPftxxxwXfJ5V2FY0xKClHH300EyZM\nYNu2bVx22WVcdNFFmWcYXrIE9nX3d+9OLR/k9NNPZ968eXrMuAtTEkiouro6br31VoYPH86MGTMy\nr1RRAdEQ55SVpZYzmDp1KrNnz2b8eD0L1hVp3oEEWr9+PY888gibN2/mtttua33FG25IvS5ZkkoA\n+5Yz+OpXv8rLL79MdXV1jqOVuCkJJNDbb79NdXU1CxYsOPSK3bvD3PaPEnf88cczYsQI1q5dm2WE\nklehTwrqxGD+yxe+8IWWJwFzZO7cucH3T6XVohODEr/S0lI9YNTFKAlITl1xxRX7xyWUrkFJQCTp\nQp8P0DmB/JZvfOMb3tTUFNs5AXf35uZmf+mll4Lvq0qLonMCkppKLO7HgM2MCRMmsHLlyljbkdzQ\nJcIEueaaa6isrMxLWyUlJRp8pIvIOgmYWS2wHWgC9rr7eDPrBzwADANqgfNdE5AEN2DAAD3wIy3k\nql9Y4e5j/MPHFK8FnnP3kcBz0bIkTP/+/bn77rtDhyFtiOvgcBqwKHq/CDgnpnakgPXo0YMLL7ww\ndBjShlwkAQeeMbNlZjYzqhvo7huj9++SmprsAGY208yqzUw3m+dBRUUFo0ePDh2GFKIcXN47Jnr9\nCPDfwBnA+wets7WNbYS+dFLUZfz48f7CCy/EeVWwVdu3bw++/yr7SzyXCN39nei1HqgiNfdg3b7p\nyKLX+mzbkc67/fbbmThxYugwpEBlOyHpkWbWa9974J9JzT34BHBxtNrFwOPZtCMi8cn2EuFAoCoa\nleZw4H53f8rMlgIPmtklwFvA+Vm2IyIxySoJuPtaoMW4Uu6+BTgrm22LSH7otmGRhNNtw0Vu5cqV\njBo1KnQYUsDUEyhyvXv3pqSkJHQYUsCUBEQSTklAJOGUBIrYD37wA/r06RM6DClwmpC0iG3fvp2e\nPXsGjWHHjh306tUraAyynyYkFZGWlAREEk5JQCThlAQkVnv27AkdgrRBSUBis27dOvr16xc6DGmD\nkoBIwikJFKn3338/+OVB6RqUBIpU7969Q4cgXYSSgEjCKQmIJJySgEjCdXpQETP7OKmpxvYZAfwA\n6AP8P2BTVP9dd3+y0xFKl7Rp0yZ+97vfhQ5D2iEnDxCZWQnwDnAq8K/ADnef24Hv6wGiHGtubiYa\nADaIpUuXcsoppwRrXzKK9QGis4A33f2tHG1PRPIkV0ngAmBx2vIVZrbCzBaYWd9MX9A0ZCKFIesk\nYGbdganAQ1HVncBxwBhgI3BTpu+5+93uPj5T90RE8icXPYHPAa+6ex2Au9e5e5O7NwO/IjUtmYgU\nqFwkgQtJOxTYNwdh5FxS05KJSIHKat6BaP7BfwIuTav+hZmNITULau1Bn0nMSktL2bx5c9ArA9K1\nZDsN2U6g/0F1/5JVRJK10A8OPfXUU8yYMSNoDNJ+umNQcm737t3U1dWFDkPaSUlAJOGUBIrM3r17\nmT17NoUwlLx0DUoCRaapqYnbbroJrr4axo2D2bOhsTF/ATQ2MnrhQpYCvwC65a9l6Sx3D15IXUlQ\nyVH5BXhzWZk7uJeVuVdWet5UVvqe7t3dwXdEsYT+eajsL9WZ/v7UEyhCFYDt3p1a2L0blizJX+NL\nlnB41PM4MopFCpuSQBFaAnhZWWqhrAwq8vinWFHB3u7dAdgVxSIFLpfd+s4WwneTiqp0A2+urHQf\nOzZ1KNDQEOcBwIEaGnzN1Km+lNShQLcC+Hmo7C8ZDwc0IWmRCjmeQFVVFdOnTw/SthySJiQVkZaU\nBEQSTklAJOGUBEQSTklAcq68vJy5c9s9zqwEpiQgOTdo0CDOOOOM0GFIOykJiCSckoBIwikJSCyO\nPPJIRo8eHToMaYd2JYFo/oB6M1uVVtfPzJ41szei175RvZnZbWZWE809MDau4KVwnXjiiTzzzDMM\nGjSo7ZUlqPb2BBYCkw+quxZ4zt1HAs9Fy5AagnxkVGaSmodAEmjQoEGsWbMmdBjShnYlAXd/AXjv\noOppwKLo/SLgnLT6e6PHSV4G+hw0DLmIFJBszgkMdPeN0ft3gYHR+2OAdWnrrY/qRKQAZTXk+D7u\n7h19EtDMZpI6XBCRgLLpCdTt6+ZHr/VR/TvA4LT1jo3qDuCai7DobdiwgZEjR4YOQ9qQTRJ4Arg4\nen8x8Hha/UXRVYJyYFvaYYMkRWMjA+fO5Y2jjtKAo4Uu00gjBxdScw1uBPaQOsa/hNTMQ88BbwD/\nCfSL1jXgduBNYCUwvh3bDz3iStGV5ubm1IhCoUYYqqxMDXKKBhwtoJJxZKHgQ4u5kkAspbm5+YA/\nxLyPOjx2bKrdqCwtgJ+JikYbTp4lS1KjDUP+Rx2uqEgNcgo0Hn64BhwtYEoCxSztDzHvow7fcANc\nfjmMHUu3WbO4Pn8tSwdpoNEiY2aMGzeOV155BduzB7773VQPoKIi9YcZDQeeT+7OYYfp/5sCkHGg\nUSWBIlNaWso//vGP0GEcQEmgYGi04aTYsWNH6BAOUGjxyIGUBIpMQ0MDvXr1ohB6eJBKAL179w4d\nhhyCkoBIwikJiCSckoBIwikJSGzq6uooLy8PHYa0QUlAYvHGG29wwQUXsHr16tChSBuUBCQW77//\nPs8//3zoMKQdlAREEk5JQCThlAREEk5JoEjV1tYGvWuwtLSUY489Nlj70n56gKiIbdy4kYEDB2Jm\nQdpft24dQ4YMCdK2ZKQHiJJm0KBBrFu3ru0VJdHaTAKtTEH272b2ejTNWJWZ9Ynqh5nZbjNbHpW7\n4gxeRLLXnp7AQlpOQfYsMNrdPwmsAa5L++xNdx8TlctyE6aIxKXNJOAZpiBz92fcfW+0+DKpuQVE\npAvKxTmBrwH/kbY83MxeM7P/MrOJOdi+ZOGss85i5cqVeW931apVTJo0Ke/tSsdlNQ2ZmV0P7AXu\ni6o2AkPcfYuZjQMeM7OT3P2DDN/VNGR5MGfOHIYOHZr3dnfv3k1NTU3e25WO63RPwMxmAF8AvuLR\ndUZ3b3D3LdH7ZaQmIDkh0/dd05DlxR/+8Afee+/gCaVFPtSpJGBmk4HvAFPdfVda/QAzK4nejwBG\nAmtzEah0zuLFi6mvr297xRxas2YNv/nNb/LapnReey4RLgZeAj5uZuvN7BJgPtALePagS4FnACvM\nbDnwMHCZu+u/ocCeeeaZvN4vsHr1au688868tSfZafOcgLtfmKH6nlbWfQR4JNugJLe+//3vM2rU\nKAYPHtz2ypI4umNQJOGUBEQSTklAJOGUBEQSTklAcuqWW27hvPPOCx2GdICSgORUU1MTTU1NocOQ\nDlASSIhvfetbuoFHMlISSIgNGzbk5fbh888/nzlz5sTejuSOkoDk1ODBg/nEJz4ROgzpACUBkYRT\nEhBJOCUBkYRTEhBJOCUByamGhgZ27NgROgzpACUByak77riDGTNmhA5DOkBJQHLqqquu4uGHHw4d\nhnSAkkCCfOc732HWrFmhw5ACoySQIM3NzbqvX1ro7DRkPzSzd9KmG/t82mfXmVmNmf3dzM6OK3AR\nyY3OTkMGcHPadGNPApjZicAFwEnRd+7YN/qwFIaqqiquu+66tleUxOjUNGSHMA34fTT/wP8CNcAp\nWcQnObZhwwZWrVrV9oqSGNmcE7gimpV4gZn1jeqOAdLHtl4f1UkBef3112MbEvwvf/kLixcvjmXb\nEo/OJoE7geOAMaSmHrupoxsws5lmVm1m1Z2MQTqppqaGu+++O5Ztv/TSSzz00EOxbFvi0akk4O51\n7t7k7s3Ar/iwy/8OkD64/bFRXaZtaBqyIjRkyBBGjx4dOgzpgM5OQzYobfFcYN9B5hPABWZWambD\nSU1D9kp2IUpXct555/GjH/0odBjSAW3OQBRNQ/ZZ4GgzWw/8G/BZMxsDOFALXArg7qvN7EHgb6Rm\nK/6mu+vCdMJ069aN3r1788EHLSajlgJk0YTCYYMwCx9EwowZM4bXXnsttu0vXbqUU07RhaECsyzT\n4bfuGBRJOCWBhFq+fDnHHnts6DCkACgJJJieIxBQEki0TZs2MWrUqNBhSGBKAgnW1NTEmjVrmDhx\nIrk+QTxq1CgeffTRnG5T4qGrAwKkHjM2s5xuc926dQwZMiSn25Ss6OqAtO76669nz549ocOQANQT\nkP127txJjx49crY99QQKjnoCcmh//OMfaWhoCB2G5JmSgOw3ZcoUNm3aFDoMyTMlATnA1q1b2bt3\nb062VVJSQv/+/XOyLYmPkoAc4JOf/CQrVqzIybY+9rGPUVtbm5NtSXyUBEQSTklAWpgwYQKPP/54\n6DAkT5QEpIXm5uac30EohUtJQDK69NJLWbBgQegwJA+UBCSj+vp6jQyUEEoC0qpFixaxcOHC0GFI\nzDo7DdkDaVOQ1ZrZ8qh+mJntTvvsrjiDl3gtX7481iHIpDB0ahoyd/+/+6YgAx4B0p8ZfTNterLL\ncheqhLBy5Uqeeuqp0GFIjNocbdjdXzCzYZk+s9Szp+cDk3IblhSKJUuW0Lt3byZPzjQdpRSDbM8J\nTATq3P2NtLrhZvaamf2XmU3McvsiErM2ewJtuBBIn3huIzDE3beY2TjgMTM7yd1bnGY2s5nAzCzb\nF5EsdbonYGaHA9OBB/bVRbMRb4neLwPeBE7I9H1NQyZSGLI5HPg/wOvuvn5fhZkNMLOS6P0IUtOQ\nrc0uRBGJU3suES4GXgI+bmbrzeyS6KMLOPBQAOAMYEV0yfBh4DJ3fy+XAUt+XXnllVRVVYUOQ+Lk\n7sELqTkNVQqs3Hjjjd7Q0ODZ2L59e/D9UNlfqjP9/WV7YlCKRG1tbYvRhvv27Uv37t0DRST5oiSQ\nQKeddhrz5s07oG7o0KGxtNWjRw9efvllysvLY9m+5EDoQwEdDuS3TJ482f/85z9n1cXvqObm5uD7\nrYLTyuGAHiBKkClTpnD11Vfzmc98Jm9tNjY2UllZmbf2pOOUBBLkzDPPZNKk/N7h3djYyM0335zX\nNqVjlAREEk5JICGGDRvGRz7ykdBhSAHS1YGEmDt3Ll/84hdDhyEFSD0BkYRTEhBJOCWBhPjSl77U\n4gahTmtshNmzYdy41GtjY6ur9uzZM2fTmklMsr3RJxeF8DdRJKJ069bNe/To4T169PDy8vLO3/1T\nWeleVuYOqdfKyja/snPnTi8tLQ3+M0h4yXizkHkBTDJhZuGDSJhu3bod8mrBPffcw9lnn535w3Hj\n4NVXP1weOxaWLWuzzQ0bNnDyySezefPmjoYrubHMM43fEboXoJ5AYZYRI0b4fffdl7OewD4DBw4M\nvm8JLrptWA7tgQce4LHHHmPEiBGsXbuW+vr6zCvecANcfnmqB3D55anldlq0aFFsDytJ5+g+AQFg\n3rx5TJ8+ncMPP5ydO3dSX1/PmWeemXnl7t1h7txOtXP22WfTq1evLCKVnAt9KKDDgbClpKTEZ82a\n1e7ufC6MHj06+H4ntGhQETlQaWkpU6ZM4ZZbbslbm08//TTbt2/PW3vSNiWBhCorK+P000/noYce\nykt7r0ZXE7785S/z3nsadrKQKAkk0BFHHMGpp57KvffeG2s7TU1NbNy4EYBx48bF2pZ0nq4OJFBF\nRQU/+9nP+NSnPhXL9puamti1axdvvvkmgwcPZvDgwbG0I7mhm4US7KMf/ej+/6lzqaqqiunTp+d8\nu5K1jDcLFUoS2ATsBIrxVrKjKc79guLdt2Ldr6HuPuDgyoJIAgBmVp0pS3V1xbpfULz7Vqz71Rqd\nExBJOCUBkYQrpCRwd+gAYlKs+wXFu2/Ful8ZFcw5AREJo5B6AiISQPAkYGaTzezvZlZjZteGjidb\nZlZrZivNbLmZVUd1/czsWTN7I3rtGzrOtpjZAjOrN7NVaXUZ98NSbot+hyvMbGy4yNvWyr790Mze\niX5vy83s82mfXRft29/NrJWRVrquoEnAzEqA24HPAScCF5rZiSFjypEKdx+TdpnpWuA5dx8JPBct\nF7qFwOSD6lrbj88BI6MyE7gzTzF21kJa7hvAzdHvbYy7PwkQ/Xu8ADgp+s4d0b/bohG6J3AKUOPu\na929Efg9MC1wTHGYBiyK3i8CzgkYS7u4+wvAwU/6tLYf04B7oyeFXwb6mNmg/ETaca3sW2umAb93\n9wZ3/1+ghtS/26IROgkcA6xLW14f1XVlDjxjZsvMbGZUN9Dd992f+y4wMExoWWttP4rl93hFdDiz\nIO2QrVj2rVWhk0AxOt3dx5LqIn/TzM5I/9BTl2O6/CWZYtmPNHcCxwFjgI3ATWHDyZ/QSeAdIP0R\ns2Ojui7L3d+JXuuBKlJdx7p93ePotZXB+wpea/vR5X+P7l7n7k3u3gz8ig+7/F1+39oSOgksBUaa\n2XAz607qBMwTgWPqNDM70sx67XsP/DOwitQ+XRytdjHweJgIs9bafjwBXBRdJSgHtqUdNnQJB53D\nOJfU7w1S+3aBmZWa2XBSJz9fyXd8cQo6qIi77zWzK4CngRJggbuvDhlTlgYCVWYGqZ/t/e7+lJkt\nBR40s0uAt4DzA8bYLma2GPgscLSZrQf+Dfg5mffjSeDzpE6a7QL+Ne8Bd0Ar+/ZZMxtD6hCnFrgU\nwN1Xm9mDwN+AvcA33b0pRNxx0R2DIgkX+nBARAJTEhBJOCUBkYRTEhBJOCUBkYRTEhBJOCUBkYRT\nEhBJuP8PigPvyJeFpisAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"NBbUcIv-RLhM","colab_type":"text"},"source":["# Try another network CNN\n","I set number of filters as 65,130,260,520,1040,2080 separatly in nn.conv2d() \n","\n","The more the number of filters, the more feature CNN model will learn\n","\n","Suprisingly, I delete the drop out function to try to achieve overfitting to see the error but it shows error is decreased. So i think it's my best error.\n"]},{"cell_type":"code","metadata":{"id":"6fJ5gNDWbCgO","colab_type":"code","colab":{}},"source":["class CNN(nn.Module):\n","    def __init__(self, output_size):\n","        super(CNN, self).__init__()\n","        # 200 x 200\n","        self.conv1 = nn.Conv2d(1, 65, 5)\n","        # (w-f)/s+1 = 196\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        # 98\n","        self.conv2 = nn.Conv2d(65,130,3)\n","        # (98-3)/1 + 1 = 96\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        # 48\n","        self.conv3 = nn.Conv2d(130,260,3,stride=1)\n","        # (48-3)/1 + 1 = 46\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","        # 23\n","        self.conv4 = nn.Conv2d(260,520,3,stride=2)\n","        # (23-3)/2 + 1 = 11\n","        self.conv5 = nn.Conv2d(520,1040,1,stride=2)\n","        # (11-1)/2+1 = 6\n","        #Linear Layer\n","        self.fc1 = nn.Linear(1040*6*6, 2080)\n","        self.fc2 = nn.Linear(2080, output_size)\n","        '''\n","        self.drop1 = nn.Dropout(p = 0.1)\n","        self.drop2 = nn.Dropout(p = 0.2)\n","        self.drop3 = nn.Dropout(p = 0.25)\n","        self.drop4 = nn.Dropout(p = 0.25)\n","        self.drop5 = nn.Dropout(p = 0.3)\n","        self.drop6 = nn.Dropout(p = 0.4)\n","        '''\n","    def forward(self, x):  \n","        x = self.pool1(F.relu(self.conv1(x)))\n","       # x = self.drop1(x)\n","        x = self.pool2(F.relu(self.conv2(x)))\n","       # x = self.drop2(x)\n","        x = self.pool3(F.relu(self.conv3(x)))\n","       # x = self.drop3(x)\n","        x = F.relu(self.conv4(x))\n","       # x = self.drop4(x)\n","        x = F.relu(self.conv5(x))\n","       # x = self.drop5(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","       # x = self.drop6(x)\n","        x = self.fc2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1FR4MmA8UqZK","colab_type":"text"},"source":["learning rate=0.0001\n","\n","n_epoches=1000"]},{"cell_type":"code","metadata":{"id":"g1UKpZSwbsoW","colab_type":"code","outputId":"795de349-db04-4bd8-b7fa-1360d18afda4","executionInfo":{"status":"ok","timestamp":1584542015067,"user_tz":-480,"elapsed":2783210,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# if you find this step is slow, use GPU mode: Runtime-> change runtime type -> GPU\n","model = CNN(output_size=12)\n","model = model.to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","train_losses, valid_losses = train(train_loader, valid_loader,\n","                                   model, criterion, optimizer,\n","                                   n_epochs=1000,\n","                                   saved_model=ROOT_FOLDER+'model.pt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.057117 \tValidation Loss: 0.037107\n","Validation loss decreased (inf --> 0.037107).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.031845 \tValidation Loss: 0.029878\n","Validation loss decreased (0.037107 --> 0.029878).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.030580 \tValidation Loss: 0.027881\n","Validation loss decreased (0.029878 --> 0.027881).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.028327 \tValidation Loss: 0.028140\n","Epoch: 5 \tTraining Loss: 0.027036 \tValidation Loss: 0.024949\n","Validation loss decreased (0.027881 --> 0.024949).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.025149 \tValidation Loss: 0.025738\n","Epoch: 7 \tTraining Loss: 0.025182 \tValidation Loss: 0.023982\n","Validation loss decreased (0.024949 --> 0.023982).  Saving model ...\n","Epoch: 8 \tTraining Loss: 0.023982 \tValidation Loss: 0.024457\n","Epoch: 9 \tTraining Loss: 0.023878 \tValidation Loss: 0.023803\n","Validation loss decreased (0.023982 --> 0.023803).  Saving model ...\n","Epoch: 10 \tTraining Loss: 0.023537 \tValidation Loss: 0.024433\n","Epoch: 11 \tTraining Loss: 0.022827 \tValidation Loss: 0.024169\n","Epoch: 12 \tTraining Loss: 0.022844 \tValidation Loss: 0.025419\n","Epoch: 13 \tTraining Loss: 0.022246 \tValidation Loss: 0.024399\n","Epoch: 14 \tTraining Loss: 0.021610 \tValidation Loss: 0.024123\n","Epoch: 15 \tTraining Loss: 0.020971 \tValidation Loss: 0.023805\n","Epoch: 16 \tTraining Loss: 0.020932 \tValidation Loss: 0.024671\n","Epoch: 17 \tTraining Loss: 0.020711 \tValidation Loss: 0.024276\n","Epoch: 18 \tTraining Loss: 0.019543 \tValidation Loss: 0.024096\n","Epoch: 19 \tTraining Loss: 0.019752 \tValidation Loss: 0.025076\n","Epoch: 20 \tTraining Loss: 0.019387 \tValidation Loss: 0.024306\n","Epoch: 21 \tTraining Loss: 0.018381 \tValidation Loss: 0.024633\n","Epoch: 22 \tTraining Loss: 0.018310 \tValidation Loss: 0.026133\n","Epoch: 23 \tTraining Loss: 0.017630 \tValidation Loss: 0.024529\n","Epoch: 24 \tTraining Loss: 0.017029 \tValidation Loss: 0.025557\n","Epoch: 25 \tTraining Loss: 0.017383 \tValidation Loss: 0.025076\n","Epoch: 26 \tTraining Loss: 0.016894 \tValidation Loss: 0.026891\n","Epoch: 27 \tTraining Loss: 0.016287 \tValidation Loss: 0.026614\n","Epoch: 28 \tTraining Loss: 0.015499 \tValidation Loss: 0.025055\n","Epoch: 29 \tTraining Loss: 0.015012 \tValidation Loss: 0.025499\n","Epoch: 30 \tTraining Loss: 0.014314 \tValidation Loss: 0.025884\n","Epoch: 31 \tTraining Loss: 0.014321 \tValidation Loss: 0.026060\n","Epoch: 32 \tTraining Loss: 0.014012 \tValidation Loss: 0.026685\n","Epoch: 33 \tTraining Loss: 0.013829 \tValidation Loss: 0.025519\n","Epoch: 34 \tTraining Loss: 0.014444 \tValidation Loss: 0.026737\n","Epoch: 35 \tTraining Loss: 0.013284 \tValidation Loss: 0.026517\n","Epoch: 36 \tTraining Loss: 0.012381 \tValidation Loss: 0.026199\n","Epoch: 37 \tTraining Loss: 0.011861 \tValidation Loss: 0.026024\n","Epoch: 38 \tTraining Loss: 0.011553 \tValidation Loss: 0.025654\n","Epoch: 39 \tTraining Loss: 0.010808 \tValidation Loss: 0.026276\n","Epoch: 40 \tTraining Loss: 0.010787 \tValidation Loss: 0.026071\n","Epoch: 41 \tTraining Loss: 0.010614 \tValidation Loss: 0.026567\n","Epoch: 42 \tTraining Loss: 0.010854 \tValidation Loss: 0.026414\n","Epoch: 43 \tTraining Loss: 0.011864 \tValidation Loss: 0.026477\n","Epoch: 44 \tTraining Loss: 0.010789 \tValidation Loss: 0.025858\n","Epoch: 45 \tTraining Loss: 0.009904 \tValidation Loss: 0.026447\n","Epoch: 46 \tTraining Loss: 0.010475 \tValidation Loss: 0.026426\n","Epoch: 47 \tTraining Loss: 0.010245 \tValidation Loss: 0.026262\n","Epoch: 48 \tTraining Loss: 0.010085 \tValidation Loss: 0.025287\n","Epoch: 49 \tTraining Loss: 0.011550 \tValidation Loss: 0.026040\n","Epoch: 50 \tTraining Loss: 0.012068 \tValidation Loss: 0.026798\n","Epoch: 51 \tTraining Loss: 0.010314 \tValidation Loss: 0.026998\n","Epoch: 52 \tTraining Loss: 0.009210 \tValidation Loss: 0.025861\n","Epoch: 53 \tTraining Loss: 0.009101 \tValidation Loss: 0.026905\n","Epoch: 54 \tTraining Loss: 0.008483 \tValidation Loss: 0.026631\n","Epoch: 55 \tTraining Loss: 0.008597 \tValidation Loss: 0.026398\n","Epoch: 56 \tTraining Loss: 0.008567 \tValidation Loss: 0.026129\n","Epoch: 57 \tTraining Loss: 0.008821 \tValidation Loss: 0.026310\n","Epoch: 58 \tTraining Loss: 0.008632 \tValidation Loss: 0.026549\n","Epoch: 59 \tTraining Loss: 0.008157 \tValidation Loss: 0.026199\n","Epoch: 60 \tTraining Loss: 0.007969 \tValidation Loss: 0.026310\n","Epoch: 61 \tTraining Loss: 0.008002 \tValidation Loss: 0.025808\n","Epoch: 62 \tTraining Loss: 0.007673 \tValidation Loss: 0.026121\n","Epoch: 63 \tTraining Loss: 0.008274 \tValidation Loss: 0.026363\n","Epoch: 64 \tTraining Loss: 0.008249 \tValidation Loss: 0.026267\n","Epoch: 65 \tTraining Loss: 0.007220 \tValidation Loss: 0.026399\n","Epoch: 66 \tTraining Loss: 0.007560 \tValidation Loss: 0.025919\n","Epoch: 67 \tTraining Loss: 0.007661 \tValidation Loss: 0.026461\n","Epoch: 68 \tTraining Loss: 0.006967 \tValidation Loss: 0.026030\n","Epoch: 69 \tTraining Loss: 0.007655 \tValidation Loss: 0.026421\n","Epoch: 70 \tTraining Loss: 0.007236 \tValidation Loss: 0.025950\n","Epoch: 71 \tTraining Loss: 0.007267 \tValidation Loss: 0.025757\n","Epoch: 72 \tTraining Loss: 0.007012 \tValidation Loss: 0.026321\n","Epoch: 73 \tTraining Loss: 0.006885 \tValidation Loss: 0.025843\n","Epoch: 74 \tTraining Loss: 0.006575 \tValidation Loss: 0.026004\n","Epoch: 75 \tTraining Loss: 0.006282 \tValidation Loss: 0.025463\n","Epoch: 76 \tTraining Loss: 0.006468 \tValidation Loss: 0.026390\n","Epoch: 77 \tTraining Loss: 0.007028 \tValidation Loss: 0.025902\n","Epoch: 78 \tTraining Loss: 0.006980 \tValidation Loss: 0.026252\n","Epoch: 79 \tTraining Loss: 0.007490 \tValidation Loss: 0.026520\n","Epoch: 80 \tTraining Loss: 0.006425 \tValidation Loss: 0.025904\n","Epoch: 81 \tTraining Loss: 0.006283 \tValidation Loss: 0.026001\n","Epoch: 82 \tTraining Loss: 0.005987 \tValidation Loss: 0.026530\n","Epoch: 83 \tTraining Loss: 0.006590 \tValidation Loss: 0.026734\n","Epoch: 84 \tTraining Loss: 0.006079 \tValidation Loss: 0.025093\n","Epoch: 85 \tTraining Loss: 0.006018 \tValidation Loss: 0.026068\n","Epoch: 86 \tTraining Loss: 0.006172 \tValidation Loss: 0.026075\n","Epoch: 87 \tTraining Loss: 0.006624 \tValidation Loss: 0.026837\n","Epoch: 88 \tTraining Loss: 0.006812 \tValidation Loss: 0.025616\n","Epoch: 89 \tTraining Loss: 0.006093 \tValidation Loss: 0.026143\n","Epoch: 90 \tTraining Loss: 0.005944 \tValidation Loss: 0.025488\n","Epoch: 91 \tTraining Loss: 0.005764 \tValidation Loss: 0.025697\n","Epoch: 92 \tTraining Loss: 0.005647 \tValidation Loss: 0.025854\n","Epoch: 93 \tTraining Loss: 0.005716 \tValidation Loss: 0.026093\n","Epoch: 94 \tTraining Loss: 0.005430 \tValidation Loss: 0.025371\n","Epoch: 95 \tTraining Loss: 0.005548 \tValidation Loss: 0.025978\n","Epoch: 96 \tTraining Loss: 0.005465 \tValidation Loss: 0.025981\n","Epoch: 97 \tTraining Loss: 0.005615 \tValidation Loss: 0.025630\n","Epoch: 98 \tTraining Loss: 0.005331 \tValidation Loss: 0.025888\n","Epoch: 99 \tTraining Loss: 0.005539 \tValidation Loss: 0.025913\n","Epoch: 100 \tTraining Loss: 0.005910 \tValidation Loss: 0.025361\n","Epoch: 101 \tTraining Loss: 0.006129 \tValidation Loss: 0.025526\n","Epoch: 102 \tTraining Loss: 0.005616 \tValidation Loss: 0.026153\n","Epoch: 103 \tTraining Loss: 0.005571 \tValidation Loss: 0.025316\n","Epoch: 104 \tTraining Loss: 0.005529 \tValidation Loss: 0.025633\n","Epoch: 105 \tTraining Loss: 0.005638 \tValidation Loss: 0.025500\n","Epoch: 106 \tTraining Loss: 0.005625 \tValidation Loss: 0.026443\n","Epoch: 107 \tTraining Loss: 0.005263 \tValidation Loss: 0.025566\n","Epoch: 108 \tTraining Loss: 0.005659 \tValidation Loss: 0.026006\n","Epoch: 109 \tTraining Loss: 0.005741 \tValidation Loss: 0.025546\n","Epoch: 110 \tTraining Loss: 0.005273 \tValidation Loss: 0.025631\n","Epoch: 111 \tTraining Loss: 0.005453 \tValidation Loss: 0.025854\n","Epoch: 112 \tTraining Loss: 0.005485 \tValidation Loss: 0.025595\n","Epoch: 113 \tTraining Loss: 0.005421 \tValidation Loss: 0.025194\n","Epoch: 114 \tTraining Loss: 0.005182 \tValidation Loss: 0.025882\n","Epoch: 115 \tTraining Loss: 0.005799 \tValidation Loss: 0.026454\n","Epoch: 116 \tTraining Loss: 0.006217 \tValidation Loss: 0.025781\n","Epoch: 117 \tTraining Loss: 0.005303 \tValidation Loss: 0.025701\n","Epoch: 118 \tTraining Loss: 0.005983 \tValidation Loss: 0.025727\n","Epoch: 119 \tTraining Loss: 0.005161 \tValidation Loss: 0.025301\n","Epoch: 120 \tTraining Loss: 0.005301 \tValidation Loss: 0.026612\n","Epoch: 121 \tTraining Loss: 0.005403 \tValidation Loss: 0.024850\n","Epoch: 122 \tTraining Loss: 0.005366 \tValidation Loss: 0.025886\n","Epoch: 123 \tTraining Loss: 0.004917 \tValidation Loss: 0.025664\n","Epoch: 124 \tTraining Loss: 0.004953 \tValidation Loss: 0.025938\n","Epoch: 125 \tTraining Loss: 0.004623 \tValidation Loss: 0.025385\n","Epoch: 126 \tTraining Loss: 0.004801 \tValidation Loss: 0.025411\n","Epoch: 127 \tTraining Loss: 0.004881 \tValidation Loss: 0.025768\n","Epoch: 128 \tTraining Loss: 0.004871 \tValidation Loss: 0.025319\n","Epoch: 129 \tTraining Loss: 0.004922 \tValidation Loss: 0.025305\n","Epoch: 130 \tTraining Loss: 0.004827 \tValidation Loss: 0.025157\n","Epoch: 131 \tTraining Loss: 0.004965 \tValidation Loss: 0.026310\n","Epoch: 132 \tTraining Loss: 0.004938 \tValidation Loss: 0.025407\n","Epoch: 133 \tTraining Loss: 0.005098 \tValidation Loss: 0.025640\n","Epoch: 134 \tTraining Loss: 0.005146 \tValidation Loss: 0.024982\n","Epoch: 135 \tTraining Loss: 0.004945 \tValidation Loss: 0.025560\n","Epoch: 136 \tTraining Loss: 0.004971 \tValidation Loss: 0.025190\n","Epoch: 137 \tTraining Loss: 0.005086 \tValidation Loss: 0.025940\n","Epoch: 138 \tTraining Loss: 0.004578 \tValidation Loss: 0.026129\n","Epoch: 139 \tTraining Loss: 0.005310 \tValidation Loss: 0.024837\n","Epoch: 140 \tTraining Loss: 0.004779 \tValidation Loss: 0.024874\n","Epoch: 141 \tTraining Loss: 0.004646 \tValidation Loss: 0.025153\n","Epoch: 142 \tTraining Loss: 0.004738 \tValidation Loss: 0.025328\n","Epoch: 143 \tTraining Loss: 0.005437 \tValidation Loss: 0.025390\n","Epoch: 144 \tTraining Loss: 0.005417 \tValidation Loss: 0.025294\n","Epoch: 145 \tTraining Loss: 0.004942 \tValidation Loss: 0.025217\n","Epoch: 146 \tTraining Loss: 0.005118 \tValidation Loss: 0.025152\n","Epoch: 147 \tTraining Loss: 0.004737 \tValidation Loss: 0.025496\n","Epoch: 148 \tTraining Loss: 0.005138 \tValidation Loss: 0.025117\n","Epoch: 149 \tTraining Loss: 0.005291 \tValidation Loss: 0.025346\n","Epoch: 150 \tTraining Loss: 0.004830 \tValidation Loss: 0.024816\n","Epoch: 151 \tTraining Loss: 0.004544 \tValidation Loss: 0.025084\n","Epoch: 152 \tTraining Loss: 0.004792 \tValidation Loss: 0.025198\n","Epoch: 153 \tTraining Loss: 0.004690 \tValidation Loss: 0.025674\n","Epoch: 154 \tTraining Loss: 0.004649 \tValidation Loss: 0.025105\n","Epoch: 155 \tTraining Loss: 0.004576 \tValidation Loss: 0.025057\n","Epoch: 156 \tTraining Loss: 0.004476 \tValidation Loss: 0.024944\n","Epoch: 157 \tTraining Loss: 0.004643 \tValidation Loss: 0.025338\n","Epoch: 158 \tTraining Loss: 0.004481 \tValidation Loss: 0.025228\n","Epoch: 159 \tTraining Loss: 0.004626 \tValidation Loss: 0.024804\n","Epoch: 160 \tTraining Loss: 0.004551 \tValidation Loss: 0.025028\n","Epoch: 161 \tTraining Loss: 0.004862 \tValidation Loss: 0.025033\n","Epoch: 162 \tTraining Loss: 0.005049 \tValidation Loss: 0.025789\n","Epoch: 163 \tTraining Loss: 0.004959 \tValidation Loss: 0.025263\n","Epoch: 164 \tTraining Loss: 0.004604 \tValidation Loss: 0.024993\n","Epoch: 165 \tTraining Loss: 0.004740 \tValidation Loss: 0.024929\n","Epoch: 166 \tTraining Loss: 0.004417 \tValidation Loss: 0.024596\n","Epoch: 167 \tTraining Loss: 0.004753 \tValidation Loss: 0.025294\n","Epoch: 168 \tTraining Loss: 0.004514 \tValidation Loss: 0.024827\n","Epoch: 169 \tTraining Loss: 0.005149 \tValidation Loss: 0.026529\n","Epoch: 170 \tTraining Loss: 0.007096 \tValidation Loss: 0.024740\n","Epoch: 171 \tTraining Loss: 0.006703 \tValidation Loss: 0.024891\n","Epoch: 172 \tTraining Loss: 0.005452 \tValidation Loss: 0.025117\n","Epoch: 173 \tTraining Loss: 0.004829 \tValidation Loss: 0.024507\n","Epoch: 174 \tTraining Loss: 0.004525 \tValidation Loss: 0.025928\n","Epoch: 175 \tTraining Loss: 0.004799 \tValidation Loss: 0.025566\n","Epoch: 176 \tTraining Loss: 0.004921 \tValidation Loss: 0.024888\n","Epoch: 177 \tTraining Loss: 0.005042 \tValidation Loss: 0.025930\n","Epoch: 178 \tTraining Loss: 0.004515 \tValidation Loss: 0.025065\n","Epoch: 179 \tTraining Loss: 0.003918 \tValidation Loss: 0.024915\n","Epoch: 180 \tTraining Loss: 0.003934 \tValidation Loss: 0.024804\n","Epoch: 181 \tTraining Loss: 0.004306 \tValidation Loss: 0.024877\n","Epoch: 182 \tTraining Loss: 0.004539 \tValidation Loss: 0.024841\n","Epoch: 183 \tTraining Loss: 0.004605 \tValidation Loss: 0.025013\n","Epoch: 184 \tTraining Loss: 0.004844 \tValidation Loss: 0.024607\n","Epoch: 185 \tTraining Loss: 0.004503 \tValidation Loss: 0.025096\n","Epoch: 186 \tTraining Loss: 0.004113 \tValidation Loss: 0.025007\n","Epoch: 187 \tTraining Loss: 0.003934 \tValidation Loss: 0.025371\n","Epoch: 188 \tTraining Loss: 0.004036 \tValidation Loss: 0.025196\n","Epoch: 189 \tTraining Loss: 0.004135 \tValidation Loss: 0.024824\n","Epoch: 190 \tTraining Loss: 0.003991 \tValidation Loss: 0.024979\n","Epoch: 191 \tTraining Loss: 0.003827 \tValidation Loss: 0.024590\n","Epoch: 192 \tTraining Loss: 0.003743 \tValidation Loss: 0.025258\n","Epoch: 193 \tTraining Loss: 0.003796 \tValidation Loss: 0.024574\n","Epoch: 194 \tTraining Loss: 0.003852 \tValidation Loss: 0.025007\n","Epoch: 195 \tTraining Loss: 0.004145 \tValidation Loss: 0.024978\n","Epoch: 196 \tTraining Loss: 0.003905 \tValidation Loss: 0.024658\n","Epoch: 197 \tTraining Loss: 0.003689 \tValidation Loss: 0.024585\n","Epoch: 198 \tTraining Loss: 0.003786 \tValidation Loss: 0.024875\n","Epoch: 199 \tTraining Loss: 0.003590 \tValidation Loss: 0.024906\n","Epoch: 200 \tTraining Loss: 0.003772 \tValidation Loss: 0.025085\n","Epoch: 201 \tTraining Loss: 0.003778 \tValidation Loss: 0.024891\n","Epoch: 202 \tTraining Loss: 0.003777 \tValidation Loss: 0.024614\n","Epoch: 203 \tTraining Loss: 0.003840 \tValidation Loss: 0.024403\n","Epoch: 204 \tTraining Loss: 0.003772 \tValidation Loss: 0.024888\n","Epoch: 205 \tTraining Loss: 0.004138 \tValidation Loss: 0.024893\n","Epoch: 206 \tTraining Loss: 0.004041 \tValidation Loss: 0.025022\n","Epoch: 207 \tTraining Loss: 0.003603 \tValidation Loss: 0.024710\n","Epoch: 208 \tTraining Loss: 0.003765 \tValidation Loss: 0.024928\n","Epoch: 209 \tTraining Loss: 0.003758 \tValidation Loss: 0.024919\n","Epoch: 210 \tTraining Loss: 0.003671 \tValidation Loss: 0.024515\n","Epoch: 211 \tTraining Loss: 0.003594 \tValidation Loss: 0.024593\n","Epoch: 212 \tTraining Loss: 0.004070 \tValidation Loss: 0.025327\n","Epoch: 213 \tTraining Loss: 0.004078 \tValidation Loss: 0.025137\n","Epoch: 214 \tTraining Loss: 0.004185 \tValidation Loss: 0.025067\n","Epoch: 215 \tTraining Loss: 0.003868 \tValidation Loss: 0.024492\n","Epoch: 216 \tTraining Loss: 0.003810 \tValidation Loss: 0.024761\n","Epoch: 217 \tTraining Loss: 0.004106 \tValidation Loss: 0.024800\n","Epoch: 218 \tTraining Loss: 0.004082 \tValidation Loss: 0.024370\n","Epoch: 219 \tTraining Loss: 0.003717 \tValidation Loss: 0.024692\n","Epoch: 220 \tTraining Loss: 0.003480 \tValidation Loss: 0.024234\n","Epoch: 221 \tTraining Loss: 0.003873 \tValidation Loss: 0.024523\n","Epoch: 222 \tTraining Loss: 0.003737 \tValidation Loss: 0.024404\n","Epoch: 223 \tTraining Loss: 0.003745 \tValidation Loss: 0.024255\n","Epoch: 224 \tTraining Loss: 0.004150 \tValidation Loss: 0.024256\n","Epoch: 225 \tTraining Loss: 0.005012 \tValidation Loss: 0.024932\n","Epoch: 226 \tTraining Loss: 0.004671 \tValidation Loss: 0.024973\n","Epoch: 227 \tTraining Loss: 0.004417 \tValidation Loss: 0.024721\n","Epoch: 228 \tTraining Loss: 0.004284 \tValidation Loss: 0.024725\n","Epoch: 229 \tTraining Loss: 0.004082 \tValidation Loss: 0.024706\n","Epoch: 230 \tTraining Loss: 0.003771 \tValidation Loss: 0.024834\n","Epoch: 231 \tTraining Loss: 0.004016 \tValidation Loss: 0.024660\n","Epoch: 232 \tTraining Loss: 0.003855 \tValidation Loss: 0.024506\n","Epoch: 233 \tTraining Loss: 0.003745 \tValidation Loss: 0.024978\n","Epoch: 234 \tTraining Loss: 0.004096 \tValidation Loss: 0.024355\n","Epoch: 235 \tTraining Loss: 0.003699 \tValidation Loss: 0.024629\n","Epoch: 236 \tTraining Loss: 0.003421 \tValidation Loss: 0.024813\n","Epoch: 237 \tTraining Loss: 0.003573 \tValidation Loss: 0.024632\n","Epoch: 238 \tTraining Loss: 0.003727 \tValidation Loss: 0.024496\n","Epoch: 239 \tTraining Loss: 0.003778 \tValidation Loss: 0.024636\n","Epoch: 240 \tTraining Loss: 0.004000 \tValidation Loss: 0.024649\n","Epoch: 241 \tTraining Loss: 0.004091 \tValidation Loss: 0.024765\n","Epoch: 242 \tTraining Loss: 0.003665 \tValidation Loss: 0.024653\n","Epoch: 243 \tTraining Loss: 0.003857 \tValidation Loss: 0.024222\n","Epoch: 244 \tTraining Loss: 0.003874 \tValidation Loss: 0.024814\n","Epoch: 245 \tTraining Loss: 0.004124 \tValidation Loss: 0.024436\n","Epoch: 246 \tTraining Loss: 0.003815 \tValidation Loss: 0.025001\n","Epoch: 247 \tTraining Loss: 0.003812 \tValidation Loss: 0.024531\n","Epoch: 248 \tTraining Loss: 0.003392 \tValidation Loss: 0.024397\n","Epoch: 249 \tTraining Loss: 0.003362 \tValidation Loss: 0.024752\n","Epoch: 250 \tTraining Loss: 0.003201 \tValidation Loss: 0.024764\n","Epoch: 251 \tTraining Loss: 0.003447 \tValidation Loss: 0.024796\n","Epoch: 252 \tTraining Loss: 0.003548 \tValidation Loss: 0.025020\n","Epoch: 253 \tTraining Loss: 0.003421 \tValidation Loss: 0.024605\n","Epoch: 254 \tTraining Loss: 0.003239 \tValidation Loss: 0.024410\n","Epoch: 255 \tTraining Loss: 0.003357 \tValidation Loss: 0.024663\n","Epoch: 256 \tTraining Loss: 0.003432 \tValidation Loss: 0.024830\n","Epoch: 257 \tTraining Loss: 0.003539 \tValidation Loss: 0.024390\n","Epoch: 258 \tTraining Loss: 0.003631 \tValidation Loss: 0.024304\n","Epoch: 259 \tTraining Loss: 0.003654 \tValidation Loss: 0.024771\n","Epoch: 260 \tTraining Loss: 0.003628 \tValidation Loss: 0.024721\n","Epoch: 261 \tTraining Loss: 0.003773 \tValidation Loss: 0.024693\n","Epoch: 262 \tTraining Loss: 0.003959 \tValidation Loss: 0.024690\n","Epoch: 263 \tTraining Loss: 0.003914 \tValidation Loss: 0.024801\n","Epoch: 264 \tTraining Loss: 0.003834 \tValidation Loss: 0.024472\n","Epoch: 265 \tTraining Loss: 0.003805 \tValidation Loss: 0.024563\n","Epoch: 266 \tTraining Loss: 0.003290 \tValidation Loss: 0.024437\n","Epoch: 267 \tTraining Loss: 0.003385 \tValidation Loss: 0.024837\n","Epoch: 268 \tTraining Loss: 0.003544 \tValidation Loss: 0.024530\n","Epoch: 269 \tTraining Loss: 0.003549 \tValidation Loss: 0.024569\n","Epoch: 270 \tTraining Loss: 0.003649 \tValidation Loss: 0.024306\n","Epoch: 271 \tTraining Loss: 0.003973 \tValidation Loss: 0.024453\n","Epoch: 272 \tTraining Loss: 0.003726 \tValidation Loss: 0.024613\n","Epoch: 273 \tTraining Loss: 0.003702 \tValidation Loss: 0.024134\n","Epoch: 274 \tTraining Loss: 0.003764 \tValidation Loss: 0.024798\n","Epoch: 275 \tTraining Loss: 0.003936 \tValidation Loss: 0.024452\n","Epoch: 276 \tTraining Loss: 0.003929 \tValidation Loss: 0.024633\n","Epoch: 277 \tTraining Loss: 0.003812 \tValidation Loss: 0.024621\n","Epoch: 278 \tTraining Loss: 0.003295 \tValidation Loss: 0.024434\n","Epoch: 279 \tTraining Loss: 0.003115 \tValidation Loss: 0.024313\n","Epoch: 280 \tTraining Loss: 0.003092 \tValidation Loss: 0.024653\n","Epoch: 281 \tTraining Loss: 0.003188 \tValidation Loss: 0.024071\n","Epoch: 282 \tTraining Loss: 0.003220 \tValidation Loss: 0.024240\n","Epoch: 283 \tTraining Loss: 0.003245 \tValidation Loss: 0.024574\n","Epoch: 284 \tTraining Loss: 0.003201 \tValidation Loss: 0.024714\n","Epoch: 285 \tTraining Loss: 0.003274 \tValidation Loss: 0.024216\n","Epoch: 286 \tTraining Loss: 0.003454 \tValidation Loss: 0.024702\n","Epoch: 287 \tTraining Loss: 0.003624 \tValidation Loss: 0.024766\n","Epoch: 288 \tTraining Loss: 0.003415 \tValidation Loss: 0.024312\n","Epoch: 289 \tTraining Loss: 0.003205 \tValidation Loss: 0.025033\n","Epoch: 290 \tTraining Loss: 0.003296 \tValidation Loss: 0.024273\n","Epoch: 291 \tTraining Loss: 0.003699 \tValidation Loss: 0.024459\n","Epoch: 292 \tTraining Loss: 0.003607 \tValidation Loss: 0.024222\n","Epoch: 293 \tTraining Loss: 0.003324 \tValidation Loss: 0.024514\n","Epoch: 294 \tTraining Loss: 0.003233 \tValidation Loss: 0.024427\n","Epoch: 295 \tTraining Loss: 0.003327 \tValidation Loss: 0.024384\n","Epoch: 296 \tTraining Loss: 0.003550 \tValidation Loss: 0.024408\n","Epoch: 297 \tTraining Loss: 0.003332 \tValidation Loss: 0.024867\n","Epoch: 298 \tTraining Loss: 0.003290 \tValidation Loss: 0.024433\n","Epoch: 299 \tTraining Loss: 0.003537 \tValidation Loss: 0.024535\n","Epoch: 300 \tTraining Loss: 0.003835 \tValidation Loss: 0.024183\n","Epoch: 301 \tTraining Loss: 0.003562 \tValidation Loss: 0.024238\n","Epoch: 302 \tTraining Loss: 0.003327 \tValidation Loss: 0.024475\n","Epoch: 303 \tTraining Loss: 0.003669 \tValidation Loss: 0.024485\n","Epoch: 304 \tTraining Loss: 0.003466 \tValidation Loss: 0.025059\n","Epoch: 305 \tTraining Loss: 0.003567 \tValidation Loss: 0.024890\n","Epoch: 306 \tTraining Loss: 0.004004 \tValidation Loss: 0.024280\n","Epoch: 307 \tTraining Loss: 0.003514 \tValidation Loss: 0.024452\n","Epoch: 308 \tTraining Loss: 0.003267 \tValidation Loss: 0.024399\n","Epoch: 309 \tTraining Loss: 0.003114 \tValidation Loss: 0.024344\n","Epoch: 310 \tTraining Loss: 0.003350 \tValidation Loss: 0.024235\n","Epoch: 311 \tTraining Loss: 0.003534 \tValidation Loss: 0.024458\n","Epoch: 312 \tTraining Loss: 0.003597 \tValidation Loss: 0.024243\n","Epoch: 313 \tTraining Loss: 0.003478 \tValidation Loss: 0.024631\n","Epoch: 314 \tTraining Loss: 0.003167 \tValidation Loss: 0.024811\n","Epoch: 315 \tTraining Loss: 0.003144 \tValidation Loss: 0.024128\n","Epoch: 316 \tTraining Loss: 0.002991 \tValidation Loss: 0.024292\n","Epoch: 317 \tTraining Loss: 0.003031 \tValidation Loss: 0.024344\n","Epoch: 318 \tTraining Loss: 0.003157 \tValidation Loss: 0.024419\n","Epoch: 319 \tTraining Loss: 0.003075 \tValidation Loss: 0.024167\n","Epoch: 320 \tTraining Loss: 0.003618 \tValidation Loss: 0.024842\n","Epoch: 321 \tTraining Loss: 0.003830 \tValidation Loss: 0.024090\n","Epoch: 322 \tTraining Loss: 0.004270 \tValidation Loss: 0.024288\n","Epoch: 323 \tTraining Loss: 0.004686 \tValidation Loss: 0.024727\n","Epoch: 324 \tTraining Loss: 0.004924 \tValidation Loss: 0.024715\n","Epoch: 325 \tTraining Loss: 0.004501 \tValidation Loss: 0.024521\n","Epoch: 326 \tTraining Loss: 0.003954 \tValidation Loss: 0.023951\n","Epoch: 327 \tTraining Loss: 0.003860 \tValidation Loss: 0.024360\n","Epoch: 328 \tTraining Loss: 0.003568 \tValidation Loss: 0.024302\n","Epoch: 329 \tTraining Loss: 0.003404 \tValidation Loss: 0.024492\n","Epoch: 330 \tTraining Loss: 0.003290 \tValidation Loss: 0.024324\n","Epoch: 331 \tTraining Loss: 0.003233 \tValidation Loss: 0.024313\n","Epoch: 332 \tTraining Loss: 0.003028 \tValidation Loss: 0.024152\n","Epoch: 333 \tTraining Loss: 0.002856 \tValidation Loss: 0.024326\n","Epoch: 334 \tTraining Loss: 0.002787 \tValidation Loss: 0.024202\n","Epoch: 335 \tTraining Loss: 0.003097 \tValidation Loss: 0.024440\n","Epoch: 336 \tTraining Loss: 0.003203 \tValidation Loss: 0.024642\n","Epoch: 337 \tTraining Loss: 0.002939 \tValidation Loss: 0.023958\n","Epoch: 338 \tTraining Loss: 0.002693 \tValidation Loss: 0.024686\n","Epoch: 339 \tTraining Loss: 0.003121 \tValidation Loss: 0.024123\n","Epoch: 340 \tTraining Loss: 0.003192 \tValidation Loss: 0.024496\n","Epoch: 341 \tTraining Loss: 0.003180 \tValidation Loss: 0.024250\n","Epoch: 342 \tTraining Loss: 0.003016 \tValidation Loss: 0.024505\n","Epoch: 343 \tTraining Loss: 0.003028 \tValidation Loss: 0.024246\n","Epoch: 344 \tTraining Loss: 0.002843 \tValidation Loss: 0.024058\n","Epoch: 345 \tTraining Loss: 0.003406 \tValidation Loss: 0.024286\n","Epoch: 346 \tTraining Loss: 0.003464 \tValidation Loss: 0.024116\n","Epoch: 347 \tTraining Loss: 0.003566 \tValidation Loss: 0.024096\n","Epoch: 348 \tTraining Loss: 0.003416 \tValidation Loss: 0.024476\n","Epoch: 349 \tTraining Loss: 0.003061 \tValidation Loss: 0.024575\n","Epoch: 350 \tTraining Loss: 0.002823 \tValidation Loss: 0.024403\n","Epoch: 351 \tTraining Loss: 0.002839 \tValidation Loss: 0.024208\n","Epoch: 352 \tTraining Loss: 0.003227 \tValidation Loss: 0.024110\n","Epoch: 353 \tTraining Loss: 0.003610 \tValidation Loss: 0.024658\n","Epoch: 354 \tTraining Loss: 0.003771 \tValidation Loss: 0.024250\n","Epoch: 355 \tTraining Loss: 0.003510 \tValidation Loss: 0.024428\n","Epoch: 356 \tTraining Loss: 0.003321 \tValidation Loss: 0.024044\n","Epoch: 357 \tTraining Loss: 0.003130 \tValidation Loss: 0.024190\n","Epoch: 358 \tTraining Loss: 0.002914 \tValidation Loss: 0.023995\n","Epoch: 359 \tTraining Loss: 0.003135 \tValidation Loss: 0.024387\n","Epoch: 360 \tTraining Loss: 0.003005 \tValidation Loss: 0.024167\n","Epoch: 361 \tTraining Loss: 0.003386 \tValidation Loss: 0.023963\n","Epoch: 362 \tTraining Loss: 0.003165 \tValidation Loss: 0.024407\n","Epoch: 363 \tTraining Loss: 0.003226 \tValidation Loss: 0.023945\n","Epoch: 364 \tTraining Loss: 0.003074 \tValidation Loss: 0.024126\n","Epoch: 365 \tTraining Loss: 0.002780 \tValidation Loss: 0.024672\n","Epoch: 366 \tTraining Loss: 0.002906 \tValidation Loss: 0.024322\n","Epoch: 367 \tTraining Loss: 0.002927 \tValidation Loss: 0.024426\n","Epoch: 368 \tTraining Loss: 0.002710 \tValidation Loss: 0.024197\n","Epoch: 369 \tTraining Loss: 0.002922 \tValidation Loss: 0.024577\n","Epoch: 370 \tTraining Loss: 0.002670 \tValidation Loss: 0.024097\n","Epoch: 371 \tTraining Loss: 0.002717 \tValidation Loss: 0.024055\n","Epoch: 372 \tTraining Loss: 0.002839 \tValidation Loss: 0.024477\n","Epoch: 373 \tTraining Loss: 0.002988 \tValidation Loss: 0.023882\n","Epoch: 374 \tTraining Loss: 0.003163 \tValidation Loss: 0.024081\n","Epoch: 375 \tTraining Loss: 0.003362 \tValidation Loss: 0.023880\n","Epoch: 376 \tTraining Loss: 0.003508 \tValidation Loss: 0.024493\n","Epoch: 377 \tTraining Loss: 0.003291 \tValidation Loss: 0.023954\n","Epoch: 378 \tTraining Loss: 0.003395 \tValidation Loss: 0.024136\n","Epoch: 379 \tTraining Loss: 0.003024 \tValidation Loss: 0.024261\n","Epoch: 380 \tTraining Loss: 0.002931 \tValidation Loss: 0.024150\n","Epoch: 381 \tTraining Loss: 0.002758 \tValidation Loss: 0.024210\n","Epoch: 382 \tTraining Loss: 0.002833 \tValidation Loss: 0.024429\n","Epoch: 383 \tTraining Loss: 0.002749 \tValidation Loss: 0.024283\n","Epoch: 384 \tTraining Loss: 0.002681 \tValidation Loss: 0.024267\n","Epoch: 385 \tTraining Loss: 0.002691 \tValidation Loss: 0.024379\n","Epoch: 386 \tTraining Loss: 0.002760 \tValidation Loss: 0.024100\n","Epoch: 387 \tTraining Loss: 0.002664 \tValidation Loss: 0.024319\n","Epoch: 388 \tTraining Loss: 0.002603 \tValidation Loss: 0.023821\n","Epoch: 389 \tTraining Loss: 0.002633 \tValidation Loss: 0.024257\n","Epoch: 390 \tTraining Loss: 0.002891 \tValidation Loss: 0.023937\n","Epoch: 391 \tTraining Loss: 0.002817 \tValidation Loss: 0.024516\n","Epoch: 392 \tTraining Loss: 0.002822 \tValidation Loss: 0.024427\n","Epoch: 393 \tTraining Loss: 0.002874 \tValidation Loss: 0.024203\n","Epoch: 394 \tTraining Loss: 0.003055 \tValidation Loss: 0.024261\n","Epoch: 395 \tTraining Loss: 0.002795 \tValidation Loss: 0.024179\n","Epoch: 396 \tTraining Loss: 0.002744 \tValidation Loss: 0.024085\n","Epoch: 397 \tTraining Loss: 0.002680 \tValidation Loss: 0.023998\n","Epoch: 398 \tTraining Loss: 0.002817 \tValidation Loss: 0.024149\n","Epoch: 399 \tTraining Loss: 0.002652 \tValidation Loss: 0.023791\n","Validation loss decreased (0.023803 --> 0.023791).  Saving model ...\n","Epoch: 400 \tTraining Loss: 0.002603 \tValidation Loss: 0.024248\n","Epoch: 401 \tTraining Loss: 0.002547 \tValidation Loss: 0.024126\n","Epoch: 402 \tTraining Loss: 0.002506 \tValidation Loss: 0.023565\n","Validation loss decreased (0.023791 --> 0.023565).  Saving model ...\n","Epoch: 403 \tTraining Loss: 0.002651 \tValidation Loss: 0.024343\n","Epoch: 404 \tTraining Loss: 0.002647 \tValidation Loss: 0.024056\n","Epoch: 405 \tTraining Loss: 0.002601 \tValidation Loss: 0.024071\n","Epoch: 406 \tTraining Loss: 0.003190 \tValidation Loss: 0.024176\n","Epoch: 407 \tTraining Loss: 0.004154 \tValidation Loss: 0.024066\n","Epoch: 408 \tTraining Loss: 0.003706 \tValidation Loss: 0.024559\n","Epoch: 409 \tTraining Loss: 0.003292 \tValidation Loss: 0.024230\n","Epoch: 410 \tTraining Loss: 0.003002 \tValidation Loss: 0.024545\n","Epoch: 411 \tTraining Loss: 0.003181 \tValidation Loss: 0.024314\n","Epoch: 412 \tTraining Loss: 0.003068 \tValidation Loss: 0.024103\n","Epoch: 413 \tTraining Loss: 0.002952 \tValidation Loss: 0.024127\n","Epoch: 414 \tTraining Loss: 0.002836 \tValidation Loss: 0.024343\n","Epoch: 415 \tTraining Loss: 0.002991 \tValidation Loss: 0.023769\n","Epoch: 416 \tTraining Loss: 0.003117 \tValidation Loss: 0.023970\n","Epoch: 417 \tTraining Loss: 0.003213 \tValidation Loss: 0.023822\n","Epoch: 418 \tTraining Loss: 0.002863 \tValidation Loss: 0.024212\n","Epoch: 419 \tTraining Loss: 0.002768 \tValidation Loss: 0.023888\n","Epoch: 420 \tTraining Loss: 0.002887 \tValidation Loss: 0.023578\n","Epoch: 421 \tTraining Loss: 0.003011 \tValidation Loss: 0.024210\n","Epoch: 422 \tTraining Loss: 0.002851 \tValidation Loss: 0.024161\n","Epoch: 423 \tTraining Loss: 0.002768 \tValidation Loss: 0.023837\n","Epoch: 424 \tTraining Loss: 0.002830 \tValidation Loss: 0.024114\n","Epoch: 425 \tTraining Loss: 0.002646 \tValidation Loss: 0.023974\n","Epoch: 426 \tTraining Loss: 0.002560 \tValidation Loss: 0.024063\n","Epoch: 427 \tTraining Loss: 0.002456 \tValidation Loss: 0.023996\n","Epoch: 428 \tTraining Loss: 0.002810 \tValidation Loss: 0.024496\n","Epoch: 429 \tTraining Loss: 0.002873 \tValidation Loss: 0.024059\n","Epoch: 430 \tTraining Loss: 0.002820 \tValidation Loss: 0.024078\n","Epoch: 431 \tTraining Loss: 0.002622 \tValidation Loss: 0.024052\n","Epoch: 432 \tTraining Loss: 0.002761 \tValidation Loss: 0.023800\n","Epoch: 433 \tTraining Loss: 0.002703 \tValidation Loss: 0.024149\n","Epoch: 434 \tTraining Loss: 0.002605 \tValidation Loss: 0.024152\n","Epoch: 435 \tTraining Loss: 0.002521 \tValidation Loss: 0.024061\n","Epoch: 436 \tTraining Loss: 0.002612 \tValidation Loss: 0.024004\n","Epoch: 437 \tTraining Loss: 0.002665 \tValidation Loss: 0.023719\n","Epoch: 438 \tTraining Loss: 0.002668 \tValidation Loss: 0.023973\n","Epoch: 439 \tTraining Loss: 0.002655 \tValidation Loss: 0.024012\n","Epoch: 440 \tTraining Loss: 0.002608 \tValidation Loss: 0.023889\n","Epoch: 441 \tTraining Loss: 0.002650 \tValidation Loss: 0.024708\n","Epoch: 442 \tTraining Loss: 0.002702 \tValidation Loss: 0.023924\n","Epoch: 443 \tTraining Loss: 0.002417 \tValidation Loss: 0.024158\n","Epoch: 444 \tTraining Loss: 0.002508 \tValidation Loss: 0.024269\n","Epoch: 445 \tTraining Loss: 0.002467 \tValidation Loss: 0.024119\n","Epoch: 446 \tTraining Loss: 0.002632 \tValidation Loss: 0.024251\n","Epoch: 447 \tTraining Loss: 0.002774 \tValidation Loss: 0.023946\n","Epoch: 448 \tTraining Loss: 0.002682 \tValidation Loss: 0.024432\n","Epoch: 449 \tTraining Loss: 0.002505 \tValidation Loss: 0.024161\n","Epoch: 450 \tTraining Loss: 0.002561 \tValidation Loss: 0.023943\n","Epoch: 451 \tTraining Loss: 0.002466 \tValidation Loss: 0.024183\n","Epoch: 452 \tTraining Loss: 0.002445 \tValidation Loss: 0.023844\n","Epoch: 453 \tTraining Loss: 0.002832 \tValidation Loss: 0.024242\n","Epoch: 454 \tTraining Loss: 0.003099 \tValidation Loss: 0.024108\n","Epoch: 455 \tTraining Loss: 0.002819 \tValidation Loss: 0.023793\n","Epoch: 456 \tTraining Loss: 0.002790 \tValidation Loss: 0.024144\n","Epoch: 457 \tTraining Loss: 0.002827 \tValidation Loss: 0.024424\n","Epoch: 458 \tTraining Loss: 0.003055 \tValidation Loss: 0.024100\n","Epoch: 459 \tTraining Loss: 0.003318 \tValidation Loss: 0.024135\n","Epoch: 460 \tTraining Loss: 0.003437 \tValidation Loss: 0.023954\n","Epoch: 461 \tTraining Loss: 0.003460 \tValidation Loss: 0.023836\n","Epoch: 462 \tTraining Loss: 0.002887 \tValidation Loss: 0.024517\n","Epoch: 463 \tTraining Loss: 0.003074 \tValidation Loss: 0.023798\n","Epoch: 464 \tTraining Loss: 0.002789 \tValidation Loss: 0.023900\n","Epoch: 465 \tTraining Loss: 0.003025 \tValidation Loss: 0.024283\n","Epoch: 466 \tTraining Loss: 0.003245 \tValidation Loss: 0.024310\n","Epoch: 467 \tTraining Loss: 0.003161 \tValidation Loss: 0.024023\n","Epoch: 468 \tTraining Loss: 0.002830 \tValidation Loss: 0.023953\n","Epoch: 469 \tTraining Loss: 0.002765 \tValidation Loss: 0.023776\n","Epoch: 470 \tTraining Loss: 0.002631 \tValidation Loss: 0.023501\n","Validation loss decreased (0.023565 --> 0.023501).  Saving model ...\n","Epoch: 471 \tTraining Loss: 0.002586 \tValidation Loss: 0.023954\n","Epoch: 472 \tTraining Loss: 0.002499 \tValidation Loss: 0.023832\n","Epoch: 473 \tTraining Loss: 0.002517 \tValidation Loss: 0.023887\n","Epoch: 474 \tTraining Loss: 0.002419 \tValidation Loss: 0.024282\n","Epoch: 475 \tTraining Loss: 0.002772 \tValidation Loss: 0.024098\n","Epoch: 476 \tTraining Loss: 0.002710 \tValidation Loss: 0.024232\n","Epoch: 477 \tTraining Loss: 0.002726 \tValidation Loss: 0.023953\n","Epoch: 478 \tTraining Loss: 0.002560 \tValidation Loss: 0.023911\n","Epoch: 479 \tTraining Loss: 0.002239 \tValidation Loss: 0.024173\n","Epoch: 480 \tTraining Loss: 0.002394 \tValidation Loss: 0.024203\n","Epoch: 481 \tTraining Loss: 0.002404 \tValidation Loss: 0.024027\n","Epoch: 482 \tTraining Loss: 0.002373 \tValidation Loss: 0.023868\n","Epoch: 483 \tTraining Loss: 0.002412 \tValidation Loss: 0.023782\n","Epoch: 484 \tTraining Loss: 0.002513 \tValidation Loss: 0.024448\n","Epoch: 485 \tTraining Loss: 0.002311 \tValidation Loss: 0.023784\n","Epoch: 486 \tTraining Loss: 0.002332 \tValidation Loss: 0.024084\n","Epoch: 487 \tTraining Loss: 0.002321 \tValidation Loss: 0.023983\n","Epoch: 488 \tTraining Loss: 0.002272 \tValidation Loss: 0.023923\n","Epoch: 489 \tTraining Loss: 0.002220 \tValidation Loss: 0.024056\n","Epoch: 490 \tTraining Loss: 0.002613 \tValidation Loss: 0.023923\n","Epoch: 491 \tTraining Loss: 0.002827 \tValidation Loss: 0.024596\n","Epoch: 492 \tTraining Loss: 0.002735 \tValidation Loss: 0.024007\n","Epoch: 493 \tTraining Loss: 0.002833 \tValidation Loss: 0.023535\n","Epoch: 494 \tTraining Loss: 0.002798 \tValidation Loss: 0.023797\n","Epoch: 495 \tTraining Loss: 0.002626 \tValidation Loss: 0.023925\n","Epoch: 496 \tTraining Loss: 0.002443 \tValidation Loss: 0.024127\n","Epoch: 497 \tTraining Loss: 0.002443 \tValidation Loss: 0.024023\n","Epoch: 498 \tTraining Loss: 0.002390 \tValidation Loss: 0.024316\n","Epoch: 499 \tTraining Loss: 0.002344 \tValidation Loss: 0.023929\n","Epoch: 500 \tTraining Loss: 0.002481 \tValidation Loss: 0.023747\n","Epoch: 501 \tTraining Loss: 0.003204 \tValidation Loss: 0.023916\n","Epoch: 502 \tTraining Loss: 0.003530 \tValidation Loss: 0.023621\n","Epoch: 503 \tTraining Loss: 0.003153 \tValidation Loss: 0.024008\n","Epoch: 504 \tTraining Loss: 0.003044 \tValidation Loss: 0.023531\n","Epoch: 505 \tTraining Loss: 0.002709 \tValidation Loss: 0.023982\n","Epoch: 506 \tTraining Loss: 0.002689 \tValidation Loss: 0.024066\n","Epoch: 507 \tTraining Loss: 0.002694 \tValidation Loss: 0.023658\n","Epoch: 508 \tTraining Loss: 0.002323 \tValidation Loss: 0.023992\n","Epoch: 509 \tTraining Loss: 0.002244 \tValidation Loss: 0.023915\n","Epoch: 510 \tTraining Loss: 0.002469 \tValidation Loss: 0.024165\n","Epoch: 511 \tTraining Loss: 0.002620 \tValidation Loss: 0.023968\n","Epoch: 512 \tTraining Loss: 0.002418 \tValidation Loss: 0.024081\n","Epoch: 513 \tTraining Loss: 0.002655 \tValidation Loss: 0.024214\n","Epoch: 514 \tTraining Loss: 0.002796 \tValidation Loss: 0.023650\n","Epoch: 515 \tTraining Loss: 0.002853 \tValidation Loss: 0.023935\n","Epoch: 516 \tTraining Loss: 0.002650 \tValidation Loss: 0.023684\n","Epoch: 517 \tTraining Loss: 0.002351 \tValidation Loss: 0.024015\n","Epoch: 518 \tTraining Loss: 0.002270 \tValidation Loss: 0.023772\n","Epoch: 519 \tTraining Loss: 0.002343 \tValidation Loss: 0.023961\n","Epoch: 520 \tTraining Loss: 0.002468 \tValidation Loss: 0.024179\n","Epoch: 521 \tTraining Loss: 0.002526 \tValidation Loss: 0.023733\n","Epoch: 522 \tTraining Loss: 0.002507 \tValidation Loss: 0.024149\n","Epoch: 523 \tTraining Loss: 0.002338 \tValidation Loss: 0.023726\n","Epoch: 524 \tTraining Loss: 0.002334 \tValidation Loss: 0.023950\n","Epoch: 525 \tTraining Loss: 0.002313 \tValidation Loss: 0.023763\n","Epoch: 526 \tTraining Loss: 0.002204 \tValidation Loss: 0.023934\n","Epoch: 527 \tTraining Loss: 0.002447 \tValidation Loss: 0.023633\n","Epoch: 528 \tTraining Loss: 0.002605 \tValidation Loss: 0.023917\n","Epoch: 529 \tTraining Loss: 0.002422 \tValidation Loss: 0.023551\n","Epoch: 530 \tTraining Loss: 0.002584 \tValidation Loss: 0.023993\n","Epoch: 531 \tTraining Loss: 0.002559 \tValidation Loss: 0.023734\n","Epoch: 532 \tTraining Loss: 0.002410 \tValidation Loss: 0.024067\n","Epoch: 533 \tTraining Loss: 0.002206 \tValidation Loss: 0.023706\n","Epoch: 534 \tTraining Loss: 0.002182 \tValidation Loss: 0.023657\n","Epoch: 535 \tTraining Loss: 0.002179 \tValidation Loss: 0.023955\n","Epoch: 536 \tTraining Loss: 0.002259 \tValidation Loss: 0.023868\n","Epoch: 537 \tTraining Loss: 0.002321 \tValidation Loss: 0.023752\n","Epoch: 538 \tTraining Loss: 0.002638 \tValidation Loss: 0.024243\n","Epoch: 539 \tTraining Loss: 0.002699 \tValidation Loss: 0.023685\n","Epoch: 540 \tTraining Loss: 0.002816 \tValidation Loss: 0.023906\n","Epoch: 541 \tTraining Loss: 0.002610 \tValidation Loss: 0.023505\n","Epoch: 542 \tTraining Loss: 0.002569 \tValidation Loss: 0.023349\n","Validation loss decreased (0.023501 --> 0.023349).  Saving model ...\n","Epoch: 543 \tTraining Loss: 0.002688 \tValidation Loss: 0.023786\n","Epoch: 544 \tTraining Loss: 0.002565 \tValidation Loss: 0.023770\n","Epoch: 545 \tTraining Loss: 0.002556 \tValidation Loss: 0.023760\n","Epoch: 546 \tTraining Loss: 0.002338 \tValidation Loss: 0.023946\n","Epoch: 547 \tTraining Loss: 0.002260 \tValidation Loss: 0.023488\n","Epoch: 548 \tTraining Loss: 0.002299 \tValidation Loss: 0.023935\n","Epoch: 549 \tTraining Loss: 0.002245 \tValidation Loss: 0.024073\n","Epoch: 550 \tTraining Loss: 0.002663 \tValidation Loss: 0.024157\n","Epoch: 551 \tTraining Loss: 0.002554 \tValidation Loss: 0.023718\n","Epoch: 552 \tTraining Loss: 0.002326 \tValidation Loss: 0.023727\n","Epoch: 553 \tTraining Loss: 0.002107 \tValidation Loss: 0.023537\n","Epoch: 554 \tTraining Loss: 0.002243 \tValidation Loss: 0.023696\n","Epoch: 555 \tTraining Loss: 0.002459 \tValidation Loss: 0.024011\n","Epoch: 556 \tTraining Loss: 0.002727 \tValidation Loss: 0.024016\n","Epoch: 557 \tTraining Loss: 0.002688 \tValidation Loss: 0.023963\n","Epoch: 558 \tTraining Loss: 0.002539 \tValidation Loss: 0.023936\n","Epoch: 559 \tTraining Loss: 0.002393 \tValidation Loss: 0.024052\n","Epoch: 560 \tTraining Loss: 0.002241 \tValidation Loss: 0.023951\n","Epoch: 561 \tTraining Loss: 0.002360 \tValidation Loss: 0.023895\n","Epoch: 562 \tTraining Loss: 0.002453 \tValidation Loss: 0.023738\n","Epoch: 563 \tTraining Loss: 0.002459 \tValidation Loss: 0.023658\n","Epoch: 564 \tTraining Loss: 0.002431 \tValidation Loss: 0.024035\n","Epoch: 565 \tTraining Loss: 0.002553 \tValidation Loss: 0.023820\n","Epoch: 566 \tTraining Loss: 0.002600 \tValidation Loss: 0.023901\n","Epoch: 567 \tTraining Loss: 0.002576 \tValidation Loss: 0.023736\n","Epoch: 568 \tTraining Loss: 0.002304 \tValidation Loss: 0.023717\n","Epoch: 569 \tTraining Loss: 0.002236 \tValidation Loss: 0.023991\n","Epoch: 570 \tTraining Loss: 0.002332 \tValidation Loss: 0.023918\n","Epoch: 571 \tTraining Loss: 0.002337 \tValidation Loss: 0.024184\n","Epoch: 572 \tTraining Loss: 0.002365 \tValidation Loss: 0.023937\n","Epoch: 573 \tTraining Loss: 0.002245 \tValidation Loss: 0.023814\n","Epoch: 574 \tTraining Loss: 0.002505 \tValidation Loss: 0.023671\n","Epoch: 575 \tTraining Loss: 0.002498 \tValidation Loss: 0.024114\n","Epoch: 576 \tTraining Loss: 0.002277 \tValidation Loss: 0.023878\n","Epoch: 577 \tTraining Loss: 0.002298 \tValidation Loss: 0.023758\n","Epoch: 578 \tTraining Loss: 0.002317 \tValidation Loss: 0.023742\n","Epoch: 579 \tTraining Loss: 0.002182 \tValidation Loss: 0.023436\n","Epoch: 580 \tTraining Loss: 0.002324 \tValidation Loss: 0.024317\n","Epoch: 581 \tTraining Loss: 0.002327 \tValidation Loss: 0.024028\n","Epoch: 582 \tTraining Loss: 0.002346 \tValidation Loss: 0.023872\n","Epoch: 583 \tTraining Loss: 0.002313 \tValidation Loss: 0.023895\n","Epoch: 584 \tTraining Loss: 0.002357 \tValidation Loss: 0.024221\n","Epoch: 585 \tTraining Loss: 0.002342 \tValidation Loss: 0.023799\n","Epoch: 586 \tTraining Loss: 0.002277 \tValidation Loss: 0.023837\n","Epoch: 587 \tTraining Loss: 0.002197 \tValidation Loss: 0.023787\n","Epoch: 588 \tTraining Loss: 0.002271 \tValidation Loss: 0.023991\n","Epoch: 589 \tTraining Loss: 0.002453 \tValidation Loss: 0.023693\n","Epoch: 590 \tTraining Loss: 0.002354 \tValidation Loss: 0.023934\n","Epoch: 591 \tTraining Loss: 0.002231 \tValidation Loss: 0.023620\n","Epoch: 592 \tTraining Loss: 0.002079 \tValidation Loss: 0.023601\n","Epoch: 593 \tTraining Loss: 0.002121 \tValidation Loss: 0.023812\n","Epoch: 594 \tTraining Loss: 0.002196 \tValidation Loss: 0.023695\n","Epoch: 595 \tTraining Loss: 0.002184 \tValidation Loss: 0.023709\n","Epoch: 596 \tTraining Loss: 0.002125 \tValidation Loss: 0.023614\n","Epoch: 597 \tTraining Loss: 0.002417 \tValidation Loss: 0.024076\n","Epoch: 598 \tTraining Loss: 0.002860 \tValidation Loss: 0.023761\n","Epoch: 599 \tTraining Loss: 0.002714 \tValidation Loss: 0.024037\n","Epoch: 600 \tTraining Loss: 0.002462 \tValidation Loss: 0.023506\n","Epoch: 601 \tTraining Loss: 0.002298 \tValidation Loss: 0.023811\n","Epoch: 602 \tTraining Loss: 0.002141 \tValidation Loss: 0.023398\n","Epoch: 603 \tTraining Loss: 0.002012 \tValidation Loss: 0.023650\n","Epoch: 604 \tTraining Loss: 0.001964 \tValidation Loss: 0.023486\n","Epoch: 605 \tTraining Loss: 0.002117 \tValidation Loss: 0.023796\n","Epoch: 606 \tTraining Loss: 0.002509 \tValidation Loss: 0.023987\n","Epoch: 607 \tTraining Loss: 0.002489 \tValidation Loss: 0.023379\n","Epoch: 608 \tTraining Loss: 0.002458 \tValidation Loss: 0.023717\n","Epoch: 609 \tTraining Loss: 0.002275 \tValidation Loss: 0.023581\n","Epoch: 610 \tTraining Loss: 0.002116 \tValidation Loss: 0.023832\n","Epoch: 611 \tTraining Loss: 0.002114 \tValidation Loss: 0.023541\n","Epoch: 612 \tTraining Loss: 0.002116 \tValidation Loss: 0.024044\n","Epoch: 613 \tTraining Loss: 0.002043 \tValidation Loss: 0.023727\n","Epoch: 614 \tTraining Loss: 0.002040 \tValidation Loss: 0.023803\n","Epoch: 615 \tTraining Loss: 0.002155 \tValidation Loss: 0.023558\n","Epoch: 616 \tTraining Loss: 0.002095 \tValidation Loss: 0.023768\n","Epoch: 617 \tTraining Loss: 0.002300 \tValidation Loss: 0.023773\n","Epoch: 618 \tTraining Loss: 0.002269 \tValidation Loss: 0.023956\n","Epoch: 619 \tTraining Loss: 0.002240 \tValidation Loss: 0.023732\n","Epoch: 620 \tTraining Loss: 0.002192 \tValidation Loss: 0.023700\n","Epoch: 621 \tTraining Loss: 0.002199 \tValidation Loss: 0.023831\n","Epoch: 622 \tTraining Loss: 0.002104 \tValidation Loss: 0.023929\n","Epoch: 623 \tTraining Loss: 0.002042 \tValidation Loss: 0.023585\n","Epoch: 624 \tTraining Loss: 0.002139 \tValidation Loss: 0.023602\n","Epoch: 625 \tTraining Loss: 0.002133 \tValidation Loss: 0.023611\n","Epoch: 626 \tTraining Loss: 0.002016 \tValidation Loss: 0.023797\n","Epoch: 627 \tTraining Loss: 0.002097 \tValidation Loss: 0.024070\n","Epoch: 628 \tTraining Loss: 0.002233 \tValidation Loss: 0.024059\n","Epoch: 629 \tTraining Loss: 0.002310 \tValidation Loss: 0.023802\n","Epoch: 630 \tTraining Loss: 0.002650 \tValidation Loss: 0.024184\n","Epoch: 631 \tTraining Loss: 0.002945 \tValidation Loss: 0.024128\n","Epoch: 632 \tTraining Loss: 0.003271 \tValidation Loss: 0.023620\n","Epoch: 633 \tTraining Loss: 0.003828 \tValidation Loss: 0.023521\n","Epoch: 634 \tTraining Loss: 0.004270 \tValidation Loss: 0.024118\n","Epoch: 635 \tTraining Loss: 0.004547 \tValidation Loss: 0.024109\n","Epoch: 636 \tTraining Loss: 0.004160 \tValidation Loss: 0.023799\n","Epoch: 637 \tTraining Loss: 0.003098 \tValidation Loss: 0.023915\n","Epoch: 638 \tTraining Loss: 0.002591 \tValidation Loss: 0.023733\n","Epoch: 639 \tTraining Loss: 0.002404 \tValidation Loss: 0.023827\n","Epoch: 640 \tTraining Loss: 0.002315 \tValidation Loss: 0.024026\n","Epoch: 641 \tTraining Loss: 0.002240 \tValidation Loss: 0.023849\n","Epoch: 642 \tTraining Loss: 0.002273 \tValidation Loss: 0.023743\n","Epoch: 643 \tTraining Loss: 0.002218 \tValidation Loss: 0.023863\n","Epoch: 644 \tTraining Loss: 0.002227 \tValidation Loss: 0.023809\n","Epoch: 645 \tTraining Loss: 0.001977 \tValidation Loss: 0.023849\n","Epoch: 646 \tTraining Loss: 0.001897 \tValidation Loss: 0.023587\n","Epoch: 647 \tTraining Loss: 0.001794 \tValidation Loss: 0.024024\n","Epoch: 648 \tTraining Loss: 0.001814 \tValidation Loss: 0.023635\n","Epoch: 649 \tTraining Loss: 0.001851 \tValidation Loss: 0.023417\n","Epoch: 650 \tTraining Loss: 0.001864 \tValidation Loss: 0.023807\n","Epoch: 651 \tTraining Loss: 0.002077 \tValidation Loss: 0.023662\n","Epoch: 652 \tTraining Loss: 0.002050 \tValidation Loss: 0.023851\n","Epoch: 653 \tTraining Loss: 0.002104 \tValidation Loss: 0.023542\n","Epoch: 654 \tTraining Loss: 0.001919 \tValidation Loss: 0.023674\n","Epoch: 655 \tTraining Loss: 0.001887 \tValidation Loss: 0.023910\n","Epoch: 656 \tTraining Loss: 0.001819 \tValidation Loss: 0.023888\n","Epoch: 657 \tTraining Loss: 0.001838 \tValidation Loss: 0.023398\n","Epoch: 658 \tTraining Loss: 0.001895 \tValidation Loss: 0.023702\n","Epoch: 659 \tTraining Loss: 0.001994 \tValidation Loss: 0.023596\n","Epoch: 660 \tTraining Loss: 0.002105 \tValidation Loss: 0.023567\n","Epoch: 661 \tTraining Loss: 0.002128 \tValidation Loss: 0.023884\n","Epoch: 662 \tTraining Loss: 0.001877 \tValidation Loss: 0.023545\n","Epoch: 663 \tTraining Loss: 0.001691 \tValidation Loss: 0.024047\n","Epoch: 664 \tTraining Loss: 0.001729 \tValidation Loss: 0.023671\n","Epoch: 665 \tTraining Loss: 0.001668 \tValidation Loss: 0.023573\n","Epoch: 666 \tTraining Loss: 0.001771 \tValidation Loss: 0.024062\n","Epoch: 667 \tTraining Loss: 0.001797 \tValidation Loss: 0.023330\n","Validation loss decreased (0.023349 --> 0.023330).  Saving model ...\n","Epoch: 668 \tTraining Loss: 0.001790 \tValidation Loss: 0.023825\n","Epoch: 669 \tTraining Loss: 0.001780 \tValidation Loss: 0.023899\n","Epoch: 670 \tTraining Loss: 0.001835 \tValidation Loss: 0.023886\n","Epoch: 671 \tTraining Loss: 0.001973 \tValidation Loss: 0.023817\n","Epoch: 672 \tTraining Loss: 0.002025 \tValidation Loss: 0.023385\n","Epoch: 673 \tTraining Loss: 0.002103 \tValidation Loss: 0.023641\n","Epoch: 674 \tTraining Loss: 0.002043 \tValidation Loss: 0.023472\n","Epoch: 675 \tTraining Loss: 0.002129 \tValidation Loss: 0.023863\n","Epoch: 676 \tTraining Loss: 0.002159 \tValidation Loss: 0.023543\n","Epoch: 677 \tTraining Loss: 0.002083 \tValidation Loss: 0.023731\n","Epoch: 678 \tTraining Loss: 0.002024 \tValidation Loss: 0.023862\n","Epoch: 679 \tTraining Loss: 0.001993 \tValidation Loss: 0.023565\n","Epoch: 680 \tTraining Loss: 0.001851 \tValidation Loss: 0.023672\n","Epoch: 681 \tTraining Loss: 0.001872 \tValidation Loss: 0.023756\n","Epoch: 682 \tTraining Loss: 0.001921 \tValidation Loss: 0.023586\n","Epoch: 683 \tTraining Loss: 0.002025 \tValidation Loss: 0.023635\n","Epoch: 684 \tTraining Loss: 0.002188 \tValidation Loss: 0.023881\n","Epoch: 685 \tTraining Loss: 0.002219 \tValidation Loss: 0.023844\n","Epoch: 686 \tTraining Loss: 0.002243 \tValidation Loss: 0.023439\n","Epoch: 687 \tTraining Loss: 0.002342 \tValidation Loss: 0.023764\n","Epoch: 688 \tTraining Loss: 0.002335 \tValidation Loss: 0.023962\n","Epoch: 689 \tTraining Loss: 0.002184 \tValidation Loss: 0.023514\n","Epoch: 690 \tTraining Loss: 0.002002 \tValidation Loss: 0.023553\n","Epoch: 691 \tTraining Loss: 0.001895 \tValidation Loss: 0.023424\n","Epoch: 692 \tTraining Loss: 0.001845 \tValidation Loss: 0.023682\n","Epoch: 693 \tTraining Loss: 0.001894 \tValidation Loss: 0.023573\n","Epoch: 694 \tTraining Loss: 0.001950 \tValidation Loss: 0.023362\n","Epoch: 695 \tTraining Loss: 0.001751 \tValidation Loss: 0.023809\n","Epoch: 696 \tTraining Loss: 0.001785 \tValidation Loss: 0.023629\n","Epoch: 697 \tTraining Loss: 0.001885 \tValidation Loss: 0.023500\n","Epoch: 698 \tTraining Loss: 0.002137 \tValidation Loss: 0.023672\n","Epoch: 699 \tTraining Loss: 0.002051 \tValidation Loss: 0.023798\n","Epoch: 700 \tTraining Loss: 0.001881 \tValidation Loss: 0.023585\n","Epoch: 701 \tTraining Loss: 0.001911 \tValidation Loss: 0.023853\n","Epoch: 702 \tTraining Loss: 0.001890 \tValidation Loss: 0.023467\n","Epoch: 703 \tTraining Loss: 0.001993 \tValidation Loss: 0.023582\n","Epoch: 704 \tTraining Loss: 0.001963 \tValidation Loss: 0.023590\n","Epoch: 705 \tTraining Loss: 0.001937 \tValidation Loss: 0.023660\n","Epoch: 706 \tTraining Loss: 0.002291 \tValidation Loss: 0.023611\n","Epoch: 707 \tTraining Loss: 0.002239 \tValidation Loss: 0.023588\n","Epoch: 708 \tTraining Loss: 0.002525 \tValidation Loss: 0.023803\n","Epoch: 709 \tTraining Loss: 0.002489 \tValidation Loss: 0.023645\n","Epoch: 710 \tTraining Loss: 0.002562 \tValidation Loss: 0.023518\n","Epoch: 711 \tTraining Loss: 0.002315 \tValidation Loss: 0.023576\n","Epoch: 712 \tTraining Loss: 0.002330 \tValidation Loss: 0.023730\n","Epoch: 713 \tTraining Loss: 0.002318 \tValidation Loss: 0.023796\n","Epoch: 714 \tTraining Loss: 0.002211 \tValidation Loss: 0.023659\n","Epoch: 715 \tTraining Loss: 0.002135 \tValidation Loss: 0.023557\n","Epoch: 716 \tTraining Loss: 0.002049 \tValidation Loss: 0.023674\n","Epoch: 717 \tTraining Loss: 0.002088 \tValidation Loss: 0.023384\n","Epoch: 718 \tTraining Loss: 0.002455 \tValidation Loss: 0.023766\n","Epoch: 719 \tTraining Loss: 0.002509 \tValidation Loss: 0.023763\n","Epoch: 720 \tTraining Loss: 0.002215 \tValidation Loss: 0.023537\n","Epoch: 721 \tTraining Loss: 0.002100 \tValidation Loss: 0.023767\n","Epoch: 722 \tTraining Loss: 0.002039 \tValidation Loss: 0.023410\n","Epoch: 723 \tTraining Loss: 0.001896 \tValidation Loss: 0.023305\n","Validation loss decreased (0.023330 --> 0.023305).  Saving model ...\n","Epoch: 724 \tTraining Loss: 0.001907 \tValidation Loss: 0.023859\n","Epoch: 725 \tTraining Loss: 0.001858 \tValidation Loss: 0.023469\n","Epoch: 726 \tTraining Loss: 0.002047 \tValidation Loss: 0.023824\n","Epoch: 727 \tTraining Loss: 0.002132 \tValidation Loss: 0.023405\n","Epoch: 728 \tTraining Loss: 0.001901 \tValidation Loss: 0.023577\n","Epoch: 729 \tTraining Loss: 0.001882 \tValidation Loss: 0.023646\n","Epoch: 730 \tTraining Loss: 0.002190 \tValidation Loss: 0.023381\n","Epoch: 731 \tTraining Loss: 0.002205 \tValidation Loss: 0.023852\n","Epoch: 732 \tTraining Loss: 0.002069 \tValidation Loss: 0.023672\n","Epoch: 733 \tTraining Loss: 0.001954 \tValidation Loss: 0.023668\n","Epoch: 734 \tTraining Loss: 0.001838 \tValidation Loss: 0.023671\n","Epoch: 735 \tTraining Loss: 0.001794 \tValidation Loss: 0.023570\n","Epoch: 736 \tTraining Loss: 0.001807 \tValidation Loss: 0.023716\n","Epoch: 737 \tTraining Loss: 0.001818 \tValidation Loss: 0.023549\n","Epoch: 738 \tTraining Loss: 0.001861 \tValidation Loss: 0.023676\n","Epoch: 739 \tTraining Loss: 0.001875 \tValidation Loss: 0.023950\n","Epoch: 740 \tTraining Loss: 0.001975 \tValidation Loss: 0.023751\n","Epoch: 741 \tTraining Loss: 0.002274 \tValidation Loss: 0.023776\n","Epoch: 742 \tTraining Loss: 0.002328 \tValidation Loss: 0.023385\n","Epoch: 743 \tTraining Loss: 0.002480 \tValidation Loss: 0.023743\n","Epoch: 744 \tTraining Loss: 0.002152 \tValidation Loss: 0.023572\n","Epoch: 745 \tTraining Loss: 0.002031 \tValidation Loss: 0.023641\n","Epoch: 746 \tTraining Loss: 0.001934 \tValidation Loss: 0.023329\n","Epoch: 747 \tTraining Loss: 0.001918 \tValidation Loss: 0.023372\n","Epoch: 748 \tTraining Loss: 0.001996 \tValidation Loss: 0.023360\n","Epoch: 749 \tTraining Loss: 0.002292 \tValidation Loss: 0.023465\n","Epoch: 750 \tTraining Loss: 0.002231 \tValidation Loss: 0.023828\n","Epoch: 751 \tTraining Loss: 0.002145 \tValidation Loss: 0.023504\n","Epoch: 752 \tTraining Loss: 0.002100 \tValidation Loss: 0.023508\n","Epoch: 753 \tTraining Loss: 0.002133 \tValidation Loss: 0.023623\n","Epoch: 754 \tTraining Loss: 0.002027 \tValidation Loss: 0.023676\n","Epoch: 755 \tTraining Loss: 0.001990 \tValidation Loss: 0.023649\n","Epoch: 756 \tTraining Loss: 0.002060 \tValidation Loss: 0.023785\n","Epoch: 757 \tTraining Loss: 0.002080 \tValidation Loss: 0.023619\n","Epoch: 758 \tTraining Loss: 0.002192 \tValidation Loss: 0.023830\n","Epoch: 759 \tTraining Loss: 0.002430 \tValidation Loss: 0.023285\n","Validation loss decreased (0.023305 --> 0.023285).  Saving model ...\n","Epoch: 760 \tTraining Loss: 0.002117 \tValidation Loss: 0.023671\n","Epoch: 761 \tTraining Loss: 0.001889 \tValidation Loss: 0.023730\n","Epoch: 762 \tTraining Loss: 0.001846 \tValidation Loss: 0.023722\n","Epoch: 763 \tTraining Loss: 0.001903 \tValidation Loss: 0.023432\n","Epoch: 764 \tTraining Loss: 0.001862 \tValidation Loss: 0.024026\n","Epoch: 765 \tTraining Loss: 0.002091 \tValidation Loss: 0.023736\n","Epoch: 766 \tTraining Loss: 0.001904 \tValidation Loss: 0.023623\n","Epoch: 767 \tTraining Loss: 0.001965 \tValidation Loss: 0.023452\n","Epoch: 768 \tTraining Loss: 0.001863 \tValidation Loss: 0.023536\n","Epoch: 769 \tTraining Loss: 0.001814 \tValidation Loss: 0.023923\n","Epoch: 770 \tTraining Loss: 0.001838 \tValidation Loss: 0.023383\n","Epoch: 771 \tTraining Loss: 0.001926 \tValidation Loss: 0.023601\n","Epoch: 772 \tTraining Loss: 0.001826 \tValidation Loss: 0.023413\n","Epoch: 773 \tTraining Loss: 0.001874 \tValidation Loss: 0.023532\n","Epoch: 774 \tTraining Loss: 0.001771 \tValidation Loss: 0.024003\n","Epoch: 775 \tTraining Loss: 0.001724 \tValidation Loss: 0.023555\n","Epoch: 776 \tTraining Loss: 0.001742 \tValidation Loss: 0.023670\n","Epoch: 777 \tTraining Loss: 0.001706 \tValidation Loss: 0.023388\n","Epoch: 778 \tTraining Loss: 0.002041 \tValidation Loss: 0.023481\n","Epoch: 779 \tTraining Loss: 0.001937 \tValidation Loss: 0.023495\n","Epoch: 780 \tTraining Loss: 0.002003 \tValidation Loss: 0.023079\n","Validation loss decreased (0.023285 --> 0.023079).  Saving model ...\n","Epoch: 781 \tTraining Loss: 0.002339 \tValidation Loss: 0.023742\n","Epoch: 782 \tTraining Loss: 0.002375 \tValidation Loss: 0.023453\n","Epoch: 783 \tTraining Loss: 0.002136 \tValidation Loss: 0.023757\n","Epoch: 784 \tTraining Loss: 0.002024 \tValidation Loss: 0.023779\n","Epoch: 785 \tTraining Loss: 0.002007 \tValidation Loss: 0.023657\n","Epoch: 786 \tTraining Loss: 0.001988 \tValidation Loss: 0.023637\n","Epoch: 787 \tTraining Loss: 0.002023 \tValidation Loss: 0.023648\n","Epoch: 788 \tTraining Loss: 0.002337 \tValidation Loss: 0.023604\n","Epoch: 789 \tTraining Loss: 0.002460 \tValidation Loss: 0.023665\n","Epoch: 790 \tTraining Loss: 0.002465 \tValidation Loss: 0.023653\n","Epoch: 791 \tTraining Loss: 0.002397 \tValidation Loss: 0.023661\n","Epoch: 792 \tTraining Loss: 0.002365 \tValidation Loss: 0.023636\n","Epoch: 793 \tTraining Loss: 0.002154 \tValidation Loss: 0.023437\n","Epoch: 794 \tTraining Loss: 0.002205 \tValidation Loss: 0.023493\n","Epoch: 795 \tTraining Loss: 0.002322 \tValidation Loss: 0.023719\n","Epoch: 796 \tTraining Loss: 0.002120 \tValidation Loss: 0.023384\n","Epoch: 797 \tTraining Loss: 0.001855 \tValidation Loss: 0.023544\n","Epoch: 798 \tTraining Loss: 0.001900 \tValidation Loss: 0.023629\n","Epoch: 799 \tTraining Loss: 0.001976 \tValidation Loss: 0.023730\n","Epoch: 800 \tTraining Loss: 0.001888 \tValidation Loss: 0.023344\n","Epoch: 801 \tTraining Loss: 0.001802 \tValidation Loss: 0.023267\n","Epoch: 802 \tTraining Loss: 0.001849 \tValidation Loss: 0.023644\n","Epoch: 803 \tTraining Loss: 0.001799 \tValidation Loss: 0.023572\n","Epoch: 804 \tTraining Loss: 0.001815 \tValidation Loss: 0.023688\n","Epoch: 805 \tTraining Loss: 0.002057 \tValidation Loss: 0.023430\n","Epoch: 806 \tTraining Loss: 0.002374 \tValidation Loss: 0.023416\n","Epoch: 807 \tTraining Loss: 0.002369 \tValidation Loss: 0.024079\n","Epoch: 808 \tTraining Loss: 0.001996 \tValidation Loss: 0.023485\n","Epoch: 809 \tTraining Loss: 0.001704 \tValidation Loss: 0.023407\n","Epoch: 810 \tTraining Loss: 0.001571 \tValidation Loss: 0.023421\n","Epoch: 811 \tTraining Loss: 0.001668 \tValidation Loss: 0.023556\n","Epoch: 812 \tTraining Loss: 0.001702 \tValidation Loss: 0.023487\n","Epoch: 813 \tTraining Loss: 0.001645 \tValidation Loss: 0.023392\n","Epoch: 814 \tTraining Loss: 0.001631 \tValidation Loss: 0.023129\n","Epoch: 815 \tTraining Loss: 0.001770 \tValidation Loss: 0.023491\n","Epoch: 816 \tTraining Loss: 0.001705 \tValidation Loss: 0.023528\n","Epoch: 817 \tTraining Loss: 0.001701 \tValidation Loss: 0.023422\n","Epoch: 818 \tTraining Loss: 0.001738 \tValidation Loss: 0.023526\n","Epoch: 819 \tTraining Loss: 0.001694 \tValidation Loss: 0.023249\n","Epoch: 820 \tTraining Loss: 0.001697 \tValidation Loss: 0.023325\n","Epoch: 821 \tTraining Loss: 0.001733 \tValidation Loss: 0.023556\n","Epoch: 822 \tTraining Loss: 0.001705 \tValidation Loss: 0.023838\n","Epoch: 823 \tTraining Loss: 0.001584 \tValidation Loss: 0.023485\n","Epoch: 824 \tTraining Loss: 0.001542 \tValidation Loss: 0.023124\n","Epoch: 825 \tTraining Loss: 0.001621 \tValidation Loss: 0.023388\n","Epoch: 826 \tTraining Loss: 0.001710 \tValidation Loss: 0.023358\n","Epoch: 827 \tTraining Loss: 0.001811 \tValidation Loss: 0.023507\n","Epoch: 828 \tTraining Loss: 0.002057 \tValidation Loss: 0.023500\n","Epoch: 829 \tTraining Loss: 0.001965 \tValidation Loss: 0.023318\n","Epoch: 830 \tTraining Loss: 0.002033 \tValidation Loss: 0.023460\n","Epoch: 831 \tTraining Loss: 0.001982 \tValidation Loss: 0.023310\n","Epoch: 832 \tTraining Loss: 0.001913 \tValidation Loss: 0.023360\n","Epoch: 833 \tTraining Loss: 0.001842 \tValidation Loss: 0.023684\n","Epoch: 834 \tTraining Loss: 0.001733 \tValidation Loss: 0.023459\n","Epoch: 835 \tTraining Loss: 0.001805 \tValidation Loss: 0.023184\n","Epoch: 836 \tTraining Loss: 0.001863 \tValidation Loss: 0.023828\n","Epoch: 837 \tTraining Loss: 0.002016 \tValidation Loss: 0.023302\n","Epoch: 838 \tTraining Loss: 0.002294 \tValidation Loss: 0.023686\n","Epoch: 839 \tTraining Loss: 0.002131 \tValidation Loss: 0.023244\n","Epoch: 840 \tTraining Loss: 0.002179 \tValidation Loss: 0.023533\n","Epoch: 841 \tTraining Loss: 0.001916 \tValidation Loss: 0.023624\n","Epoch: 842 \tTraining Loss: 0.002024 \tValidation Loss: 0.023192\n","Epoch: 843 \tTraining Loss: 0.002032 \tValidation Loss: 0.023749\n","Epoch: 844 \tTraining Loss: 0.002036 \tValidation Loss: 0.023395\n","Epoch: 845 \tTraining Loss: 0.001944 \tValidation Loss: 0.023047\n","Validation loss decreased (0.023079 --> 0.023047).  Saving model ...\n","Epoch: 846 \tTraining Loss: 0.001903 \tValidation Loss: 0.023702\n","Epoch: 847 \tTraining Loss: 0.001808 \tValidation Loss: 0.023678\n","Epoch: 848 \tTraining Loss: 0.001713 \tValidation Loss: 0.023795\n","Epoch: 849 \tTraining Loss: 0.001768 \tValidation Loss: 0.023344\n","Epoch: 850 \tTraining Loss: 0.001808 \tValidation Loss: 0.023560\n","Epoch: 851 \tTraining Loss: 0.001808 \tValidation Loss: 0.023451\n","Epoch: 852 \tTraining Loss: 0.001792 \tValidation Loss: 0.023811\n","Epoch: 853 \tTraining Loss: 0.001772 \tValidation Loss: 0.023477\n","Epoch: 854 \tTraining Loss: 0.001825 \tValidation Loss: 0.023457\n","Epoch: 855 \tTraining Loss: 0.001808 \tValidation Loss: 0.023520\n","Epoch: 856 \tTraining Loss: 0.001827 \tValidation Loss: 0.023627\n","Epoch: 857 \tTraining Loss: 0.001784 \tValidation Loss: 0.023204\n","Epoch: 858 \tTraining Loss: 0.001791 \tValidation Loss: 0.023695\n","Epoch: 859 \tTraining Loss: 0.001968 \tValidation Loss: 0.023552\n","Epoch: 860 \tTraining Loss: 0.002000 \tValidation Loss: 0.023468\n","Epoch: 861 \tTraining Loss: 0.001912 \tValidation Loss: 0.023761\n","Epoch: 862 \tTraining Loss: 0.001966 \tValidation Loss: 0.023696\n","Epoch: 863 \tTraining Loss: 0.001912 \tValidation Loss: 0.023442\n","Epoch: 864 \tTraining Loss: 0.001967 \tValidation Loss: 0.023457\n","Epoch: 865 \tTraining Loss: 0.001974 \tValidation Loss: 0.023618\n","Epoch: 866 \tTraining Loss: 0.001950 \tValidation Loss: 0.023499\n","Epoch: 867 \tTraining Loss: 0.002158 \tValidation Loss: 0.023602\n","Epoch: 868 \tTraining Loss: 0.002102 \tValidation Loss: 0.023530\n","Epoch: 869 \tTraining Loss: 0.001804 \tValidation Loss: 0.023402\n","Epoch: 870 \tTraining Loss: 0.001650 \tValidation Loss: 0.023607\n","Epoch: 871 \tTraining Loss: 0.001670 \tValidation Loss: 0.023573\n","Epoch: 872 \tTraining Loss: 0.001908 \tValidation Loss: 0.023526\n","Epoch: 873 \tTraining Loss: 0.001995 \tValidation Loss: 0.023881\n","Epoch: 874 \tTraining Loss: 0.002014 \tValidation Loss: 0.023967\n","Epoch: 875 \tTraining Loss: 0.002237 \tValidation Loss: 0.023595\n","Epoch: 876 \tTraining Loss: 0.002261 \tValidation Loss: 0.023386\n","Epoch: 877 \tTraining Loss: 0.002617 \tValidation Loss: 0.023316\n","Epoch: 878 \tTraining Loss: 0.002367 \tValidation Loss: 0.023728\n","Epoch: 879 \tTraining Loss: 0.002181 \tValidation Loss: 0.023894\n","Epoch: 880 \tTraining Loss: 0.002254 \tValidation Loss: 0.023622\n","Epoch: 881 \tTraining Loss: 0.002116 \tValidation Loss: 0.023549\n","Epoch: 882 \tTraining Loss: 0.001871 \tValidation Loss: 0.023746\n","Epoch: 883 \tTraining Loss: 0.001796 \tValidation Loss: 0.023613\n","Epoch: 884 \tTraining Loss: 0.001700 \tValidation Loss: 0.023445\n","Epoch: 885 \tTraining Loss: 0.001797 \tValidation Loss: 0.023347\n","Epoch: 886 \tTraining Loss: 0.001779 \tValidation Loss: 0.023273\n","Epoch: 887 \tTraining Loss: 0.001702 \tValidation Loss: 0.023606\n","Epoch: 888 \tTraining Loss: 0.001899 \tValidation Loss: 0.023524\n","Epoch: 889 \tTraining Loss: 0.001934 \tValidation Loss: 0.023629\n","Epoch: 890 \tTraining Loss: 0.001793 \tValidation Loss: 0.023315\n","Epoch: 891 \tTraining Loss: 0.001736 \tValidation Loss: 0.023531\n","Epoch: 892 \tTraining Loss: 0.001679 \tValidation Loss: 0.023453\n","Epoch: 893 \tTraining Loss: 0.001700 \tValidation Loss: 0.023433\n","Epoch: 894 \tTraining Loss: 0.001730 \tValidation Loss: 0.023260\n","Epoch: 895 \tTraining Loss: 0.001700 \tValidation Loss: 0.023851\n","Epoch: 896 \tTraining Loss: 0.001666 \tValidation Loss: 0.023617\n","Epoch: 897 \tTraining Loss: 0.001582 \tValidation Loss: 0.023562\n","Epoch: 898 \tTraining Loss: 0.001799 \tValidation Loss: 0.023359\n","Epoch: 899 \tTraining Loss: 0.001646 \tValidation Loss: 0.023426\n","Epoch: 900 \tTraining Loss: 0.001556 \tValidation Loss: 0.023370\n","Epoch: 901 \tTraining Loss: 0.001569 \tValidation Loss: 0.023135\n","Epoch: 902 \tTraining Loss: 0.001690 \tValidation Loss: 0.023442\n","Epoch: 903 \tTraining Loss: 0.001783 \tValidation Loss: 0.023414\n","Epoch: 904 \tTraining Loss: 0.001949 \tValidation Loss: 0.023720\n","Epoch: 905 \tTraining Loss: 0.002028 \tValidation Loss: 0.023484\n","Epoch: 906 \tTraining Loss: 0.001893 \tValidation Loss: 0.023522\n","Epoch: 907 \tTraining Loss: 0.001567 \tValidation Loss: 0.023492\n","Epoch: 908 \tTraining Loss: 0.001521 \tValidation Loss: 0.023607\n","Epoch: 909 \tTraining Loss: 0.001633 \tValidation Loss: 0.023495\n","Epoch: 910 \tTraining Loss: 0.001856 \tValidation Loss: 0.023667\n","Epoch: 911 \tTraining Loss: 0.001789 \tValidation Loss: 0.023739\n","Epoch: 912 \tTraining Loss: 0.001773 \tValidation Loss: 0.023486\n","Epoch: 913 \tTraining Loss: 0.001714 \tValidation Loss: 0.023425\n","Epoch: 914 \tTraining Loss: 0.001567 \tValidation Loss: 0.023431\n","Epoch: 915 \tTraining Loss: 0.001583 \tValidation Loss: 0.023463\n","Epoch: 916 \tTraining Loss: 0.001483 \tValidation Loss: 0.023535\n","Epoch: 917 \tTraining Loss: 0.001474 \tValidation Loss: 0.023135\n","Epoch: 918 \tTraining Loss: 0.001489 \tValidation Loss: 0.023430\n","Epoch: 919 \tTraining Loss: 0.001529 \tValidation Loss: 0.023631\n","Epoch: 920 \tTraining Loss: 0.001700 \tValidation Loss: 0.023457\n","Epoch: 921 \tTraining Loss: 0.001751 \tValidation Loss: 0.023302\n","Epoch: 922 \tTraining Loss: 0.001714 \tValidation Loss: 0.023456\n","Epoch: 923 \tTraining Loss: 0.001831 \tValidation Loss: 0.023463\n","Epoch: 924 \tTraining Loss: 0.001930 \tValidation Loss: 0.023452\n","Epoch: 925 \tTraining Loss: 0.002021 \tValidation Loss: 0.023575\n","Epoch: 926 \tTraining Loss: 0.001825 \tValidation Loss: 0.023304\n","Epoch: 927 \tTraining Loss: 0.001869 \tValidation Loss: 0.023527\n","Epoch: 928 \tTraining Loss: 0.001959 \tValidation Loss: 0.023673\n","Epoch: 929 \tTraining Loss: 0.001953 \tValidation Loss: 0.023230\n","Epoch: 930 \tTraining Loss: 0.001991 \tValidation Loss: 0.023381\n","Epoch: 931 \tTraining Loss: 0.002068 \tValidation Loss: 0.023625\n","Epoch: 932 \tTraining Loss: 0.002122 \tValidation Loss: 0.023596\n","Epoch: 933 \tTraining Loss: 0.001969 \tValidation Loss: 0.023385\n","Epoch: 934 \tTraining Loss: 0.001774 \tValidation Loss: 0.023467\n","Epoch: 935 \tTraining Loss: 0.001704 \tValidation Loss: 0.023596\n","Epoch: 936 \tTraining Loss: 0.001802 \tValidation Loss: 0.023198\n","Epoch: 937 \tTraining Loss: 0.001792 \tValidation Loss: 0.023049\n","Epoch: 938 \tTraining Loss: 0.002046 \tValidation Loss: 0.023453\n","Epoch: 939 \tTraining Loss: 0.001866 \tValidation Loss: 0.023313\n","Epoch: 940 \tTraining Loss: 0.001643 \tValidation Loss: 0.023081\n","Epoch: 941 \tTraining Loss: 0.001675 \tValidation Loss: 0.023514\n","Epoch: 942 \tTraining Loss: 0.001674 \tValidation Loss: 0.023267\n","Epoch: 943 \tTraining Loss: 0.001667 \tValidation Loss: 0.023370\n","Epoch: 944 \tTraining Loss: 0.001593 \tValidation Loss: 0.023739\n","Epoch: 945 \tTraining Loss: 0.001505 \tValidation Loss: 0.023539\n","Epoch: 946 \tTraining Loss: 0.001508 \tValidation Loss: 0.023194\n","Epoch: 947 \tTraining Loss: 0.001484 \tValidation Loss: 0.023545\n","Epoch: 948 \tTraining Loss: 0.001679 \tValidation Loss: 0.023274\n","Epoch: 949 \tTraining Loss: 0.001654 \tValidation Loss: 0.023352\n","Epoch: 950 \tTraining Loss: 0.001703 \tValidation Loss: 0.023473\n","Epoch: 951 \tTraining Loss: 0.001743 \tValidation Loss: 0.023258\n","Epoch: 952 \tTraining Loss: 0.001659 \tValidation Loss: 0.023498\n","Epoch: 953 \tTraining Loss: 0.001564 \tValidation Loss: 0.023441\n","Epoch: 954 \tTraining Loss: 0.001552 \tValidation Loss: 0.023076\n","Epoch: 955 \tTraining Loss: 0.001539 \tValidation Loss: 0.023631\n","Epoch: 956 \tTraining Loss: 0.001669 \tValidation Loss: 0.023255\n","Epoch: 957 \tTraining Loss: 0.001725 \tValidation Loss: 0.023469\n","Epoch: 958 \tTraining Loss: 0.001731 \tValidation Loss: 0.023551\n","Epoch: 959 \tTraining Loss: 0.001847 \tValidation Loss: 0.022903\n","Validation loss decreased (0.023047 --> 0.022903).  Saving model ...\n","Epoch: 960 \tTraining Loss: 0.001989 \tValidation Loss: 0.023578\n","Epoch: 961 \tTraining Loss: 0.001980 \tValidation Loss: 0.023457\n","Epoch: 962 \tTraining Loss: 0.001906 \tValidation Loss: 0.023432\n","Epoch: 963 \tTraining Loss: 0.002057 \tValidation Loss: 0.023357\n","Epoch: 964 \tTraining Loss: 0.001864 \tValidation Loss: 0.023534\n","Epoch: 965 \tTraining Loss: 0.001878 \tValidation Loss: 0.023474\n","Epoch: 966 \tTraining Loss: 0.001842 \tValidation Loss: 0.023505\n","Epoch: 967 \tTraining Loss: 0.001793 \tValidation Loss: 0.023556\n","Epoch: 968 \tTraining Loss: 0.001722 \tValidation Loss: 0.023410\n","Epoch: 969 \tTraining Loss: 0.001541 \tValidation Loss: 0.023500\n","Epoch: 970 \tTraining Loss: 0.001586 \tValidation Loss: 0.023142\n","Epoch: 971 \tTraining Loss: 0.001682 \tValidation Loss: 0.023373\n","Epoch: 972 \tTraining Loss: 0.001554 \tValidation Loss: 0.023491\n","Epoch: 973 \tTraining Loss: 0.001477 \tValidation Loss: 0.023504\n","Epoch: 974 \tTraining Loss: 0.001503 \tValidation Loss: 0.023093\n","Epoch: 975 \tTraining Loss: 0.001537 \tValidation Loss: 0.023163\n","Epoch: 976 \tTraining Loss: 0.001551 \tValidation Loss: 0.023295\n","Epoch: 977 \tTraining Loss: 0.001527 \tValidation Loss: 0.023473\n","Epoch: 978 \tTraining Loss: 0.001457 \tValidation Loss: 0.023741\n","Epoch: 979 \tTraining Loss: 0.001433 \tValidation Loss: 0.023607\n","Epoch: 980 \tTraining Loss: 0.001378 \tValidation Loss: 0.023504\n","Epoch: 981 \tTraining Loss: 0.001512 \tValidation Loss: 0.023297\n","Epoch: 982 \tTraining Loss: 0.001681 \tValidation Loss: 0.023545\n","Epoch: 983 \tTraining Loss: 0.001600 \tValidation Loss: 0.023494\n","Epoch: 984 \tTraining Loss: 0.001771 \tValidation Loss: 0.023466\n","Epoch: 985 \tTraining Loss: 0.001926 \tValidation Loss: 0.023447\n","Epoch: 986 \tTraining Loss: 0.001835 \tValidation Loss: 0.023605\n","Epoch: 987 \tTraining Loss: 0.001651 \tValidation Loss: 0.023609\n","Epoch: 988 \tTraining Loss: 0.001749 \tValidation Loss: 0.023607\n","Epoch: 989 \tTraining Loss: 0.001967 \tValidation Loss: 0.023421\n","Epoch: 990 \tTraining Loss: 0.001810 \tValidation Loss: 0.023340\n","Epoch: 991 \tTraining Loss: 0.001652 \tValidation Loss: 0.023149\n","Epoch: 992 \tTraining Loss: 0.001588 \tValidation Loss: 0.023422\n","Epoch: 993 \tTraining Loss: 0.001633 \tValidation Loss: 0.023702\n","Epoch: 994 \tTraining Loss: 0.001745 \tValidation Loss: 0.023536\n","Epoch: 995 \tTraining Loss: 0.001772 \tValidation Loss: 0.023293\n","Epoch: 996 \tTraining Loss: 0.001895 \tValidation Loss: 0.023703\n","Epoch: 997 \tTraining Loss: 0.001650 \tValidation Loss: 0.023513\n","Epoch: 998 \tTraining Loss: 0.001665 \tValidation Loss: 0.023341\n","Epoch: 999 \tTraining Loss: 0.001936 \tValidation Loss: 0.023734\n","Epoch: 1000 \tTraining Loss: 0.001867 \tValidation Loss: 0.023575\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xai0Rt-gZul3","colab_type":"text"},"source":["my error is 0.060\n","\n","It's the lowest error that I made. "]},{"cell_type":"code","metadata":{"id":"nbuBaaQ0cSfC","colab_type":"code","outputId":"e48305ca-a16b-4261-dafd-055a14f6f5f8","executionInfo":{"status":"ok","timestamp":1584542022207,"user_tz":-480,"elapsed":1456,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Evaluate this one\n","model.load_state_dict(torch.load(ROOT_FOLDER+'model.pt'))\n","predictions = predict(test_loader, model)\n","np.save(ROOT_FOLDER+'s123456789.npy', predictions)\n","\n","# baseline: 0.069\n","print('Error: ', np.linalg.norm(predictions-KPT_S_TEST.reshape((predictions.shape[0],-1)), axis=1).mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Error:  0.06068283605538413\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J9qqydNkds6I","colab_type":"code","outputId":"8216d81c-b952-4070-bed6-68a3d3fdc3b8","executionInfo":{"status":"ok","timestamp":1584536815237,"user_tz":-480,"elapsed":1105,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":538}},"source":["idx = np.random.randint(predictions.shape[0])\n","print(idx)\n","draw_points(IMG_S_TEST[idx,:,:], predictions[idx,:].reshape((-1,2)))\n","draw_points(IMG_S_TEST[idx,:,:], KPT_S_TEST[idx,:,:])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["257\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnJKASMb9f8TUJc6JfEZ0AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"lbM1snro5Ehw","colab_type":"text"},"source":["# Front View detection\n","\n","Delete the KPT_S_TEST line because no test_kpt_side.npy files in the Google drive"]},{"cell_type":"code","metadata":{"id":"AU1on6WU5Jah","colab_type":"code","outputId":"ce6491d4-b3d9-4eed-be79-8da899b33f72","executionInfo":{"status":"ok","timestamp":1584941883642,"user_tz":-480,"elapsed":1984,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def draw_points(image, kpts):\n","    plt.figure()\n","    plt.imshow(image, cmap='gray')\n","    keypoints = (kpts+0.5)*IMG_SIZE\n","    plt.scatter(keypoints[:, 0], keypoints[:, 1], s=50, marker='.', c='r')\n","\n","# load side view data\n","IMG_SIZE = 200\n","IMG_S_TRAIN = np.load(ROOT_FOLDER+'data/train_img_front.npy')\n","IMG_S_TRAIN = np.unpackbits(IMG_S_TRAIN).reshape((-1,IMG_SIZE,IMG_SIZE))\n","IMG_S_TEST = np.load(ROOT_FOLDER+'data/test_img_front.npy')\n","IMG_S_TEST = np.unpackbits(IMG_S_TEST).reshape((-1,IMG_SIZE,IMG_SIZE))\n","KPT_S_TRAIN = np.load(ROOT_FOLDER+'data/train_kpt_front.npy')/IMG_SIZE - 0.5\n","#KPT_S_TEST = np.load(ROOT_FOLDER+'data/test_kpt_side.npy')/IMG_SIZE - 0.5\n","\n","# show one\n","idx = 0\n","draw_points(IMG_S_TRAIN[idx,:,:], KPT_S_TRAIN[idx,:,:])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"mNBi5A1c5djy","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class SideKeypointsDataset(Dataset):\n","    '''Side View Keypoints Dataset'''\n","    def __init__(self, img, kpt, train=True, transform=None):\n","        self.img = img\n","        self.kpt = kpt\n","        self.train = train\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return self.img.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        image = self.img[idx,:,:].astype(np.float32)\n","        if self.train:\n","            keypoints = self.kpt[idx,:,:].ravel().astype(np.float32)\n","        else:\n","            keypoints = None\n","        sample = {'image': image, 'keypoints': keypoints}\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_iZnJARg5jJc","colab_type":"code","colab":{}},"source":["from torch.utils.data.sampler import SubsetRandomSampler\n","\n","def prepare_train_valid_loaders(trainset, valid_size=0.2, \n","                                batch_size=128):\n","    '''\n","    Split trainset data and prepare DataLoader for training and validation\n","    \n","    Args:\n","        trainset (Dataset): data \n","        valid_size (float): validation size, defalut=0.2\n","        batch_size (int) : batch size, default=128\n","    ''' \n","    \n","    # obtain training indices that will be used for validation\n","    num_train = len(trainset)\n","    indices = list(range(num_train))\n","    np.random.shuffle(indices)\n","    split = int(np.floor(valid_size * num_train))\n","    train_idx, valid_idx = indices[split:], indices[:split]\n","    \n","    # define samplers for obtaining training and validation batches\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","    \n","    # prepare data loaders\n","    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                               sampler=train_sampler)\n","    valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                               sampler=valid_sampler)\n","    \n","    return train_loader, valid_loader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SNPGGt05n7R","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","import cv2\n","\n","class Rescale(object):\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, key_pts = sample['image'], sample['keypoints']\n","        h, w = image.shape[:2]\n","        new_w = np.random.randint(w, self.output_size)\n","        new_h = new_w\n","        new_h, new_w = int(new_h), int(new_w)\n","        img = cv2.resize(image, (new_w, new_h))    \n","        if key_pts is not None:\n","            return {'image': img, 'keypoints': key_pts}\n","        else:\n","            return {'image': img}\n","\n","class RandomCrop(object):\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, key_pts = sample['image'], sample['keypoints']\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","        if h == new_h:\n","            return sample\n","        top = np.random.randint(0, h - new_h)\n","        left = np.random.randint(0, w - new_w)\n","        #left = top # temp\n","        image = image[top: top + new_h,\n","                      left: left + new_w]\n","        if key_pts is not None:\n","            #key_pts = key_pts - [left/output_size, top/output_size]\n","            key_pts[0::2] = ((key_pts[0::2]+0.5)*w-left)/new_w-0.5\n","            key_pts[1::2] = ((key_pts[1::2]+0.5)*h-top)/new_h-0.5\n","            return {'image': image, 'keypoints': key_pts}\n","        else:\n","            return {'image': image}\n","\n","class ToTensor(object):\n","    '''Convert ndarrays in sample to Tensors.'''\n","    def __call__(self, sample):\n","        image, keypoints = sample['image'], sample['keypoints']\n","        # swap color axis because\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.reshape(1, IMG_SIZE, IMG_SIZE)\n","        image = torch.from_numpy(image)\n","        if keypoints is not None:\n","            keypoints = torch.from_numpy(keypoints)\n","            return {'image': image, 'keypoints': keypoints}\n","        else:\n","            return {'image': image}\n","\n","batch_size = 32\n","valid_size = 0.2 # percentage of training set to use as validation\n","\n","# Define a transform to normalize the data\n","tsfm_train = transforms.Compose([Rescale(205), RandomCrop(200), ToTensor()])\n","tsfm_test = transforms.Compose([ToTensor()])\n","\n","# Load the training data and test data\n","trainset = SideKeypointsDataset(IMG_S_TRAIN, KPT_S_TRAIN, transform=tsfm_train)\n","testset = SideKeypointsDataset(IMG_S_TEST, None, train=False, transform=tsfm_test)\n","\n","# prepare data loaders\n","train_loader, valid_loader = prepare_train_valid_loaders(trainset, \n","                                                         valid_size,\n","                                                         batch_size)\n","\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cbN8U_jM5y7v","colab_type":"text"},"source":["# Neural Network Structure"]},{"cell_type":"code","metadata":{"id":"u-Gn1iy25tfF","colab_type":"code","colab":{}},"source":["from torch import nn, optim\n","import torch.nn.functional as F\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_layers, drop_p =0.5):\n","        '''\n","        Buid a forward network with arbitrary hidden layers.\n","        Arguments\n","            ---------\n","            input_size (integer): size of the input layer\n","            output_size (integer): size of the output layer\n","            hidden_layers (list of integers):, the sizes of each hidden layers\n","        '''\n","        super(MLP, self).__init__()\n","        # hidden layers\n","        layer_sizes = [(input_size, hidden_layers[0])] \\\n","                      + list(zip(hidden_layers[:-1], hidden_layers[1:]))\n","        self.hidden_layers = nn.ModuleList([nn.Linear(h1, h2) \n","                                            for h1, h2 in layer_sizes])\n","        self.output = nn.Linear(hidden_layers[-1], output_size)\n","        self.dropout = nn.Dropout(drop_p)\n","        \n","    def forward(self, x):\n","        ''' Forward pass through the network, returns the output logits '''\n","        # flatten inputs\n","        x = x.view(x.shape[0], -1)\n","        for layer in self.hidden_layers:\n","            x = F.relu(layer(x))\n","            x = self.dropout(x)\n","        x = self.output(x)    \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJluO7td5xB7","colab_type":"code","colab":{}},"source":["def train(train_loader, valid_loader, model, criterion, optimizer, \n","          n_epochs=50, saved_model='model.pt'):\n","    '''\n","    Train the model\n","    \n","    Args:\n","        train_loader (DataLoader): DataLoader for train Dataset\n","        valid_loader (DataLoader): DataLoader for valid Dataset\n","        model (nn.Module): model to be trained on\n","        criterion (torch.nn): loss funtion\n","        optimizer (torch.optim): optimization algorithms\n","        n_epochs (int): number of epochs to train the model\n","        saved_model (str): file path for saving model\n","    \n","    Return:\n","        tuple of train_losses, valid_losses\n","    '''\n","\n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = np.Inf # set initial \"min\" to infinity\n","\n","    train_losses = []\n","    valid_losses = []\n","\n","    for epoch in range(n_epochs):\n","        # monitor training loss\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train() # prep model for training\n","        for batch in train_loader:\n","            # clear the gradients of all optimized variables\n","            optimizer.zero_grad()\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(batch['image'].to(device))\n","            # calculate the loss\n","            loss = criterion(output, batch['keypoints'].to(device))\n","            # backward pass: compute gradient of the loss with respect to model parameters\n","            loss.backward()\n","            # perform a single optimization step (parameter update)\n","            optimizer.step()\n","            # update running training loss\n","            train_loss += loss.item()*batch['image'].size(0)\n","\n","        ######################    \n","        # validate the model #\n","        ######################\n","        model.eval() # prep model for evaluation\n","        for batch in valid_loader:\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(batch['image'].to(device))\n","            # calculate the loss\n","            loss = criterion(output, batch['keypoints'].to(device))\n","            # update running validation loss \n","            valid_loss += loss.item()*batch['image'].size(0)\n","\n","        # print training/validation statistics \n","        # calculate average Root Mean Square loss over an epoch\n","        train_loss = np.sqrt(train_loss/len(train_loader.sampler.indices))\n","        valid_loss = np.sqrt(valid_loss/len(valid_loader.sampler.indices))\n","\n","        train_losses.append(train_loss)\n","        valid_losses.append(valid_loss)\n","\n","        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'\n","              .format(epoch+1, train_loss, valid_loss))\n","\n","        # save model if validation loss has decreased\n","        if valid_loss <= valid_loss_min:\n","            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n","                  .format(valid_loss_min, valid_loss))\n","            torch.save(model.state_dict(), saved_model)\n","            valid_loss_min = valid_loss\n","            \n","    return train_losses, valid_losses"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tiC2qb-mWLjt","colab_type":"text"},"source":["# set output_size=18 \n","\n","# Front view key points=9x2=18\n","\n","lr=0.0001\n","\n","n_epoches=500"]},{"cell_type":"code","metadata":{"id":"mft9Yndw59Yb","colab_type":"code","outputId":"a6a0a54f-4e98-4615-a248-a523674441b8","executionInfo":{"status":"ok","timestamp":1584942198653,"user_tz":-480,"elapsed":167403,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from torch import optim\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# output_size ?\n","# Front view keypoints: 9x2 = 18\n","model = MLP(input_size=IMG_SIZE*IMG_SIZE, output_size=18, \n","            hidden_layers=[128, 64], drop_p=0.1)\n","model = model.to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","train_losses, valid_losses = train(train_loader, valid_loader,\n","                                   model, criterion, optimizer,\n","                                   n_epochs=500,\n","                                   saved_model=ROOT_FOLDER+'model.pt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.120717 \tValidation Loss: 0.061749\n","Validation loss decreased (inf --> 0.061749).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.083406 \tValidation Loss: 0.049598\n","Validation loss decreased (0.061749 --> 0.049598).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.075888 \tValidation Loss: 0.038615\n","Validation loss decreased (0.049598 --> 0.038615).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.068336 \tValidation Loss: 0.035752\n","Validation loss decreased (0.038615 --> 0.035752).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.064282 \tValidation Loss: 0.031648\n","Validation loss decreased (0.035752 --> 0.031648).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.059929 \tValidation Loss: 0.030633\n","Validation loss decreased (0.031648 --> 0.030633).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.056649 \tValidation Loss: 0.037229\n","Epoch: 8 \tTraining Loss: 0.055779 \tValidation Loss: 0.029072\n","Validation loss decreased (0.030633 --> 0.029072).  Saving model ...\n","Epoch: 9 \tTraining Loss: 0.052408 \tValidation Loss: 0.032996\n","Epoch: 10 \tTraining Loss: 0.051721 \tValidation Loss: 0.029753\n","Epoch: 11 \tTraining Loss: 0.052554 \tValidation Loss: 0.027610\n","Validation loss decreased (0.029072 --> 0.027610).  Saving model ...\n","Epoch: 12 \tTraining Loss: 0.049661 \tValidation Loss: 0.026675\n","Validation loss decreased (0.027610 --> 0.026675).  Saving model ...\n","Epoch: 13 \tTraining Loss: 0.049019 \tValidation Loss: 0.029813\n","Epoch: 14 \tTraining Loss: 0.048457 \tValidation Loss: 0.026217\n","Validation loss decreased (0.026675 --> 0.026217).  Saving model ...\n","Epoch: 15 \tTraining Loss: 0.047134 \tValidation Loss: 0.026072\n","Validation loss decreased (0.026217 --> 0.026072).  Saving model ...\n","Epoch: 16 \tTraining Loss: 0.045322 \tValidation Loss: 0.025131\n","Validation loss decreased (0.026072 --> 0.025131).  Saving model ...\n","Epoch: 17 \tTraining Loss: 0.045922 \tValidation Loss: 0.024765\n","Validation loss decreased (0.025131 --> 0.024765).  Saving model ...\n","Epoch: 18 \tTraining Loss: 0.045720 \tValidation Loss: 0.029114\n","Epoch: 19 \tTraining Loss: 0.044370 \tValidation Loss: 0.024871\n","Epoch: 20 \tTraining Loss: 0.044835 \tValidation Loss: 0.043937\n","Epoch: 21 \tTraining Loss: 0.046789 \tValidation Loss: 0.032037\n","Epoch: 22 \tTraining Loss: 0.041776 \tValidation Loss: 0.024869\n","Epoch: 23 \tTraining Loss: 0.041540 \tValidation Loss: 0.025088\n","Epoch: 24 \tTraining Loss: 0.041511 \tValidation Loss: 0.025690\n","Epoch: 25 \tTraining Loss: 0.043026 \tValidation Loss: 0.023979\n","Validation loss decreased (0.024765 --> 0.023979).  Saving model ...\n","Epoch: 26 \tTraining Loss: 0.040927 \tValidation Loss: 0.030709\n","Epoch: 27 \tTraining Loss: 0.041521 \tValidation Loss: 0.024923\n","Epoch: 28 \tTraining Loss: 0.040193 \tValidation Loss: 0.023467\n","Validation loss decreased (0.023979 --> 0.023467).  Saving model ...\n","Epoch: 29 \tTraining Loss: 0.040634 \tValidation Loss: 0.032775\n","Epoch: 30 \tTraining Loss: 0.041030 \tValidation Loss: 0.026402\n","Epoch: 31 \tTraining Loss: 0.039003 \tValidation Loss: 0.023129\n","Validation loss decreased (0.023467 --> 0.023129).  Saving model ...\n","Epoch: 32 \tTraining Loss: 0.039901 \tValidation Loss: 0.023189\n","Epoch: 33 \tTraining Loss: 0.039904 \tValidation Loss: 0.029281\n","Epoch: 34 \tTraining Loss: 0.039649 \tValidation Loss: 0.023382\n","Epoch: 35 \tTraining Loss: 0.041156 \tValidation Loss: 0.025220\n","Epoch: 36 \tTraining Loss: 0.037751 \tValidation Loss: 0.028417\n","Epoch: 37 \tTraining Loss: 0.038000 \tValidation Loss: 0.024787\n","Epoch: 38 \tTraining Loss: 0.036717 \tValidation Loss: 0.023748\n","Epoch: 39 \tTraining Loss: 0.036414 \tValidation Loss: 0.023183\n","Epoch: 40 \tTraining Loss: 0.035149 \tValidation Loss: 0.025120\n","Epoch: 41 \tTraining Loss: 0.037786 \tValidation Loss: 0.023817\n","Epoch: 42 \tTraining Loss: 0.036768 \tValidation Loss: 0.022499\n","Validation loss decreased (0.023129 --> 0.022499).  Saving model ...\n","Epoch: 43 \tTraining Loss: 0.036133 \tValidation Loss: 0.023152\n","Epoch: 44 \tTraining Loss: 0.035483 \tValidation Loss: 0.026015\n","Epoch: 45 \tTraining Loss: 0.035070 \tValidation Loss: 0.022792\n","Epoch: 46 \tTraining Loss: 0.034437 \tValidation Loss: 0.025663\n","Epoch: 47 \tTraining Loss: 0.034713 \tValidation Loss: 0.022215\n","Validation loss decreased (0.022499 --> 0.022215).  Saving model ...\n","Epoch: 48 \tTraining Loss: 0.034658 \tValidation Loss: 0.024218\n","Epoch: 49 \tTraining Loss: 0.034047 \tValidation Loss: 0.021899\n","Validation loss decreased (0.022215 --> 0.021899).  Saving model ...\n","Epoch: 50 \tTraining Loss: 0.035124 \tValidation Loss: 0.027471\n","Epoch: 51 \tTraining Loss: 0.035296 \tValidation Loss: 0.029891\n","Epoch: 52 \tTraining Loss: 0.034906 \tValidation Loss: 0.026929\n","Epoch: 53 \tTraining Loss: 0.035167 \tValidation Loss: 0.023279\n","Epoch: 54 \tTraining Loss: 0.034406 \tValidation Loss: 0.023843\n","Epoch: 55 \tTraining Loss: 0.033399 \tValidation Loss: 0.024782\n","Epoch: 56 \tTraining Loss: 0.032650 \tValidation Loss: 0.021808\n","Validation loss decreased (0.021899 --> 0.021808).  Saving model ...\n","Epoch: 57 \tTraining Loss: 0.032107 \tValidation Loss: 0.021989\n","Epoch: 58 \tTraining Loss: 0.031760 \tValidation Loss: 0.024522\n","Epoch: 59 \tTraining Loss: 0.031817 \tValidation Loss: 0.021816\n","Epoch: 60 \tTraining Loss: 0.031913 \tValidation Loss: 0.026806\n","Epoch: 61 \tTraining Loss: 0.031909 \tValidation Loss: 0.023339\n","Epoch: 62 \tTraining Loss: 0.031160 \tValidation Loss: 0.026024\n","Epoch: 63 \tTraining Loss: 0.031674 \tValidation Loss: 0.028227\n","Epoch: 64 \tTraining Loss: 0.033168 \tValidation Loss: 0.021281\n","Validation loss decreased (0.021808 --> 0.021281).  Saving model ...\n","Epoch: 65 \tTraining Loss: 0.031713 \tValidation Loss: 0.021781\n","Epoch: 66 \tTraining Loss: 0.031160 \tValidation Loss: 0.023021\n","Epoch: 67 \tTraining Loss: 0.031244 \tValidation Loss: 0.021701\n","Epoch: 68 \tTraining Loss: 0.030670 \tValidation Loss: 0.022161\n","Epoch: 69 \tTraining Loss: 0.030421 \tValidation Loss: 0.021573\n","Epoch: 70 \tTraining Loss: 0.029626 \tValidation Loss: 0.021062\n","Validation loss decreased (0.021281 --> 0.021062).  Saving model ...\n","Epoch: 71 \tTraining Loss: 0.029867 \tValidation Loss: 0.020952\n","Validation loss decreased (0.021062 --> 0.020952).  Saving model ...\n","Epoch: 72 \tTraining Loss: 0.032305 \tValidation Loss: 0.021730\n","Epoch: 73 \tTraining Loss: 0.031718 \tValidation Loss: 0.030933\n","Epoch: 74 \tTraining Loss: 0.030812 \tValidation Loss: 0.020877\n","Validation loss decreased (0.020952 --> 0.020877).  Saving model ...\n","Epoch: 75 \tTraining Loss: 0.029301 \tValidation Loss: 0.022414\n","Epoch: 76 \tTraining Loss: 0.028869 \tValidation Loss: 0.021630\n","Epoch: 77 \tTraining Loss: 0.028725 \tValidation Loss: 0.020871\n","Validation loss decreased (0.020877 --> 0.020871).  Saving model ...\n","Epoch: 78 \tTraining Loss: 0.028972 \tValidation Loss: 0.022239\n","Epoch: 79 \tTraining Loss: 0.028512 \tValidation Loss: 0.023761\n","Epoch: 80 \tTraining Loss: 0.029018 \tValidation Loss: 0.024241\n","Epoch: 81 \tTraining Loss: 0.028478 \tValidation Loss: 0.021093\n","Epoch: 82 \tTraining Loss: 0.028106 \tValidation Loss: 0.022931\n","Epoch: 83 \tTraining Loss: 0.027630 \tValidation Loss: 0.022373\n","Epoch: 84 \tTraining Loss: 0.027169 \tValidation Loss: 0.021874\n","Epoch: 85 \tTraining Loss: 0.027672 \tValidation Loss: 0.021246\n","Epoch: 86 \tTraining Loss: 0.027327 \tValidation Loss: 0.021318\n","Epoch: 87 \tTraining Loss: 0.027288 \tValidation Loss: 0.022921\n","Epoch: 88 \tTraining Loss: 0.027213 \tValidation Loss: 0.020379\n","Validation loss decreased (0.020871 --> 0.020379).  Saving model ...\n","Epoch: 89 \tTraining Loss: 0.027056 \tValidation Loss: 0.020758\n","Epoch: 90 \tTraining Loss: 0.027341 \tValidation Loss: 0.021768\n","Epoch: 91 \tTraining Loss: 0.026520 \tValidation Loss: 0.021378\n","Epoch: 92 \tTraining Loss: 0.026367 \tValidation Loss: 0.021531\n","Epoch: 93 \tTraining Loss: 0.027508 \tValidation Loss: 0.021226\n","Epoch: 94 \tTraining Loss: 0.026167 \tValidation Loss: 0.020408\n","Epoch: 95 \tTraining Loss: 0.026490 \tValidation Loss: 0.022510\n","Epoch: 96 \tTraining Loss: 0.025893 \tValidation Loss: 0.020554\n","Epoch: 97 \tTraining Loss: 0.025648 \tValidation Loss: 0.023905\n","Epoch: 98 \tTraining Loss: 0.025543 \tValidation Loss: 0.020562\n","Epoch: 99 \tTraining Loss: 0.025738 \tValidation Loss: 0.020198\n","Validation loss decreased (0.020379 --> 0.020198).  Saving model ...\n","Epoch: 100 \tTraining Loss: 0.026462 \tValidation Loss: 0.022717\n","Epoch: 101 \tTraining Loss: 0.025815 \tValidation Loss: 0.020407\n","Epoch: 102 \tTraining Loss: 0.025479 \tValidation Loss: 0.021149\n","Epoch: 103 \tTraining Loss: 0.024993 \tValidation Loss: 0.021189\n","Epoch: 104 \tTraining Loss: 0.025517 \tValidation Loss: 0.023135\n","Epoch: 105 \tTraining Loss: 0.024799 \tValidation Loss: 0.024828\n","Epoch: 106 \tTraining Loss: 0.026096 \tValidation Loss: 0.020268\n","Epoch: 107 \tTraining Loss: 0.025364 \tValidation Loss: 0.020625\n","Epoch: 108 \tTraining Loss: 0.024783 \tValidation Loss: 0.020580\n","Epoch: 109 \tTraining Loss: 0.025731 \tValidation Loss: 0.020468\n","Epoch: 110 \tTraining Loss: 0.025142 \tValidation Loss: 0.022137\n","Epoch: 111 \tTraining Loss: 0.024853 \tValidation Loss: 0.021595\n","Epoch: 112 \tTraining Loss: 0.024738 \tValidation Loss: 0.020840\n","Epoch: 113 \tTraining Loss: 0.024237 \tValidation Loss: 0.020669\n","Epoch: 114 \tTraining Loss: 0.023691 \tValidation Loss: 0.019845\n","Validation loss decreased (0.020198 --> 0.019845).  Saving model ...\n","Epoch: 115 \tTraining Loss: 0.024569 \tValidation Loss: 0.023203\n","Epoch: 116 \tTraining Loss: 0.023776 \tValidation Loss: 0.020570\n","Epoch: 117 \tTraining Loss: 0.023936 \tValidation Loss: 0.019437\n","Validation loss decreased (0.019845 --> 0.019437).  Saving model ...\n","Epoch: 118 \tTraining Loss: 0.023281 \tValidation Loss: 0.021387\n","Epoch: 119 \tTraining Loss: 0.023287 \tValidation Loss: 0.022041\n","Epoch: 120 \tTraining Loss: 0.023575 \tValidation Loss: 0.019966\n","Epoch: 121 \tTraining Loss: 0.023407 \tValidation Loss: 0.019230\n","Validation loss decreased (0.019437 --> 0.019230).  Saving model ...\n","Epoch: 122 \tTraining Loss: 0.023328 \tValidation Loss: 0.022092\n","Epoch: 123 \tTraining Loss: 0.022571 \tValidation Loss: 0.022304\n","Epoch: 124 \tTraining Loss: 0.022852 \tValidation Loss: 0.020767\n","Epoch: 125 \tTraining Loss: 0.022811 \tValidation Loss: 0.020912\n","Epoch: 126 \tTraining Loss: 0.022818 \tValidation Loss: 0.019517\n","Epoch: 127 \tTraining Loss: 0.022658 \tValidation Loss: 0.019810\n","Epoch: 128 \tTraining Loss: 0.022260 \tValidation Loss: 0.020392\n","Epoch: 129 \tTraining Loss: 0.022757 \tValidation Loss: 0.019742\n","Epoch: 130 \tTraining Loss: 0.022789 \tValidation Loss: 0.020410\n","Epoch: 131 \tTraining Loss: 0.022896 \tValidation Loss: 0.019515\n","Epoch: 132 \tTraining Loss: 0.023056 \tValidation Loss: 0.019546\n","Epoch: 133 \tTraining Loss: 0.022392 \tValidation Loss: 0.019677\n","Epoch: 134 \tTraining Loss: 0.022557 \tValidation Loss: 0.019179\n","Validation loss decreased (0.019230 --> 0.019179).  Saving model ...\n","Epoch: 135 \tTraining Loss: 0.022427 \tValidation Loss: 0.021091\n","Epoch: 136 \tTraining Loss: 0.022218 \tValidation Loss: 0.020374\n","Epoch: 137 \tTraining Loss: 0.022129 \tValidation Loss: 0.019244\n","Epoch: 138 \tTraining Loss: 0.022328 \tValidation Loss: 0.019922\n","Epoch: 139 \tTraining Loss: 0.021876 \tValidation Loss: 0.019964\n","Epoch: 140 \tTraining Loss: 0.021608 \tValidation Loss: 0.020892\n","Epoch: 141 \tTraining Loss: 0.022272 \tValidation Loss: 0.019740\n","Epoch: 142 \tTraining Loss: 0.021734 \tValidation Loss: 0.019090\n","Validation loss decreased (0.019179 --> 0.019090).  Saving model ...\n","Epoch: 143 \tTraining Loss: 0.021630 \tValidation Loss: 0.019718\n","Epoch: 144 \tTraining Loss: 0.021605 \tValidation Loss: 0.019440\n","Epoch: 145 \tTraining Loss: 0.021480 \tValidation Loss: 0.020067\n","Epoch: 146 \tTraining Loss: 0.021738 \tValidation Loss: 0.022395\n","Epoch: 147 \tTraining Loss: 0.021612 \tValidation Loss: 0.021192\n","Epoch: 148 \tTraining Loss: 0.021282 \tValidation Loss: 0.020044\n","Epoch: 149 \tTraining Loss: 0.021391 \tValidation Loss: 0.019044\n","Validation loss decreased (0.019090 --> 0.019044).  Saving model ...\n","Epoch: 150 \tTraining Loss: 0.021328 \tValidation Loss: 0.020568\n","Epoch: 151 \tTraining Loss: 0.021382 \tValidation Loss: 0.019316\n","Epoch: 152 \tTraining Loss: 0.020895 \tValidation Loss: 0.019548\n","Epoch: 153 \tTraining Loss: 0.020019 \tValidation Loss: 0.019422\n","Epoch: 154 \tTraining Loss: 0.020881 \tValidation Loss: 0.019320\n","Epoch: 155 \tTraining Loss: 0.021075 \tValidation Loss: 0.021961\n","Epoch: 156 \tTraining Loss: 0.020740 \tValidation Loss: 0.020657\n","Epoch: 157 \tTraining Loss: 0.020647 \tValidation Loss: 0.019891\n","Epoch: 158 \tTraining Loss: 0.020528 \tValidation Loss: 0.018858\n","Validation loss decreased (0.019044 --> 0.018858).  Saving model ...\n","Epoch: 159 \tTraining Loss: 0.020625 \tValidation Loss: 0.018966\n","Epoch: 160 \tTraining Loss: 0.019950 \tValidation Loss: 0.020093\n","Epoch: 161 \tTraining Loss: 0.019899 \tValidation Loss: 0.019548\n","Epoch: 162 \tTraining Loss: 0.020342 \tValidation Loss: 0.019038\n","Epoch: 163 \tTraining Loss: 0.020070 \tValidation Loss: 0.018137\n","Validation loss decreased (0.018858 --> 0.018137).  Saving model ...\n","Epoch: 164 \tTraining Loss: 0.020277 \tValidation Loss: 0.019316\n","Epoch: 165 \tTraining Loss: 0.020061 \tValidation Loss: 0.019226\n","Epoch: 166 \tTraining Loss: 0.020601 \tValidation Loss: 0.018994\n","Epoch: 167 \tTraining Loss: 0.020485 \tValidation Loss: 0.019312\n","Epoch: 168 \tTraining Loss: 0.020556 \tValidation Loss: 0.019111\n","Epoch: 169 \tTraining Loss: 0.019558 \tValidation Loss: 0.019720\n","Epoch: 170 \tTraining Loss: 0.019949 \tValidation Loss: 0.019092\n","Epoch: 171 \tTraining Loss: 0.020361 \tValidation Loss: 0.018602\n","Epoch: 172 \tTraining Loss: 0.019418 \tValidation Loss: 0.019400\n","Epoch: 173 \tTraining Loss: 0.019628 \tValidation Loss: 0.018744\n","Epoch: 174 \tTraining Loss: 0.019532 \tValidation Loss: 0.018623\n","Epoch: 175 \tTraining Loss: 0.019482 \tValidation Loss: 0.018973\n","Epoch: 176 \tTraining Loss: 0.020302 \tValidation Loss: 0.019776\n","Epoch: 177 \tTraining Loss: 0.019573 \tValidation Loss: 0.020322\n","Epoch: 178 \tTraining Loss: 0.020073 \tValidation Loss: 0.020145\n","Epoch: 179 \tTraining Loss: 0.019589 \tValidation Loss: 0.019565\n","Epoch: 180 \tTraining Loss: 0.019446 \tValidation Loss: 0.019985\n","Epoch: 181 \tTraining Loss: 0.020148 \tValidation Loss: 0.018625\n","Epoch: 182 \tTraining Loss: 0.019547 \tValidation Loss: 0.019284\n","Epoch: 183 \tTraining Loss: 0.019035 \tValidation Loss: 0.019047\n","Epoch: 184 \tTraining Loss: 0.019771 \tValidation Loss: 0.019303\n","Epoch: 185 \tTraining Loss: 0.019650 \tValidation Loss: 0.018560\n","Epoch: 186 \tTraining Loss: 0.019419 \tValidation Loss: 0.019462\n","Epoch: 187 \tTraining Loss: 0.019434 \tValidation Loss: 0.019641\n","Epoch: 188 \tTraining Loss: 0.019145 \tValidation Loss: 0.019684\n","Epoch: 189 \tTraining Loss: 0.018971 \tValidation Loss: 0.019370\n","Epoch: 190 \tTraining Loss: 0.019701 \tValidation Loss: 0.019294\n","Epoch: 191 \tTraining Loss: 0.019477 \tValidation Loss: 0.019415\n","Epoch: 192 \tTraining Loss: 0.019133 \tValidation Loss: 0.019710\n","Epoch: 193 \tTraining Loss: 0.019321 \tValidation Loss: 0.019527\n","Epoch: 194 \tTraining Loss: 0.018754 \tValidation Loss: 0.019349\n","Epoch: 195 \tTraining Loss: 0.019365 \tValidation Loss: 0.019376\n","Epoch: 196 \tTraining Loss: 0.018572 \tValidation Loss: 0.019520\n","Epoch: 197 \tTraining Loss: 0.018549 \tValidation Loss: 0.019105\n","Epoch: 198 \tTraining Loss: 0.018351 \tValidation Loss: 0.018641\n","Epoch: 199 \tTraining Loss: 0.018803 \tValidation Loss: 0.019510\n","Epoch: 200 \tTraining Loss: 0.019020 \tValidation Loss: 0.019146\n","Epoch: 201 \tTraining Loss: 0.018607 \tValidation Loss: 0.019103\n","Epoch: 202 \tTraining Loss: 0.018710 \tValidation Loss: 0.018745\n","Epoch: 203 \tTraining Loss: 0.018944 \tValidation Loss: 0.019139\n","Epoch: 204 \tTraining Loss: 0.019515 \tValidation Loss: 0.018653\n","Epoch: 205 \tTraining Loss: 0.018730 \tValidation Loss: 0.018185\n","Epoch: 206 \tTraining Loss: 0.018408 \tValidation Loss: 0.018157\n","Epoch: 207 \tTraining Loss: 0.018861 \tValidation Loss: 0.019040\n","Epoch: 208 \tTraining Loss: 0.019028 \tValidation Loss: 0.018384\n","Epoch: 209 \tTraining Loss: 0.018731 \tValidation Loss: 0.018906\n","Epoch: 210 \tTraining Loss: 0.018211 \tValidation Loss: 0.018121\n","Validation loss decreased (0.018137 --> 0.018121).  Saving model ...\n","Epoch: 211 \tTraining Loss: 0.017945 \tValidation Loss: 0.019202\n","Epoch: 212 \tTraining Loss: 0.018714 \tValidation Loss: 0.018524\n","Epoch: 213 \tTraining Loss: 0.018885 \tValidation Loss: 0.018792\n","Epoch: 214 \tTraining Loss: 0.018516 \tValidation Loss: 0.018777\n","Epoch: 215 \tTraining Loss: 0.018723 \tValidation Loss: 0.018312\n","Epoch: 216 \tTraining Loss: 0.018335 \tValidation Loss: 0.018577\n","Epoch: 217 \tTraining Loss: 0.018358 \tValidation Loss: 0.018373\n","Epoch: 218 \tTraining Loss: 0.018300 \tValidation Loss: 0.018582\n","Epoch: 219 \tTraining Loss: 0.018729 \tValidation Loss: 0.018779\n","Epoch: 220 \tTraining Loss: 0.018521 \tValidation Loss: 0.018600\n","Epoch: 221 \tTraining Loss: 0.018210 \tValidation Loss: 0.019126\n","Epoch: 222 \tTraining Loss: 0.018408 \tValidation Loss: 0.018905\n","Epoch: 223 \tTraining Loss: 0.019456 \tValidation Loss: 0.019294\n","Epoch: 224 \tTraining Loss: 0.018300 \tValidation Loss: 0.019021\n","Epoch: 225 \tTraining Loss: 0.018268 \tValidation Loss: 0.018966\n","Epoch: 226 \tTraining Loss: 0.018001 \tValidation Loss: 0.018800\n","Epoch: 227 \tTraining Loss: 0.018483 \tValidation Loss: 0.018442\n","Epoch: 228 \tTraining Loss: 0.018437 \tValidation Loss: 0.018753\n","Epoch: 229 \tTraining Loss: 0.018033 \tValidation Loss: 0.018876\n","Epoch: 230 \tTraining Loss: 0.018296 \tValidation Loss: 0.018936\n","Epoch: 231 \tTraining Loss: 0.017987 \tValidation Loss: 0.018393\n","Epoch: 232 \tTraining Loss: 0.018109 \tValidation Loss: 0.019091\n","Epoch: 233 \tTraining Loss: 0.017625 \tValidation Loss: 0.018422\n","Epoch: 234 \tTraining Loss: 0.017960 \tValidation Loss: 0.018558\n","Epoch: 235 \tTraining Loss: 0.017707 \tValidation Loss: 0.018505\n","Epoch: 236 \tTraining Loss: 0.017698 \tValidation Loss: 0.018950\n","Epoch: 237 \tTraining Loss: 0.017786 \tValidation Loss: 0.018587\n","Epoch: 238 \tTraining Loss: 0.017463 \tValidation Loss: 0.018930\n","Epoch: 239 \tTraining Loss: 0.017956 \tValidation Loss: 0.018216\n","Epoch: 240 \tTraining Loss: 0.017803 \tValidation Loss: 0.018983\n","Epoch: 241 \tTraining Loss: 0.018158 \tValidation Loss: 0.019719\n","Epoch: 242 \tTraining Loss: 0.018214 \tValidation Loss: 0.018624\n","Epoch: 243 \tTraining Loss: 0.017378 \tValidation Loss: 0.018736\n","Epoch: 244 \tTraining Loss: 0.017500 \tValidation Loss: 0.018084\n","Validation loss decreased (0.018121 --> 0.018084).  Saving model ...\n","Epoch: 245 \tTraining Loss: 0.017858 \tValidation Loss: 0.018343\n","Epoch: 246 \tTraining Loss: 0.017800 \tValidation Loss: 0.019411\n","Epoch: 247 \tTraining Loss: 0.017408 \tValidation Loss: 0.017920\n","Validation loss decreased (0.018084 --> 0.017920).  Saving model ...\n","Epoch: 248 \tTraining Loss: 0.017464 \tValidation Loss: 0.018211\n","Epoch: 249 \tTraining Loss: 0.017450 \tValidation Loss: 0.018788\n","Epoch: 250 \tTraining Loss: 0.017325 \tValidation Loss: 0.018988\n","Epoch: 251 \tTraining Loss: 0.017122 \tValidation Loss: 0.018423\n","Epoch: 252 \tTraining Loss: 0.017583 \tValidation Loss: 0.018427\n","Epoch: 253 \tTraining Loss: 0.017490 \tValidation Loss: 0.018202\n","Epoch: 254 \tTraining Loss: 0.017289 \tValidation Loss: 0.018734\n","Epoch: 255 \tTraining Loss: 0.017752 \tValidation Loss: 0.018295\n","Epoch: 256 \tTraining Loss: 0.017515 \tValidation Loss: 0.018873\n","Epoch: 257 \tTraining Loss: 0.017415 \tValidation Loss: 0.018963\n","Epoch: 258 \tTraining Loss: 0.017166 \tValidation Loss: 0.019666\n","Epoch: 259 \tTraining Loss: 0.016863 \tValidation Loss: 0.018471\n","Epoch: 260 \tTraining Loss: 0.017314 \tValidation Loss: 0.018628\n","Epoch: 261 \tTraining Loss: 0.017408 \tValidation Loss: 0.018626\n","Epoch: 262 \tTraining Loss: 0.017437 \tValidation Loss: 0.019002\n","Epoch: 263 \tTraining Loss: 0.017696 \tValidation Loss: 0.019023\n","Epoch: 264 \tTraining Loss: 0.017737 \tValidation Loss: 0.018644\n","Epoch: 265 \tTraining Loss: 0.017380 \tValidation Loss: 0.018106\n","Epoch: 266 \tTraining Loss: 0.017572 \tValidation Loss: 0.018419\n","Epoch: 267 \tTraining Loss: 0.017107 \tValidation Loss: 0.018435\n","Epoch: 268 \tTraining Loss: 0.017113 \tValidation Loss: 0.019272\n","Epoch: 269 \tTraining Loss: 0.017539 \tValidation Loss: 0.018873\n","Epoch: 270 \tTraining Loss: 0.018033 \tValidation Loss: 0.019019\n","Epoch: 271 \tTraining Loss: 0.017893 \tValidation Loss: 0.018371\n","Epoch: 272 \tTraining Loss: 0.017888 \tValidation Loss: 0.018803\n","Epoch: 273 \tTraining Loss: 0.016971 \tValidation Loss: 0.018577\n","Epoch: 274 \tTraining Loss: 0.016710 \tValidation Loss: 0.018267\n","Epoch: 275 \tTraining Loss: 0.017130 \tValidation Loss: 0.018511\n","Epoch: 276 \tTraining Loss: 0.017180 \tValidation Loss: 0.018376\n","Epoch: 277 \tTraining Loss: 0.017017 \tValidation Loss: 0.018717\n","Epoch: 278 \tTraining Loss: 0.016553 \tValidation Loss: 0.018277\n","Epoch: 279 \tTraining Loss: 0.016598 \tValidation Loss: 0.018275\n","Epoch: 280 \tTraining Loss: 0.016631 \tValidation Loss: 0.018471\n","Epoch: 281 \tTraining Loss: 0.016229 \tValidation Loss: 0.018718\n","Epoch: 282 \tTraining Loss: 0.016936 \tValidation Loss: 0.018532\n","Epoch: 283 \tTraining Loss: 0.016428 \tValidation Loss: 0.018974\n","Epoch: 284 \tTraining Loss: 0.016747 \tValidation Loss: 0.018783\n","Epoch: 285 \tTraining Loss: 0.016910 \tValidation Loss: 0.018144\n","Epoch: 286 \tTraining Loss: 0.016519 \tValidation Loss: 0.018616\n","Epoch: 287 \tTraining Loss: 0.016962 \tValidation Loss: 0.018327\n","Epoch: 288 \tTraining Loss: 0.016373 \tValidation Loss: 0.018511\n","Epoch: 289 \tTraining Loss: 0.016817 \tValidation Loss: 0.018509\n","Epoch: 290 \tTraining Loss: 0.016853 \tValidation Loss: 0.018126\n","Epoch: 291 \tTraining Loss: 0.016851 \tValidation Loss: 0.018291\n","Epoch: 292 \tTraining Loss: 0.016764 \tValidation Loss: 0.018092\n","Epoch: 293 \tTraining Loss: 0.016543 \tValidation Loss: 0.018548\n","Epoch: 294 \tTraining Loss: 0.016800 \tValidation Loss: 0.018730\n","Epoch: 295 \tTraining Loss: 0.016573 \tValidation Loss: 0.018504\n","Epoch: 296 \tTraining Loss: 0.016266 \tValidation Loss: 0.018408\n","Epoch: 297 \tTraining Loss: 0.016817 \tValidation Loss: 0.018457\n","Epoch: 298 \tTraining Loss: 0.016333 \tValidation Loss: 0.018308\n","Epoch: 299 \tTraining Loss: 0.016575 \tValidation Loss: 0.017966\n","Epoch: 300 \tTraining Loss: 0.016377 \tValidation Loss: 0.018381\n","Epoch: 301 \tTraining Loss: 0.016103 \tValidation Loss: 0.018796\n","Epoch: 302 \tTraining Loss: 0.016171 \tValidation Loss: 0.018358\n","Epoch: 303 \tTraining Loss: 0.015938 \tValidation Loss: 0.018177\n","Epoch: 304 \tTraining Loss: 0.016679 \tValidation Loss: 0.018736\n","Epoch: 305 \tTraining Loss: 0.016843 \tValidation Loss: 0.018833\n","Epoch: 306 \tTraining Loss: 0.016666 \tValidation Loss: 0.018128\n","Epoch: 307 \tTraining Loss: 0.016497 \tValidation Loss: 0.018514\n","Epoch: 308 \tTraining Loss: 0.016193 \tValidation Loss: 0.018182\n","Epoch: 309 \tTraining Loss: 0.016523 \tValidation Loss: 0.018562\n","Epoch: 310 \tTraining Loss: 0.016383 \tValidation Loss: 0.018446\n","Epoch: 311 \tTraining Loss: 0.016044 \tValidation Loss: 0.018546\n","Epoch: 312 \tTraining Loss: 0.016136 \tValidation Loss: 0.018801\n","Epoch: 313 \tTraining Loss: 0.016190 \tValidation Loss: 0.018134\n","Epoch: 314 \tTraining Loss: 0.015509 \tValidation Loss: 0.018244\n","Epoch: 315 \tTraining Loss: 0.016304 \tValidation Loss: 0.018693\n","Epoch: 316 \tTraining Loss: 0.016379 \tValidation Loss: 0.018101\n","Epoch: 317 \tTraining Loss: 0.016882 \tValidation Loss: 0.018844\n","Epoch: 318 \tTraining Loss: 0.016771 \tValidation Loss: 0.018391\n","Epoch: 319 \tTraining Loss: 0.016655 \tValidation Loss: 0.017977\n","Epoch: 320 \tTraining Loss: 0.015916 \tValidation Loss: 0.018358\n","Epoch: 321 \tTraining Loss: 0.016166 \tValidation Loss: 0.018108\n","Epoch: 322 \tTraining Loss: 0.015834 \tValidation Loss: 0.017871\n","Validation loss decreased (0.017920 --> 0.017871).  Saving model ...\n","Epoch: 323 \tTraining Loss: 0.015879 \tValidation Loss: 0.018587\n","Epoch: 324 \tTraining Loss: 0.016012 \tValidation Loss: 0.018379\n","Epoch: 325 \tTraining Loss: 0.016278 \tValidation Loss: 0.018608\n","Epoch: 326 \tTraining Loss: 0.016872 \tValidation Loss: 0.019375\n","Epoch: 327 \tTraining Loss: 0.016111 \tValidation Loss: 0.018729\n","Epoch: 328 \tTraining Loss: 0.016208 \tValidation Loss: 0.018691\n","Epoch: 329 \tTraining Loss: 0.015916 \tValidation Loss: 0.018351\n","Epoch: 330 \tTraining Loss: 0.015659 \tValidation Loss: 0.018431\n","Epoch: 331 \tTraining Loss: 0.016206 \tValidation Loss: 0.017849\n","Validation loss decreased (0.017871 --> 0.017849).  Saving model ...\n","Epoch: 332 \tTraining Loss: 0.015213 \tValidation Loss: 0.018187\n","Epoch: 333 \tTraining Loss: 0.015774 \tValidation Loss: 0.018406\n","Epoch: 334 \tTraining Loss: 0.016390 \tValidation Loss: 0.018205\n","Epoch: 335 \tTraining Loss: 0.015612 \tValidation Loss: 0.018434\n","Epoch: 336 \tTraining Loss: 0.016415 \tValidation Loss: 0.018622\n","Epoch: 337 \tTraining Loss: 0.015944 \tValidation Loss: 0.018585\n","Epoch: 338 \tTraining Loss: 0.016006 \tValidation Loss: 0.018132\n","Epoch: 339 \tTraining Loss: 0.015535 \tValidation Loss: 0.018068\n","Epoch: 340 \tTraining Loss: 0.015407 \tValidation Loss: 0.018300\n","Epoch: 341 \tTraining Loss: 0.016445 \tValidation Loss: 0.018445\n","Epoch: 342 \tTraining Loss: 0.015885 \tValidation Loss: 0.018830\n","Epoch: 343 \tTraining Loss: 0.015946 \tValidation Loss: 0.018199\n","Epoch: 344 \tTraining Loss: 0.015472 \tValidation Loss: 0.018368\n","Epoch: 345 \tTraining Loss: 0.016084 \tValidation Loss: 0.018501\n","Epoch: 346 \tTraining Loss: 0.016557 \tValidation Loss: 0.018756\n","Epoch: 347 \tTraining Loss: 0.015345 \tValidation Loss: 0.018091\n","Epoch: 348 \tTraining Loss: 0.015581 \tValidation Loss: 0.019031\n","Epoch: 349 \tTraining Loss: 0.016207 \tValidation Loss: 0.018326\n","Epoch: 350 \tTraining Loss: 0.015734 \tValidation Loss: 0.018218\n","Epoch: 351 \tTraining Loss: 0.015769 \tValidation Loss: 0.018399\n","Epoch: 352 \tTraining Loss: 0.016153 \tValidation Loss: 0.018197\n","Epoch: 353 \tTraining Loss: 0.016052 \tValidation Loss: 0.018431\n","Epoch: 354 \tTraining Loss: 0.015617 \tValidation Loss: 0.018540\n","Epoch: 355 \tTraining Loss: 0.015477 \tValidation Loss: 0.018494\n","Epoch: 356 \tTraining Loss: 0.015575 \tValidation Loss: 0.018607\n","Epoch: 357 \tTraining Loss: 0.015014 \tValidation Loss: 0.018357\n","Epoch: 358 \tTraining Loss: 0.015437 \tValidation Loss: 0.018281\n","Epoch: 359 \tTraining Loss: 0.015485 \tValidation Loss: 0.018470\n","Epoch: 360 \tTraining Loss: 0.015513 \tValidation Loss: 0.018421\n","Epoch: 361 \tTraining Loss: 0.015293 \tValidation Loss: 0.018212\n","Epoch: 362 \tTraining Loss: 0.015227 \tValidation Loss: 0.018218\n","Epoch: 363 \tTraining Loss: 0.015337 \tValidation Loss: 0.018337\n","Epoch: 364 \tTraining Loss: 0.015480 \tValidation Loss: 0.018121\n","Epoch: 365 \tTraining Loss: 0.015432 \tValidation Loss: 0.018378\n","Epoch: 366 \tTraining Loss: 0.015542 \tValidation Loss: 0.018632\n","Epoch: 367 \tTraining Loss: 0.015214 \tValidation Loss: 0.018486\n","Epoch: 368 \tTraining Loss: 0.015319 \tValidation Loss: 0.018239\n","Epoch: 369 \tTraining Loss: 0.015365 \tValidation Loss: 0.017848\n","Validation loss decreased (0.017849 --> 0.017848).  Saving model ...\n","Epoch: 370 \tTraining Loss: 0.014924 \tValidation Loss: 0.018138\n","Epoch: 371 \tTraining Loss: 0.015098 \tValidation Loss: 0.017880\n","Epoch: 372 \tTraining Loss: 0.015149 \tValidation Loss: 0.017650\n","Validation loss decreased (0.017848 --> 0.017650).  Saving model ...\n","Epoch: 373 \tTraining Loss: 0.014858 \tValidation Loss: 0.018033\n","Epoch: 374 \tTraining Loss: 0.015132 \tValidation Loss: 0.017925\n","Epoch: 375 \tTraining Loss: 0.015101 \tValidation Loss: 0.018124\n","Epoch: 376 \tTraining Loss: 0.014762 \tValidation Loss: 0.018215\n","Epoch: 377 \tTraining Loss: 0.015267 \tValidation Loss: 0.018088\n","Epoch: 378 \tTraining Loss: 0.015070 \tValidation Loss: 0.018113\n","Epoch: 379 \tTraining Loss: 0.014957 \tValidation Loss: 0.018320\n","Epoch: 380 \tTraining Loss: 0.015228 \tValidation Loss: 0.018460\n","Epoch: 381 \tTraining Loss: 0.015203 \tValidation Loss: 0.018340\n","Epoch: 382 \tTraining Loss: 0.015321 \tValidation Loss: 0.018270\n","Epoch: 383 \tTraining Loss: 0.015577 \tValidation Loss: 0.018017\n","Epoch: 384 \tTraining Loss: 0.015321 \tValidation Loss: 0.018420\n","Epoch: 385 \tTraining Loss: 0.015437 \tValidation Loss: 0.018526\n","Epoch: 386 \tTraining Loss: 0.015574 \tValidation Loss: 0.018200\n","Epoch: 387 \tTraining Loss: 0.015314 \tValidation Loss: 0.018010\n","Epoch: 388 \tTraining Loss: 0.015016 \tValidation Loss: 0.018178\n","Epoch: 389 \tTraining Loss: 0.015100 \tValidation Loss: 0.018210\n","Epoch: 390 \tTraining Loss: 0.015078 \tValidation Loss: 0.018127\n","Epoch: 391 \tTraining Loss: 0.014987 \tValidation Loss: 0.018417\n","Epoch: 392 \tTraining Loss: 0.014744 \tValidation Loss: 0.018353\n","Epoch: 393 \tTraining Loss: 0.014781 \tValidation Loss: 0.018282\n","Epoch: 394 \tTraining Loss: 0.014659 \tValidation Loss: 0.018064\n","Epoch: 395 \tTraining Loss: 0.014293 \tValidation Loss: 0.018539\n","Epoch: 396 \tTraining Loss: 0.015400 \tValidation Loss: 0.018779\n","Epoch: 397 \tTraining Loss: 0.015222 \tValidation Loss: 0.018464\n","Epoch: 398 \tTraining Loss: 0.015016 \tValidation Loss: 0.018142\n","Epoch: 399 \tTraining Loss: 0.015250 \tValidation Loss: 0.018404\n","Epoch: 400 \tTraining Loss: 0.014706 \tValidation Loss: 0.018298\n","Epoch: 401 \tTraining Loss: 0.014853 \tValidation Loss: 0.018418\n","Epoch: 402 \tTraining Loss: 0.014923 \tValidation Loss: 0.018373\n","Epoch: 403 \tTraining Loss: 0.014722 \tValidation Loss: 0.017925\n","Epoch: 404 \tTraining Loss: 0.014757 \tValidation Loss: 0.018898\n","Epoch: 405 \tTraining Loss: 0.014684 \tValidation Loss: 0.018827\n","Epoch: 406 \tTraining Loss: 0.014672 \tValidation Loss: 0.018397\n","Epoch: 407 \tTraining Loss: 0.015014 \tValidation Loss: 0.018165\n","Epoch: 408 \tTraining Loss: 0.014740 \tValidation Loss: 0.019014\n","Epoch: 409 \tTraining Loss: 0.014769 \tValidation Loss: 0.018731\n","Epoch: 410 \tTraining Loss: 0.015203 \tValidation Loss: 0.018285\n","Epoch: 411 \tTraining Loss: 0.015224 \tValidation Loss: 0.018936\n","Epoch: 412 \tTraining Loss: 0.015220 \tValidation Loss: 0.018274\n","Epoch: 413 \tTraining Loss: 0.014568 \tValidation Loss: 0.017883\n","Epoch: 414 \tTraining Loss: 0.014583 \tValidation Loss: 0.018231\n","Epoch: 415 \tTraining Loss: 0.014885 \tValidation Loss: 0.018572\n","Epoch: 416 \tTraining Loss: 0.014782 \tValidation Loss: 0.018929\n","Epoch: 417 \tTraining Loss: 0.014576 \tValidation Loss: 0.017656\n","Epoch: 418 \tTraining Loss: 0.014290 \tValidation Loss: 0.018306\n","Epoch: 419 \tTraining Loss: 0.014474 \tValidation Loss: 0.017951\n","Epoch: 420 \tTraining Loss: 0.015204 \tValidation Loss: 0.018417\n","Epoch: 421 \tTraining Loss: 0.014417 \tValidation Loss: 0.018110\n","Epoch: 422 \tTraining Loss: 0.014559 \tValidation Loss: 0.018304\n","Epoch: 423 \tTraining Loss: 0.014435 \tValidation Loss: 0.018474\n","Epoch: 424 \tTraining Loss: 0.015011 \tValidation Loss: 0.018213\n","Epoch: 425 \tTraining Loss: 0.014117 \tValidation Loss: 0.018820\n","Epoch: 426 \tTraining Loss: 0.014389 \tValidation Loss: 0.018791\n","Epoch: 427 \tTraining Loss: 0.014779 \tValidation Loss: 0.018508\n","Epoch: 428 \tTraining Loss: 0.014702 \tValidation Loss: 0.018399\n","Epoch: 429 \tTraining Loss: 0.014920 \tValidation Loss: 0.018488\n","Epoch: 430 \tTraining Loss: 0.014746 \tValidation Loss: 0.018103\n","Epoch: 431 \tTraining Loss: 0.015377 \tValidation Loss: 0.018276\n","Epoch: 432 \tTraining Loss: 0.014528 \tValidation Loss: 0.018010\n","Epoch: 433 \tTraining Loss: 0.014442 \tValidation Loss: 0.018707\n","Epoch: 434 \tTraining Loss: 0.014269 \tValidation Loss: 0.018204\n","Epoch: 435 \tTraining Loss: 0.014397 \tValidation Loss: 0.018697\n","Epoch: 436 \tTraining Loss: 0.014350 \tValidation Loss: 0.018444\n","Epoch: 437 \tTraining Loss: 0.014345 \tValidation Loss: 0.018418\n","Epoch: 438 \tTraining Loss: 0.014194 \tValidation Loss: 0.018387\n","Epoch: 439 \tTraining Loss: 0.014029 \tValidation Loss: 0.018420\n","Epoch: 440 \tTraining Loss: 0.014128 \tValidation Loss: 0.018578\n","Epoch: 441 \tTraining Loss: 0.013971 \tValidation Loss: 0.018268\n","Epoch: 442 \tTraining Loss: 0.014293 \tValidation Loss: 0.018340\n","Epoch: 443 \tTraining Loss: 0.014322 \tValidation Loss: 0.018289\n","Epoch: 444 \tTraining Loss: 0.014468 \tValidation Loss: 0.018624\n","Epoch: 445 \tTraining Loss: 0.014934 \tValidation Loss: 0.017878\n","Epoch: 446 \tTraining Loss: 0.013905 \tValidation Loss: 0.018237\n","Epoch: 447 \tTraining Loss: 0.014671 \tValidation Loss: 0.018221\n","Epoch: 448 \tTraining Loss: 0.014311 \tValidation Loss: 0.018502\n","Epoch: 449 \tTraining Loss: 0.014214 \tValidation Loss: 0.018176\n","Epoch: 450 \tTraining Loss: 0.014418 \tValidation Loss: 0.018580\n","Epoch: 451 \tTraining Loss: 0.013949 \tValidation Loss: 0.018087\n","Epoch: 452 \tTraining Loss: 0.014470 \tValidation Loss: 0.018161\n","Epoch: 453 \tTraining Loss: 0.014234 \tValidation Loss: 0.018064\n","Epoch: 454 \tTraining Loss: 0.013633 \tValidation Loss: 0.018186\n","Epoch: 455 \tTraining Loss: 0.014125 \tValidation Loss: 0.018054\n","Epoch: 456 \tTraining Loss: 0.014203 \tValidation Loss: 0.018694\n","Epoch: 457 \tTraining Loss: 0.013962 \tValidation Loss: 0.018102\n","Epoch: 458 \tTraining Loss: 0.014507 \tValidation Loss: 0.018472\n","Epoch: 459 \tTraining Loss: 0.014410 \tValidation Loss: 0.018491\n","Epoch: 460 \tTraining Loss: 0.013759 \tValidation Loss: 0.018324\n","Epoch: 461 \tTraining Loss: 0.013983 \tValidation Loss: 0.018624\n","Epoch: 462 \tTraining Loss: 0.014173 \tValidation Loss: 0.018297\n","Epoch: 463 \tTraining Loss: 0.013962 \tValidation Loss: 0.018439\n","Epoch: 464 \tTraining Loss: 0.014290 \tValidation Loss: 0.018591\n","Epoch: 465 \tTraining Loss: 0.014278 \tValidation Loss: 0.018212\n","Epoch: 466 \tTraining Loss: 0.014102 \tValidation Loss: 0.017966\n","Epoch: 467 \tTraining Loss: 0.014248 \tValidation Loss: 0.018597\n","Epoch: 468 \tTraining Loss: 0.014269 \tValidation Loss: 0.018413\n","Epoch: 469 \tTraining Loss: 0.013914 \tValidation Loss: 0.018633\n","Epoch: 470 \tTraining Loss: 0.014268 \tValidation Loss: 0.018434\n","Epoch: 471 \tTraining Loss: 0.013796 \tValidation Loss: 0.018611\n","Epoch: 472 \tTraining Loss: 0.014344 \tValidation Loss: 0.019263\n","Epoch: 473 \tTraining Loss: 0.014345 \tValidation Loss: 0.018287\n","Epoch: 474 \tTraining Loss: 0.014046 \tValidation Loss: 0.018281\n","Epoch: 475 \tTraining Loss: 0.014334 \tValidation Loss: 0.018181\n","Epoch: 476 \tTraining Loss: 0.013781 \tValidation Loss: 0.018634\n","Epoch: 477 \tTraining Loss: 0.013935 \tValidation Loss: 0.018341\n","Epoch: 478 \tTraining Loss: 0.013727 \tValidation Loss: 0.018814\n","Epoch: 479 \tTraining Loss: 0.013416 \tValidation Loss: 0.018294\n","Epoch: 480 \tTraining Loss: 0.013938 \tValidation Loss: 0.018417\n","Epoch: 481 \tTraining Loss: 0.013584 \tValidation Loss: 0.018800\n","Epoch: 482 \tTraining Loss: 0.013849 \tValidation Loss: 0.018475\n","Epoch: 483 \tTraining Loss: 0.013729 \tValidation Loss: 0.018890\n","Epoch: 484 \tTraining Loss: 0.014282 \tValidation Loss: 0.018466\n","Epoch: 485 \tTraining Loss: 0.013862 \tValidation Loss: 0.018715\n","Epoch: 486 \tTraining Loss: 0.013957 \tValidation Loss: 0.018388\n","Epoch: 487 \tTraining Loss: 0.013921 \tValidation Loss: 0.018678\n","Epoch: 488 \tTraining Loss: 0.013697 \tValidation Loss: 0.018560\n","Epoch: 489 \tTraining Loss: 0.014073 \tValidation Loss: 0.018676\n","Epoch: 490 \tTraining Loss: 0.013580 \tValidation Loss: 0.018243\n","Epoch: 491 \tTraining Loss: 0.013810 \tValidation Loss: 0.018203\n","Epoch: 492 \tTraining Loss: 0.014013 \tValidation Loss: 0.018663\n","Epoch: 493 \tTraining Loss: 0.013934 \tValidation Loss: 0.018238\n","Epoch: 494 \tTraining Loss: 0.013879 \tValidation Loss: 0.018082\n","Epoch: 495 \tTraining Loss: 0.013728 \tValidation Loss: 0.018320\n","Epoch: 496 \tTraining Loss: 0.014514 \tValidation Loss: 0.018387\n","Epoch: 497 \tTraining Loss: 0.013503 \tValidation Loss: 0.018086\n","Epoch: 498 \tTraining Loss: 0.013712 \tValidation Loss: 0.018691\n","Epoch: 499 \tTraining Loss: 0.013571 \tValidation Loss: 0.018196\n","Epoch: 500 \tTraining Loss: 0.013528 \tValidation Loss: 0.017914\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l2T1hp4b60mT","colab_type":"text"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"SqP7XSAY6CEq","colab_type":"code","colab":{}},"source":["def predict(data_loader, model):\n","    '''\n","    Predict keypoints\n","    Args:\n","        data_loader (DataLoader): DataLoader for Dataset\n","        model (nn.Module): trained model for prediction.\n","    Return:\n","        predictions (array-like): keypoints in float (no. of images x keypoints).\n","    '''\n","    \n","    model.eval() # prep model for evaluation\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(data_loader):\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(batch['image'].to(device)).cpu().numpy()\n","            if i == 0:\n","                predictions = output\n","            else:\n","                predictions = np.vstack((predictions, output))\n","\n","    return predictions"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CoeQ74Zf7Ba1","colab_type":"text"},"source":["#Visualize the front view\n","\n","I have to delete the print('Error:') line because no prediction error "]},{"cell_type":"code","metadata":{"id":"Qv062zrt65fS","colab_type":"code","outputId":"ca9c6aaf-900c-4b6a-c3b7-9005f65cb978","executionInfo":{"status":"ok","timestamp":1584942269199,"user_tz":-480,"elapsed":706,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["model.load_state_dict(torch.load(ROOT_FOLDER+'model.pt'))\n","predictions = predict(test_loader, model)\n","\n","# print('Error: ', np.linalg.norm(predictions-KPT_S_TEST.reshape((predictions.shape[0],-1)), axis=1).mean())\n","\n","idx = 100\n","draw_points(IMG_S_TEST[idx,:,:], predictions[idx,:].reshape((-1,2)))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxuMsCRjjcf8P+1R69kfQDFAAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"-ll8UrPK7IYV","colab_type":"text"},"source":["# Try another network CNN\n","\n","I set number of filters as 65,130,260,520,1040,2080 separately"]},{"cell_type":"code","metadata":{"id":"S3EkmtZS7Mbe","colab_type":"code","colab":{}},"source":["class CNN(nn.Module):\n","    def __init__(self, output_size):\n","        super(CNN, self).__init__()\n","        # 200 x 200\n","        self.conv1 = nn.Conv2d(1, 65, 5)\n","        # (w-f)/s+1 = 196\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        # 98\n","        self.conv2 = nn.Conv2d(65,130,3)\n","        # (98-3)/1 + 1 = 96\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        # 48\n","        self.conv3 = nn.Conv2d(130,260,3,stride=1)\n","        # (48-3)/1 + 1 = 46\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","        # 23\n","        self.conv4 = nn.Conv2d(260,520,3,stride=2)\n","        # (23-3)/2 + 1 = 11\n","        self.conv5 = nn.Conv2d(520,1040,1,stride=2)\n","        # (11-1)/2+1 = 6\n","        #Linear Layer\n","        self.fc1 = nn.Linear(1040*6*6, 2080)\n","        self.fc2 = nn.Linear(2080, output_size)\n","        \n","        self.drop1 = nn.Dropout(p = 0.1)\n","        self.drop2 = nn.Dropout(p = 0.2)\n","        self.drop3 = nn.Dropout(p = 0.25)\n","        self.drop4 = nn.Dropout(p = 0.25)\n","        self.drop5 = nn.Dropout(p = 0.3)\n","        self.drop6 = nn.Dropout(p = 0.4)\n","\n","    def forward(self, x):  \n","        x = self.pool1(F.relu(self.conv1(x)))\n","        x = self.drop1(x)\n","        x = self.pool2(F.relu(self.conv2(x)))\n","        x = self.drop2(x)\n","        x = self.pool3(F.relu(self.conv3(x)))\n","        x = self.drop3(x)\n","        x = F.relu(self.conv4(x))\n","        x = self.drop4(x)\n","        x = F.relu(self.conv5(x))\n","        x = self.drop5(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = self.drop6(x)\n","        x = self.fc2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mU2d-Lg3YQwm","colab_type":"text"},"source":["lr=0.0001\n","\n","n_epoches=500"]},{"cell_type":"code","metadata":{"id":"EGmvKqnf7t-a","colab_type":"code","outputId":"7e2dfaed-8c93-4b06-f32d-155aaec846e1","executionInfo":{"status":"ok","timestamp":1584945689399,"user_tz":-480,"elapsed":3182215,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# if you find this step is slow, use GPU mode: Runtime-> change runtime type -> GPU\n","model = CNN(output_size=18)\n","model = model.to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","train_losses, valid_losses = train(train_loader, valid_loader,\n","                                   model, criterion, optimizer,\n","                                   n_epochs=500,\n","                                   saved_model=ROOT_FOLDER+'model.pt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.066250 \tValidation Loss: 0.029822\n","Validation loss decreased (inf --> 0.029822).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.036841 \tValidation Loss: 0.028822\n","Validation loss decreased (0.029822 --> 0.028822).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.034291 \tValidation Loss: 0.028521\n","Validation loss decreased (0.028822 --> 0.028521).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.032153 \tValidation Loss: 0.030999\n","Epoch: 5 \tTraining Loss: 0.030677 \tValidation Loss: 0.032073\n","Epoch: 6 \tTraining Loss: 0.029381 \tValidation Loss: 0.032618\n","Epoch: 7 \tTraining Loss: 0.028184 \tValidation Loss: 0.026424\n","Validation loss decreased (0.028521 --> 0.026424).  Saving model ...\n","Epoch: 8 \tTraining Loss: 0.027170 \tValidation Loss: 0.025081\n","Validation loss decreased (0.026424 --> 0.025081).  Saving model ...\n","Epoch: 9 \tTraining Loss: 0.026621 \tValidation Loss: 0.025836\n","Epoch: 10 \tTraining Loss: 0.026070 \tValidation Loss: 0.026378\n","Epoch: 11 \tTraining Loss: 0.025455 \tValidation Loss: 0.024784\n","Validation loss decreased (0.025081 --> 0.024784).  Saving model ...\n","Epoch: 12 \tTraining Loss: 0.025002 \tValidation Loss: 0.022982\n","Validation loss decreased (0.024784 --> 0.022982).  Saving model ...\n","Epoch: 13 \tTraining Loss: 0.024139 \tValidation Loss: 0.023917\n","Epoch: 14 \tTraining Loss: 0.023858 \tValidation Loss: 0.025464\n","Epoch: 15 \tTraining Loss: 0.024180 \tValidation Loss: 0.022466\n","Validation loss decreased (0.022982 --> 0.022466).  Saving model ...\n","Epoch: 16 \tTraining Loss: 0.023688 \tValidation Loss: 0.022126\n","Validation loss decreased (0.022466 --> 0.022126).  Saving model ...\n","Epoch: 17 \tTraining Loss: 0.023206 \tValidation Loss: 0.022584\n","Epoch: 18 \tTraining Loss: 0.022634 \tValidation Loss: 0.021522\n","Validation loss decreased (0.022126 --> 0.021522).  Saving model ...\n","Epoch: 19 \tTraining Loss: 0.022729 \tValidation Loss: 0.023789\n","Epoch: 20 \tTraining Loss: 0.023126 \tValidation Loss: 0.024957\n","Epoch: 21 \tTraining Loss: 0.022959 \tValidation Loss: 0.022048\n","Epoch: 22 \tTraining Loss: 0.021948 \tValidation Loss: 0.019632\n","Validation loss decreased (0.021522 --> 0.019632).  Saving model ...\n","Epoch: 23 \tTraining Loss: 0.021849 \tValidation Loss: 0.019240\n","Validation loss decreased (0.019632 --> 0.019240).  Saving model ...\n","Epoch: 24 \tTraining Loss: 0.022060 \tValidation Loss: 0.019107\n","Validation loss decreased (0.019240 --> 0.019107).  Saving model ...\n","Epoch: 25 \tTraining Loss: 0.022065 \tValidation Loss: 0.019345\n","Epoch: 26 \tTraining Loss: 0.021786 \tValidation Loss: 0.019317\n","Epoch: 27 \tTraining Loss: 0.021221 \tValidation Loss: 0.019403\n","Epoch: 28 \tTraining Loss: 0.020415 \tValidation Loss: 0.019317\n","Epoch: 29 \tTraining Loss: 0.021010 \tValidation Loss: 0.018967\n","Validation loss decreased (0.019107 --> 0.018967).  Saving model ...\n","Epoch: 30 \tTraining Loss: 0.020570 \tValidation Loss: 0.018980\n","Epoch: 31 \tTraining Loss: 0.020872 \tValidation Loss: 0.019337\n","Epoch: 32 \tTraining Loss: 0.022360 \tValidation Loss: 0.018886\n","Validation loss decreased (0.018967 --> 0.018886).  Saving model ...\n","Epoch: 33 \tTraining Loss: 0.020629 \tValidation Loss: 0.019709\n","Epoch: 34 \tTraining Loss: 0.020863 \tValidation Loss: 0.018333\n","Validation loss decreased (0.018886 --> 0.018333).  Saving model ...\n","Epoch: 35 \tTraining Loss: 0.020094 \tValidation Loss: 0.019223\n","Epoch: 36 \tTraining Loss: 0.020624 \tValidation Loss: 0.019782\n","Epoch: 37 \tTraining Loss: 0.019351 \tValidation Loss: 0.018045\n","Validation loss decreased (0.018333 --> 0.018045).  Saving model ...\n","Epoch: 38 \tTraining Loss: 0.019798 \tValidation Loss: 0.017523\n","Validation loss decreased (0.018045 --> 0.017523).  Saving model ...\n","Epoch: 39 \tTraining Loss: 0.019168 \tValidation Loss: 0.018620\n","Epoch: 40 \tTraining Loss: 0.019414 \tValidation Loss: 0.019146\n","Epoch: 41 \tTraining Loss: 0.019349 \tValidation Loss: 0.019291\n","Epoch: 42 \tTraining Loss: 0.018986 \tValidation Loss: 0.017596\n","Epoch: 43 \tTraining Loss: 0.018859 \tValidation Loss: 0.017689\n","Epoch: 44 \tTraining Loss: 0.018999 \tValidation Loss: 0.018682\n","Epoch: 45 \tTraining Loss: 0.019404 \tValidation Loss: 0.020171\n","Epoch: 46 \tTraining Loss: 0.019198 \tValidation Loss: 0.018505\n","Epoch: 47 \tTraining Loss: 0.018104 \tValidation Loss: 0.017908\n","Epoch: 48 \tTraining Loss: 0.018109 \tValidation Loss: 0.019973\n","Epoch: 49 \tTraining Loss: 0.018379 \tValidation Loss: 0.017357\n","Validation loss decreased (0.017523 --> 0.017357).  Saving model ...\n","Epoch: 50 \tTraining Loss: 0.018108 \tValidation Loss: 0.017313\n","Validation loss decreased (0.017357 --> 0.017313).  Saving model ...\n","Epoch: 51 \tTraining Loss: 0.018494 \tValidation Loss: 0.018149\n","Epoch: 52 \tTraining Loss: 0.018069 \tValidation Loss: 0.019213\n","Epoch: 53 \tTraining Loss: 0.018533 \tValidation Loss: 0.017642\n","Epoch: 54 \tTraining Loss: 0.018017 \tValidation Loss: 0.017126\n","Validation loss decreased (0.017313 --> 0.017126).  Saving model ...\n","Epoch: 55 \tTraining Loss: 0.018130 \tValidation Loss: 0.018567\n","Epoch: 56 \tTraining Loss: 0.019114 \tValidation Loss: 0.020504\n","Epoch: 57 \tTraining Loss: 0.018767 \tValidation Loss: 0.017572\n","Epoch: 58 \tTraining Loss: 0.017915 \tValidation Loss: 0.017191\n","Epoch: 59 \tTraining Loss: 0.017637 \tValidation Loss: 0.017599\n","Epoch: 60 \tTraining Loss: 0.017654 \tValidation Loss: 0.018227\n","Epoch: 61 \tTraining Loss: 0.017353 \tValidation Loss: 0.016999\n","Validation loss decreased (0.017126 --> 0.016999).  Saving model ...\n","Epoch: 62 \tTraining Loss: 0.017632 \tValidation Loss: 0.017215\n","Epoch: 63 \tTraining Loss: 0.017622 \tValidation Loss: 0.016221\n","Validation loss decreased (0.016999 --> 0.016221).  Saving model ...\n","Epoch: 64 \tTraining Loss: 0.017702 \tValidation Loss: 0.016847\n","Epoch: 65 \tTraining Loss: 0.018602 \tValidation Loss: 0.017635\n","Epoch: 66 \tTraining Loss: 0.017993 \tValidation Loss: 0.017716\n","Epoch: 67 \tTraining Loss: 0.016615 \tValidation Loss: 0.017930\n","Epoch: 68 \tTraining Loss: 0.016946 \tValidation Loss: 0.017668\n","Epoch: 69 \tTraining Loss: 0.016832 \tValidation Loss: 0.016605\n","Epoch: 70 \tTraining Loss: 0.016916 \tValidation Loss: 0.019416\n","Epoch: 71 \tTraining Loss: 0.016905 \tValidation Loss: 0.019208\n","Epoch: 72 \tTraining Loss: 0.017216 \tValidation Loss: 0.016977\n","Epoch: 73 \tTraining Loss: 0.016780 \tValidation Loss: 0.018200\n","Epoch: 74 \tTraining Loss: 0.016697 \tValidation Loss: 0.017653\n","Epoch: 75 \tTraining Loss: 0.017050 \tValidation Loss: 0.016733\n","Epoch: 76 \tTraining Loss: 0.016447 \tValidation Loss: 0.017140\n","Epoch: 77 \tTraining Loss: 0.016287 \tValidation Loss: 0.016654\n","Epoch: 78 \tTraining Loss: 0.015885 \tValidation Loss: 0.016654\n","Epoch: 79 \tTraining Loss: 0.016588 \tValidation Loss: 0.016731\n","Epoch: 80 \tTraining Loss: 0.016534 \tValidation Loss: 0.019299\n","Epoch: 81 \tTraining Loss: 0.016637 \tValidation Loss: 0.016547\n","Epoch: 82 \tTraining Loss: 0.016466 \tValidation Loss: 0.016261\n","Epoch: 83 \tTraining Loss: 0.016435 \tValidation Loss: 0.016906\n","Epoch: 84 \tTraining Loss: 0.016070 \tValidation Loss: 0.017458\n","Epoch: 85 \tTraining Loss: 0.016033 \tValidation Loss: 0.016810\n","Epoch: 86 \tTraining Loss: 0.016620 \tValidation Loss: 0.017013\n","Epoch: 87 \tTraining Loss: 0.016106 \tValidation Loss: 0.016508\n","Epoch: 88 \tTraining Loss: 0.015962 \tValidation Loss: 0.016875\n","Epoch: 89 \tTraining Loss: 0.015788 \tValidation Loss: 0.018110\n","Epoch: 90 \tTraining Loss: 0.015840 \tValidation Loss: 0.017217\n","Epoch: 91 \tTraining Loss: 0.016095 \tValidation Loss: 0.016261\n","Epoch: 92 \tTraining Loss: 0.015981 \tValidation Loss: 0.018088\n","Epoch: 93 \tTraining Loss: 0.016347 \tValidation Loss: 0.017413\n","Epoch: 94 \tTraining Loss: 0.015903 \tValidation Loss: 0.017697\n","Epoch: 95 \tTraining Loss: 0.015731 \tValidation Loss: 0.017649\n","Epoch: 96 \tTraining Loss: 0.015774 \tValidation Loss: 0.016673\n","Epoch: 97 \tTraining Loss: 0.016235 \tValidation Loss: 0.017498\n","Epoch: 98 \tTraining Loss: 0.015616 \tValidation Loss: 0.016606\n","Epoch: 99 \tTraining Loss: 0.015468 \tValidation Loss: 0.017005\n","Epoch: 100 \tTraining Loss: 0.015212 \tValidation Loss: 0.016689\n","Epoch: 101 \tTraining Loss: 0.015089 \tValidation Loss: 0.016715\n","Epoch: 102 \tTraining Loss: 0.015121 \tValidation Loss: 0.017433\n","Epoch: 103 \tTraining Loss: 0.015371 \tValidation Loss: 0.016775\n","Epoch: 104 \tTraining Loss: 0.015254 \tValidation Loss: 0.017487\n","Epoch: 105 \tTraining Loss: 0.015615 \tValidation Loss: 0.017291\n","Epoch: 106 \tTraining Loss: 0.015398 \tValidation Loss: 0.016434\n","Epoch: 107 \tTraining Loss: 0.015437 \tValidation Loss: 0.016952\n","Epoch: 108 \tTraining Loss: 0.015386 \tValidation Loss: 0.016155\n","Validation loss decreased (0.016221 --> 0.016155).  Saving model ...\n","Epoch: 109 \tTraining Loss: 0.015656 \tValidation Loss: 0.016412\n","Epoch: 110 \tTraining Loss: 0.014839 \tValidation Loss: 0.016647\n","Epoch: 111 \tTraining Loss: 0.014609 \tValidation Loss: 0.017409\n","Epoch: 112 \tTraining Loss: 0.014646 \tValidation Loss: 0.017569\n","Epoch: 113 \tTraining Loss: 0.015080 \tValidation Loss: 0.017824\n","Epoch: 114 \tTraining Loss: 0.014739 \tValidation Loss: 0.017353\n","Epoch: 115 \tTraining Loss: 0.015096 \tValidation Loss: 0.016334\n","Epoch: 116 \tTraining Loss: 0.014949 \tValidation Loss: 0.016874\n","Epoch: 117 \tTraining Loss: 0.014849 \tValidation Loss: 0.017626\n","Epoch: 118 \tTraining Loss: 0.015046 \tValidation Loss: 0.016975\n","Epoch: 119 \tTraining Loss: 0.014657 \tValidation Loss: 0.017347\n","Epoch: 120 \tTraining Loss: 0.015206 \tValidation Loss: 0.016895\n","Epoch: 121 \tTraining Loss: 0.014620 \tValidation Loss: 0.016483\n","Epoch: 122 \tTraining Loss: 0.014659 \tValidation Loss: 0.016753\n","Epoch: 123 \tTraining Loss: 0.014997 \tValidation Loss: 0.016302\n","Epoch: 124 \tTraining Loss: 0.015401 \tValidation Loss: 0.019704\n","Epoch: 125 \tTraining Loss: 0.014545 \tValidation Loss: 0.016278\n","Epoch: 126 \tTraining Loss: 0.014713 \tValidation Loss: 0.017929\n","Epoch: 127 \tTraining Loss: 0.015023 \tValidation Loss: 0.016129\n","Validation loss decreased (0.016155 --> 0.016129).  Saving model ...\n","Epoch: 128 \tTraining Loss: 0.014178 \tValidation Loss: 0.017450\n","Epoch: 129 \tTraining Loss: 0.014155 \tValidation Loss: 0.016356\n","Epoch: 130 \tTraining Loss: 0.014790 \tValidation Loss: 0.017256\n","Epoch: 131 \tTraining Loss: 0.014504 \tValidation Loss: 0.016748\n","Epoch: 132 \tTraining Loss: 0.014741 \tValidation Loss: 0.017370\n","Epoch: 133 \tTraining Loss: 0.014132 \tValidation Loss: 0.017397\n","Epoch: 134 \tTraining Loss: 0.014216 \tValidation Loss: 0.017268\n","Epoch: 135 \tTraining Loss: 0.014123 \tValidation Loss: 0.017971\n","Epoch: 136 \tTraining Loss: 0.013897 \tValidation Loss: 0.017449\n","Epoch: 137 \tTraining Loss: 0.014258 \tValidation Loss: 0.018522\n","Epoch: 138 \tTraining Loss: 0.013965 \tValidation Loss: 0.016619\n","Epoch: 139 \tTraining Loss: 0.014377 \tValidation Loss: 0.018206\n","Epoch: 140 \tTraining Loss: 0.014998 \tValidation Loss: 0.016856\n","Epoch: 141 \tTraining Loss: 0.014196 \tValidation Loss: 0.017186\n","Epoch: 142 \tTraining Loss: 0.013812 \tValidation Loss: 0.016458\n","Epoch: 143 \tTraining Loss: 0.013933 \tValidation Loss: 0.017082\n","Epoch: 144 \tTraining Loss: 0.013729 \tValidation Loss: 0.018338\n","Epoch: 145 \tTraining Loss: 0.013804 \tValidation Loss: 0.016884\n","Epoch: 146 \tTraining Loss: 0.013806 \tValidation Loss: 0.017709\n","Epoch: 147 \tTraining Loss: 0.013540 \tValidation Loss: 0.017349\n","Epoch: 148 \tTraining Loss: 0.014057 \tValidation Loss: 0.017290\n","Epoch: 149 \tTraining Loss: 0.013728 \tValidation Loss: 0.018492\n","Epoch: 150 \tTraining Loss: 0.013801 \tValidation Loss: 0.017213\n","Epoch: 151 \tTraining Loss: 0.013611 \tValidation Loss: 0.016855\n","Epoch: 152 \tTraining Loss: 0.013574 \tValidation Loss: 0.017591\n","Epoch: 153 \tTraining Loss: 0.013308 \tValidation Loss: 0.017939\n","Epoch: 154 \tTraining Loss: 0.014141 \tValidation Loss: 0.017031\n","Epoch: 155 \tTraining Loss: 0.013839 \tValidation Loss: 0.016409\n","Epoch: 156 \tTraining Loss: 0.013832 \tValidation Loss: 0.017564\n","Epoch: 157 \tTraining Loss: 0.013707 \tValidation Loss: 0.016763\n","Epoch: 158 \tTraining Loss: 0.013693 \tValidation Loss: 0.016628\n","Epoch: 159 \tTraining Loss: 0.013563 \tValidation Loss: 0.018074\n","Epoch: 160 \tTraining Loss: 0.013409 \tValidation Loss: 0.016675\n","Epoch: 161 \tTraining Loss: 0.013407 \tValidation Loss: 0.017639\n","Epoch: 162 \tTraining Loss: 0.013403 \tValidation Loss: 0.016448\n","Epoch: 163 \tTraining Loss: 0.013407 \tValidation Loss: 0.015964\n","Validation loss decreased (0.016129 --> 0.015964).  Saving model ...\n","Epoch: 164 \tTraining Loss: 0.013636 \tValidation Loss: 0.017645\n","Epoch: 165 \tTraining Loss: 0.013591 \tValidation Loss: 0.017140\n","Epoch: 166 \tTraining Loss: 0.013844 \tValidation Loss: 0.017109\n","Epoch: 167 \tTraining Loss: 0.013410 \tValidation Loss: 0.018016\n","Epoch: 168 \tTraining Loss: 0.012985 \tValidation Loss: 0.017078\n","Epoch: 169 \tTraining Loss: 0.013413 \tValidation Loss: 0.016954\n","Epoch: 170 \tTraining Loss: 0.013865 \tValidation Loss: 0.018316\n","Epoch: 171 \tTraining Loss: 0.013115 \tValidation Loss: 0.016175\n","Epoch: 172 \tTraining Loss: 0.013359 \tValidation Loss: 0.018575\n","Epoch: 173 \tTraining Loss: 0.013075 \tValidation Loss: 0.017115\n","Epoch: 174 \tTraining Loss: 0.013069 \tValidation Loss: 0.017263\n","Epoch: 175 \tTraining Loss: 0.012889 \tValidation Loss: 0.019624\n","Epoch: 176 \tTraining Loss: 0.013353 \tValidation Loss: 0.016474\n","Epoch: 177 \tTraining Loss: 0.013505 \tValidation Loss: 0.019205\n","Epoch: 178 \tTraining Loss: 0.013013 \tValidation Loss: 0.018910\n","Epoch: 179 \tTraining Loss: 0.013022 \tValidation Loss: 0.017055\n","Epoch: 180 \tTraining Loss: 0.012914 \tValidation Loss: 0.016913\n","Epoch: 181 \tTraining Loss: 0.013203 \tValidation Loss: 0.016237\n","Epoch: 182 \tTraining Loss: 0.013061 \tValidation Loss: 0.016896\n","Epoch: 183 \tTraining Loss: 0.013038 \tValidation Loss: 0.017368\n","Epoch: 184 \tTraining Loss: 0.012803 \tValidation Loss: 0.017233\n","Epoch: 185 \tTraining Loss: 0.012560 \tValidation Loss: 0.016962\n","Epoch: 186 \tTraining Loss: 0.012882 \tValidation Loss: 0.016520\n","Epoch: 187 \tTraining Loss: 0.012603 \tValidation Loss: 0.017056\n","Epoch: 188 \tTraining Loss: 0.012602 \tValidation Loss: 0.017598\n","Epoch: 189 \tTraining Loss: 0.012618 \tValidation Loss: 0.016351\n","Epoch: 190 \tTraining Loss: 0.013032 \tValidation Loss: 0.019771\n","Epoch: 191 \tTraining Loss: 0.012819 \tValidation Loss: 0.019707\n","Epoch: 192 \tTraining Loss: 0.012731 \tValidation Loss: 0.018187\n","Epoch: 193 \tTraining Loss: 0.012309 \tValidation Loss: 0.017558\n","Epoch: 194 \tTraining Loss: 0.012383 \tValidation Loss: 0.017837\n","Epoch: 195 \tTraining Loss: 0.012664 \tValidation Loss: 0.017755\n","Epoch: 196 \tTraining Loss: 0.012590 \tValidation Loss: 0.017096\n","Epoch: 197 \tTraining Loss: 0.012579 \tValidation Loss: 0.017001\n","Epoch: 198 \tTraining Loss: 0.012522 \tValidation Loss: 0.019031\n","Epoch: 199 \tTraining Loss: 0.012439 \tValidation Loss: 0.018863\n","Epoch: 200 \tTraining Loss: 0.012825 \tValidation Loss: 0.017978\n","Epoch: 201 \tTraining Loss: 0.012417 \tValidation Loss: 0.016445\n","Epoch: 202 \tTraining Loss: 0.012571 \tValidation Loss: 0.016528\n","Epoch: 203 \tTraining Loss: 0.012575 \tValidation Loss: 0.017314\n","Epoch: 204 \tTraining Loss: 0.012098 \tValidation Loss: 0.019220\n","Epoch: 205 \tTraining Loss: 0.012824 \tValidation Loss: 0.017031\n","Epoch: 206 \tTraining Loss: 0.012744 \tValidation Loss: 0.016936\n","Epoch: 207 \tTraining Loss: 0.012633 \tValidation Loss: 0.018276\n","Epoch: 208 \tTraining Loss: 0.012450 \tValidation Loss: 0.018650\n","Epoch: 209 \tTraining Loss: 0.012552 \tValidation Loss: 0.017393\n","Epoch: 210 \tTraining Loss: 0.012525 \tValidation Loss: 0.017252\n","Epoch: 211 \tTraining Loss: 0.012440 \tValidation Loss: 0.017868\n","Epoch: 212 \tTraining Loss: 0.012217 \tValidation Loss: 0.017449\n","Epoch: 213 \tTraining Loss: 0.012127 \tValidation Loss: 0.017203\n","Epoch: 214 \tTraining Loss: 0.012302 \tValidation Loss: 0.016803\n","Epoch: 215 \tTraining Loss: 0.011993 \tValidation Loss: 0.017372\n","Epoch: 216 \tTraining Loss: 0.011821 \tValidation Loss: 0.017673\n","Epoch: 217 \tTraining Loss: 0.011924 \tValidation Loss: 0.017696\n","Epoch: 218 \tTraining Loss: 0.012332 \tValidation Loss: 0.018206\n","Epoch: 219 \tTraining Loss: 0.012174 \tValidation Loss: 0.019164\n","Epoch: 220 \tTraining Loss: 0.012195 \tValidation Loss: 0.018304\n","Epoch: 221 \tTraining Loss: 0.012214 \tValidation Loss: 0.017181\n","Epoch: 222 \tTraining Loss: 0.012132 \tValidation Loss: 0.017353\n","Epoch: 223 \tTraining Loss: 0.011762 \tValidation Loss: 0.017753\n","Epoch: 224 \tTraining Loss: 0.012434 \tValidation Loss: 0.016703\n","Epoch: 225 \tTraining Loss: 0.011845 \tValidation Loss: 0.017575\n","Epoch: 226 \tTraining Loss: 0.011977 \tValidation Loss: 0.017235\n","Epoch: 227 \tTraining Loss: 0.011862 \tValidation Loss: 0.017079\n","Epoch: 228 \tTraining Loss: 0.011879 \tValidation Loss: 0.016617\n","Epoch: 229 \tTraining Loss: 0.011776 \tValidation Loss: 0.017092\n","Epoch: 230 \tTraining Loss: 0.012206 \tValidation Loss: 0.020010\n","Epoch: 231 \tTraining Loss: 0.012958 \tValidation Loss: 0.017155\n","Epoch: 232 \tTraining Loss: 0.011821 \tValidation Loss: 0.017471\n","Epoch: 233 \tTraining Loss: 0.011969 \tValidation Loss: 0.017414\n","Epoch: 234 \tTraining Loss: 0.012230 \tValidation Loss: 0.020114\n","Epoch: 235 \tTraining Loss: 0.012302 \tValidation Loss: 0.017278\n","Epoch: 236 \tTraining Loss: 0.012300 \tValidation Loss: 0.019181\n","Epoch: 237 \tTraining Loss: 0.011738 \tValidation Loss: 0.017987\n","Epoch: 238 \tTraining Loss: 0.011412 \tValidation Loss: 0.017908\n","Epoch: 239 \tTraining Loss: 0.011806 \tValidation Loss: 0.017211\n","Epoch: 240 \tTraining Loss: 0.011592 \tValidation Loss: 0.017640\n","Epoch: 241 \tTraining Loss: 0.011873 \tValidation Loss: 0.016429\n","Epoch: 242 \tTraining Loss: 0.011772 \tValidation Loss: 0.017097\n","Epoch: 243 \tTraining Loss: 0.011855 \tValidation Loss: 0.017107\n","Epoch: 244 \tTraining Loss: 0.012014 \tValidation Loss: 0.017755\n","Epoch: 245 \tTraining Loss: 0.012484 \tValidation Loss: 0.017177\n","Epoch: 246 \tTraining Loss: 0.011709 \tValidation Loss: 0.017761\n","Epoch: 247 \tTraining Loss: 0.011602 \tValidation Loss: 0.016814\n","Epoch: 248 \tTraining Loss: 0.011472 \tValidation Loss: 0.016647\n","Epoch: 249 \tTraining Loss: 0.012078 \tValidation Loss: 0.016467\n","Epoch: 250 \tTraining Loss: 0.011516 \tValidation Loss: 0.017481\n","Epoch: 251 \tTraining Loss: 0.011588 \tValidation Loss: 0.016588\n","Epoch: 252 \tTraining Loss: 0.011427 \tValidation Loss: 0.017619\n","Epoch: 253 \tTraining Loss: 0.011297 \tValidation Loss: 0.017314\n","Epoch: 254 \tTraining Loss: 0.011163 \tValidation Loss: 0.016782\n","Epoch: 255 \tTraining Loss: 0.011369 \tValidation Loss: 0.017210\n","Epoch: 256 \tTraining Loss: 0.011484 \tValidation Loss: 0.016648\n","Epoch: 257 \tTraining Loss: 0.011700 \tValidation Loss: 0.016292\n","Epoch: 258 \tTraining Loss: 0.011729 \tValidation Loss: 0.017017\n","Epoch: 259 \tTraining Loss: 0.011418 \tValidation Loss: 0.016807\n","Epoch: 260 \tTraining Loss: 0.011386 \tValidation Loss: 0.016314\n","Epoch: 261 \tTraining Loss: 0.011383 \tValidation Loss: 0.017081\n","Epoch: 262 \tTraining Loss: 0.011619 \tValidation Loss: 0.016984\n","Epoch: 263 \tTraining Loss: 0.011061 \tValidation Loss: 0.016798\n","Epoch: 264 \tTraining Loss: 0.010980 \tValidation Loss: 0.017224\n","Epoch: 265 \tTraining Loss: 0.011550 \tValidation Loss: 0.018782\n","Epoch: 266 \tTraining Loss: 0.011404 \tValidation Loss: 0.016489\n","Epoch: 267 \tTraining Loss: 0.011517 \tValidation Loss: 0.016762\n","Epoch: 268 \tTraining Loss: 0.011289 \tValidation Loss: 0.016703\n","Epoch: 269 \tTraining Loss: 0.011271 \tValidation Loss: 0.017761\n","Epoch: 270 \tTraining Loss: 0.010889 \tValidation Loss: 0.017429\n","Epoch: 271 \tTraining Loss: 0.011487 \tValidation Loss: 0.018606\n","Epoch: 272 \tTraining Loss: 0.011379 \tValidation Loss: 0.018223\n","Epoch: 273 \tTraining Loss: 0.011670 \tValidation Loss: 0.017319\n","Epoch: 274 \tTraining Loss: 0.011524 \tValidation Loss: 0.018119\n","Epoch: 275 \tTraining Loss: 0.012633 \tValidation Loss: 0.017320\n","Epoch: 276 \tTraining Loss: 0.011930 \tValidation Loss: 0.018289\n","Epoch: 277 \tTraining Loss: 0.011378 \tValidation Loss: 0.018360\n","Epoch: 278 \tTraining Loss: 0.011036 \tValidation Loss: 0.017197\n","Epoch: 279 \tTraining Loss: 0.011451 \tValidation Loss: 0.016934\n","Epoch: 280 \tTraining Loss: 0.011185 \tValidation Loss: 0.016968\n","Epoch: 281 \tTraining Loss: 0.011136 \tValidation Loss: 0.018342\n","Epoch: 282 \tTraining Loss: 0.010836 \tValidation Loss: 0.017214\n","Epoch: 283 \tTraining Loss: 0.011027 \tValidation Loss: 0.016760\n","Epoch: 284 \tTraining Loss: 0.010723 \tValidation Loss: 0.017206\n","Epoch: 285 \tTraining Loss: 0.011058 \tValidation Loss: 0.016574\n","Epoch: 286 \tTraining Loss: 0.011168 \tValidation Loss: 0.016866\n","Epoch: 287 \tTraining Loss: 0.010873 \tValidation Loss: 0.017140\n","Epoch: 288 \tTraining Loss: 0.010788 \tValidation Loss: 0.017853\n","Epoch: 289 \tTraining Loss: 0.010787 \tValidation Loss: 0.017574\n","Epoch: 290 \tTraining Loss: 0.010801 \tValidation Loss: 0.016638\n","Epoch: 291 \tTraining Loss: 0.010878 \tValidation Loss: 0.018078\n","Epoch: 292 \tTraining Loss: 0.010974 \tValidation Loss: 0.016916\n","Epoch: 293 \tTraining Loss: 0.011071 \tValidation Loss: 0.017166\n","Epoch: 294 \tTraining Loss: 0.010518 \tValidation Loss: 0.016386\n","Epoch: 295 \tTraining Loss: 0.010784 \tValidation Loss: 0.017981\n","Epoch: 296 \tTraining Loss: 0.010822 \tValidation Loss: 0.017217\n","Epoch: 297 \tTraining Loss: 0.011165 \tValidation Loss: 0.016732\n","Epoch: 298 \tTraining Loss: 0.010958 \tValidation Loss: 0.017703\n","Epoch: 299 \tTraining Loss: 0.010893 \tValidation Loss: 0.017285\n","Epoch: 300 \tTraining Loss: 0.011026 \tValidation Loss: 0.016579\n","Epoch: 301 \tTraining Loss: 0.011185 \tValidation Loss: 0.016345\n","Epoch: 302 \tTraining Loss: 0.010794 \tValidation Loss: 0.016384\n","Epoch: 303 \tTraining Loss: 0.010913 \tValidation Loss: 0.016566\n","Epoch: 304 \tTraining Loss: 0.010694 \tValidation Loss: 0.016751\n","Epoch: 305 \tTraining Loss: 0.010871 \tValidation Loss: 0.016627\n","Epoch: 306 \tTraining Loss: 0.011006 \tValidation Loss: 0.017106\n","Epoch: 307 \tTraining Loss: 0.010675 \tValidation Loss: 0.016533\n","Epoch: 308 \tTraining Loss: 0.010834 \tValidation Loss: 0.016183\n","Epoch: 309 \tTraining Loss: 0.010418 \tValidation Loss: 0.017244\n","Epoch: 310 \tTraining Loss: 0.010798 \tValidation Loss: 0.016871\n","Epoch: 311 \tTraining Loss: 0.010653 \tValidation Loss: 0.016697\n","Epoch: 312 \tTraining Loss: 0.010698 \tValidation Loss: 0.016941\n","Epoch: 313 \tTraining Loss: 0.010963 \tValidation Loss: 0.017337\n","Epoch: 314 \tTraining Loss: 0.011404 \tValidation Loss: 0.017733\n","Epoch: 315 \tTraining Loss: 0.010671 \tValidation Loss: 0.016492\n","Epoch: 316 \tTraining Loss: 0.010601 \tValidation Loss: 0.016567\n","Epoch: 317 \tTraining Loss: 0.010910 \tValidation Loss: 0.016421\n","Epoch: 318 \tTraining Loss: 0.010620 \tValidation Loss: 0.018443\n","Epoch: 319 \tTraining Loss: 0.010632 \tValidation Loss: 0.017999\n","Epoch: 320 \tTraining Loss: 0.010809 \tValidation Loss: 0.017487\n","Epoch: 321 \tTraining Loss: 0.010823 \tValidation Loss: 0.016615\n","Epoch: 322 \tTraining Loss: 0.010457 \tValidation Loss: 0.017305\n","Epoch: 323 \tTraining Loss: 0.010495 \tValidation Loss: 0.017738\n","Epoch: 324 \tTraining Loss: 0.010494 \tValidation Loss: 0.016984\n","Epoch: 325 \tTraining Loss: 0.010274 \tValidation Loss: 0.016683\n","Epoch: 326 \tTraining Loss: 0.010506 \tValidation Loss: 0.016926\n","Epoch: 327 \tTraining Loss: 0.010451 \tValidation Loss: 0.016750\n","Epoch: 328 \tTraining Loss: 0.010797 \tValidation Loss: 0.017733\n","Epoch: 329 \tTraining Loss: 0.010493 \tValidation Loss: 0.017593\n","Epoch: 330 \tTraining Loss: 0.010484 \tValidation Loss: 0.017058\n","Epoch: 331 \tTraining Loss: 0.010300 \tValidation Loss: 0.016658\n","Epoch: 332 \tTraining Loss: 0.010533 \tValidation Loss: 0.016423\n","Epoch: 333 \tTraining Loss: 0.010577 \tValidation Loss: 0.016857\n","Epoch: 334 \tTraining Loss: 0.010136 \tValidation Loss: 0.016826\n","Epoch: 335 \tTraining Loss: 0.010427 \tValidation Loss: 0.017141\n","Epoch: 336 \tTraining Loss: 0.010185 \tValidation Loss: 0.016407\n","Epoch: 337 \tTraining Loss: 0.010529 \tValidation Loss: 0.017245\n","Epoch: 338 \tTraining Loss: 0.010352 \tValidation Loss: 0.016759\n","Epoch: 339 \tTraining Loss: 0.010040 \tValidation Loss: 0.016398\n","Epoch: 340 \tTraining Loss: 0.010446 \tValidation Loss: 0.017759\n","Epoch: 341 \tTraining Loss: 0.010238 \tValidation Loss: 0.016902\n","Epoch: 342 \tTraining Loss: 0.010685 \tValidation Loss: 0.017720\n","Epoch: 343 \tTraining Loss: 0.010628 \tValidation Loss: 0.017962\n","Epoch: 344 \tTraining Loss: 0.010671 \tValidation Loss: 0.017083\n","Epoch: 345 \tTraining Loss: 0.010401 \tValidation Loss: 0.016899\n","Epoch: 346 \tTraining Loss: 0.010195 \tValidation Loss: 0.016617\n","Epoch: 347 \tTraining Loss: 0.010499 \tValidation Loss: 0.016147\n","Epoch: 348 \tTraining Loss: 0.010271 \tValidation Loss: 0.016436\n","Epoch: 349 \tTraining Loss: 0.010362 \tValidation Loss: 0.016494\n","Epoch: 350 \tTraining Loss: 0.010373 \tValidation Loss: 0.017084\n","Epoch: 351 \tTraining Loss: 0.010161 \tValidation Loss: 0.017458\n","Epoch: 352 \tTraining Loss: 0.010529 \tValidation Loss: 0.016686\n","Epoch: 353 \tTraining Loss: 0.010216 \tValidation Loss: 0.017488\n","Epoch: 354 \tTraining Loss: 0.010193 \tValidation Loss: 0.017308\n","Epoch: 355 \tTraining Loss: 0.010249 \tValidation Loss: 0.018082\n","Epoch: 356 \tTraining Loss: 0.010451 \tValidation Loss: 0.016688\n","Epoch: 357 \tTraining Loss: 0.010362 \tValidation Loss: 0.016730\n","Epoch: 358 \tTraining Loss: 0.010255 \tValidation Loss: 0.016898\n","Epoch: 359 \tTraining Loss: 0.010134 \tValidation Loss: 0.017008\n","Epoch: 360 \tTraining Loss: 0.010003 \tValidation Loss: 0.017098\n","Epoch: 361 \tTraining Loss: 0.010030 \tValidation Loss: 0.017167\n","Epoch: 362 \tTraining Loss: 0.010268 \tValidation Loss: 0.017702\n","Epoch: 363 \tTraining Loss: 0.010116 \tValidation Loss: 0.016783\n","Epoch: 364 \tTraining Loss: 0.009936 \tValidation Loss: 0.018285\n","Epoch: 365 \tTraining Loss: 0.010540 \tValidation Loss: 0.016553\n","Epoch: 366 \tTraining Loss: 0.009854 \tValidation Loss: 0.016805\n","Epoch: 367 \tTraining Loss: 0.009770 \tValidation Loss: 0.016732\n","Epoch: 368 \tTraining Loss: 0.009907 \tValidation Loss: 0.017164\n","Epoch: 369 \tTraining Loss: 0.010062 \tValidation Loss: 0.017597\n","Epoch: 370 \tTraining Loss: 0.010029 \tValidation Loss: 0.016979\n","Epoch: 371 \tTraining Loss: 0.009949 \tValidation Loss: 0.016716\n","Epoch: 372 \tTraining Loss: 0.009983 \tValidation Loss: 0.017497\n","Epoch: 373 \tTraining Loss: 0.010350 \tValidation Loss: 0.017074\n","Epoch: 374 \tTraining Loss: 0.009925 \tValidation Loss: 0.017157\n","Epoch: 375 \tTraining Loss: 0.009939 \tValidation Loss: 0.018035\n","Epoch: 376 \tTraining Loss: 0.010102 \tValidation Loss: 0.017111\n","Epoch: 377 \tTraining Loss: 0.009901 \tValidation Loss: 0.016887\n","Epoch: 378 \tTraining Loss: 0.009846 \tValidation Loss: 0.016508\n","Epoch: 379 \tTraining Loss: 0.009887 \tValidation Loss: 0.017000\n","Epoch: 380 \tTraining Loss: 0.009825 \tValidation Loss: 0.017027\n","Epoch: 381 \tTraining Loss: 0.010090 \tValidation Loss: 0.017444\n","Epoch: 382 \tTraining Loss: 0.009820 \tValidation Loss: 0.016477\n","Epoch: 383 \tTraining Loss: 0.010003 \tValidation Loss: 0.017335\n","Epoch: 384 \tTraining Loss: 0.010135 \tValidation Loss: 0.017450\n","Epoch: 385 \tTraining Loss: 0.009915 \tValidation Loss: 0.016541\n","Epoch: 386 \tTraining Loss: 0.009824 \tValidation Loss: 0.017528\n","Epoch: 387 \tTraining Loss: 0.009791 \tValidation Loss: 0.017566\n","Epoch: 388 \tTraining Loss: 0.009698 \tValidation Loss: 0.016930\n","Epoch: 389 \tTraining Loss: 0.009797 \tValidation Loss: 0.016878\n","Epoch: 390 \tTraining Loss: 0.009794 \tValidation Loss: 0.017567\n","Epoch: 391 \tTraining Loss: 0.009652 \tValidation Loss: 0.016940\n","Epoch: 392 \tTraining Loss: 0.009943 \tValidation Loss: 0.016762\n","Epoch: 393 \tTraining Loss: 0.009957 \tValidation Loss: 0.016177\n","Epoch: 394 \tTraining Loss: 0.010005 \tValidation Loss: 0.016800\n","Epoch: 395 \tTraining Loss: 0.009475 \tValidation Loss: 0.016792\n","Epoch: 396 \tTraining Loss: 0.009654 \tValidation Loss: 0.016572\n","Epoch: 397 \tTraining Loss: 0.009640 \tValidation Loss: 0.016591\n","Epoch: 398 \tTraining Loss: 0.009528 \tValidation Loss: 0.017252\n","Epoch: 399 \tTraining Loss: 0.009728 \tValidation Loss: 0.016707\n","Epoch: 400 \tTraining Loss: 0.009432 \tValidation Loss: 0.017364\n","Epoch: 401 \tTraining Loss: 0.009470 \tValidation Loss: 0.016851\n","Epoch: 402 \tTraining Loss: 0.009588 \tValidation Loss: 0.016518\n","Epoch: 403 \tTraining Loss: 0.009995 \tValidation Loss: 0.017006\n","Epoch: 404 \tTraining Loss: 0.009779 \tValidation Loss: 0.016826\n","Epoch: 405 \tTraining Loss: 0.009944 \tValidation Loss: 0.016452\n","Epoch: 406 \tTraining Loss: 0.009842 \tValidation Loss: 0.018308\n","Epoch: 407 \tTraining Loss: 0.009902 \tValidation Loss: 0.017197\n","Epoch: 408 \tTraining Loss: 0.009812 \tValidation Loss: 0.017275\n","Epoch: 409 \tTraining Loss: 0.009675 \tValidation Loss: 0.016357\n","Epoch: 410 \tTraining Loss: 0.009424 \tValidation Loss: 0.016435\n","Epoch: 411 \tTraining Loss: 0.009789 \tValidation Loss: 0.017817\n","Epoch: 412 \tTraining Loss: 0.009470 \tValidation Loss: 0.016822\n","Epoch: 413 \tTraining Loss: 0.009785 \tValidation Loss: 0.016118\n","Epoch: 414 \tTraining Loss: 0.009918 \tValidation Loss: 0.018082\n","Epoch: 415 \tTraining Loss: 0.009443 \tValidation Loss: 0.016421\n","Epoch: 416 \tTraining Loss: 0.009572 \tValidation Loss: 0.017120\n","Epoch: 417 \tTraining Loss: 0.009403 \tValidation Loss: 0.017242\n","Epoch: 418 \tTraining Loss: 0.009771 \tValidation Loss: 0.016588\n","Epoch: 419 \tTraining Loss: 0.009641 \tValidation Loss: 0.016997\n","Epoch: 420 \tTraining Loss: 0.009557 \tValidation Loss: 0.016991\n","Epoch: 421 \tTraining Loss: 0.009453 \tValidation Loss: 0.016947\n","Epoch: 422 \tTraining Loss: 0.009381 \tValidation Loss: 0.018646\n","Epoch: 423 \tTraining Loss: 0.009815 \tValidation Loss: 0.016674\n","Epoch: 424 \tTraining Loss: 0.009689 \tValidation Loss: 0.016088\n","Epoch: 425 \tTraining Loss: 0.009770 \tValidation Loss: 0.016686\n","Epoch: 426 \tTraining Loss: 0.009875 \tValidation Loss: 0.018122\n","Epoch: 427 \tTraining Loss: 0.009994 \tValidation Loss: 0.016039\n","Epoch: 428 \tTraining Loss: 0.009887 \tValidation Loss: 0.018636\n","Epoch: 429 \tTraining Loss: 0.009472 \tValidation Loss: 0.016559\n","Epoch: 430 \tTraining Loss: 0.009434 \tValidation Loss: 0.015961\n","Validation loss decreased (0.015964 --> 0.015961).  Saving model ...\n","Epoch: 431 \tTraining Loss: 0.009460 \tValidation Loss: 0.016636\n","Epoch: 432 \tTraining Loss: 0.009278 \tValidation Loss: 0.017552\n","Epoch: 433 \tTraining Loss: 0.009271 \tValidation Loss: 0.016617\n","Epoch: 434 \tTraining Loss: 0.009440 \tValidation Loss: 0.017128\n","Epoch: 435 \tTraining Loss: 0.009503 \tValidation Loss: 0.016524\n","Epoch: 436 \tTraining Loss: 0.009254 \tValidation Loss: 0.016550\n","Epoch: 437 \tTraining Loss: 0.009186 \tValidation Loss: 0.016542\n","Epoch: 438 \tTraining Loss: 0.009281 \tValidation Loss: 0.016625\n","Epoch: 439 \tTraining Loss: 0.009457 \tValidation Loss: 0.017301\n","Epoch: 440 \tTraining Loss: 0.009278 \tValidation Loss: 0.016446\n","Epoch: 441 \tTraining Loss: 0.009175 \tValidation Loss: 0.016692\n","Epoch: 442 \tTraining Loss: 0.009349 \tValidation Loss: 0.016151\n","Epoch: 443 \tTraining Loss: 0.009495 \tValidation Loss: 0.016502\n","Epoch: 444 \tTraining Loss: 0.009320 \tValidation Loss: 0.016419\n","Epoch: 445 \tTraining Loss: 0.009330 \tValidation Loss: 0.016835\n","Epoch: 446 \tTraining Loss: 0.009374 \tValidation Loss: 0.016992\n","Epoch: 447 \tTraining Loss: 0.009242 \tValidation Loss: 0.015992\n","Epoch: 448 \tTraining Loss: 0.008992 \tValidation Loss: 0.016636\n","Epoch: 449 \tTraining Loss: 0.009128 \tValidation Loss: 0.016381\n","Epoch: 450 \tTraining Loss: 0.009131 \tValidation Loss: 0.016450\n","Epoch: 451 \tTraining Loss: 0.009178 \tValidation Loss: 0.016110\n","Epoch: 452 \tTraining Loss: 0.009241 \tValidation Loss: 0.016313\n","Epoch: 453 \tTraining Loss: 0.009302 \tValidation Loss: 0.016523\n","Epoch: 454 \tTraining Loss: 0.009380 \tValidation Loss: 0.016479\n","Epoch: 455 \tTraining Loss: 0.009347 \tValidation Loss: 0.017144\n","Epoch: 456 \tTraining Loss: 0.009522 \tValidation Loss: 0.016006\n","Epoch: 457 \tTraining Loss: 0.009542 \tValidation Loss: 0.016439\n","Epoch: 458 \tTraining Loss: 0.009311 \tValidation Loss: 0.016380\n","Epoch: 459 \tTraining Loss: 0.009088 \tValidation Loss: 0.016827\n","Epoch: 460 \tTraining Loss: 0.009152 \tValidation Loss: 0.015972\n","Epoch: 461 \tTraining Loss: 0.009166 \tValidation Loss: 0.016489\n","Epoch: 462 \tTraining Loss: 0.009298 \tValidation Loss: 0.016669\n","Epoch: 463 \tTraining Loss: 0.009130 \tValidation Loss: 0.017228\n","Epoch: 464 \tTraining Loss: 0.009238 \tValidation Loss: 0.016228\n","Epoch: 465 \tTraining Loss: 0.009363 \tValidation Loss: 0.016351\n","Epoch: 466 \tTraining Loss: 0.008911 \tValidation Loss: 0.016123\n","Epoch: 467 \tTraining Loss: 0.008921 \tValidation Loss: 0.016645\n","Epoch: 468 \tTraining Loss: 0.009010 \tValidation Loss: 0.016134\n","Epoch: 469 \tTraining Loss: 0.009128 \tValidation Loss: 0.017302\n","Epoch: 470 \tTraining Loss: 0.008912 \tValidation Loss: 0.016335\n","Epoch: 471 \tTraining Loss: 0.009179 \tValidation Loss: 0.017176\n","Epoch: 472 \tTraining Loss: 0.009260 \tValidation Loss: 0.016555\n","Epoch: 473 \tTraining Loss: 0.009136 \tValidation Loss: 0.016124\n","Epoch: 474 \tTraining Loss: 0.009037 \tValidation Loss: 0.016792\n","Epoch: 475 \tTraining Loss: 0.009076 \tValidation Loss: 0.016820\n","Epoch: 476 \tTraining Loss: 0.008826 \tValidation Loss: 0.016261\n","Epoch: 477 \tTraining Loss: 0.008892 \tValidation Loss: 0.016761\n","Epoch: 478 \tTraining Loss: 0.008940 \tValidation Loss: 0.016307\n","Epoch: 479 \tTraining Loss: 0.009032 \tValidation Loss: 0.017234\n","Epoch: 480 \tTraining Loss: 0.009087 \tValidation Loss: 0.015958\n","Validation loss decreased (0.015961 --> 0.015958).  Saving model ...\n","Epoch: 481 \tTraining Loss: 0.009306 \tValidation Loss: 0.016451\n","Epoch: 482 \tTraining Loss: 0.009120 \tValidation Loss: 0.017318\n","Epoch: 483 \tTraining Loss: 0.008913 \tValidation Loss: 0.015915\n","Validation loss decreased (0.015958 --> 0.015915).  Saving model ...\n","Epoch: 484 \tTraining Loss: 0.009210 \tValidation Loss: 0.016509\n","Epoch: 485 \tTraining Loss: 0.008950 \tValidation Loss: 0.016453\n","Epoch: 486 \tTraining Loss: 0.008774 \tValidation Loss: 0.016608\n","Epoch: 487 \tTraining Loss: 0.009190 \tValidation Loss: 0.016152\n","Epoch: 488 \tTraining Loss: 0.008905 \tValidation Loss: 0.016589\n","Epoch: 489 \tTraining Loss: 0.008576 \tValidation Loss: 0.016665\n","Epoch: 490 \tTraining Loss: 0.009087 \tValidation Loss: 0.016299\n","Epoch: 491 \tTraining Loss: 0.008936 \tValidation Loss: 0.016438\n","Epoch: 492 \tTraining Loss: 0.008762 \tValidation Loss: 0.016385\n","Epoch: 493 \tTraining Loss: 0.008893 \tValidation Loss: 0.015947\n","Epoch: 494 \tTraining Loss: 0.008761 \tValidation Loss: 0.016187\n","Epoch: 495 \tTraining Loss: 0.009019 \tValidation Loss: 0.016519\n","Epoch: 496 \tTraining Loss: 0.008691 \tValidation Loss: 0.016735\n","Epoch: 497 \tTraining Loss: 0.008875 \tValidation Loss: 0.016745\n","Epoch: 498 \tTraining Loss: 0.008659 \tValidation Loss: 0.016187\n","Epoch: 499 \tTraining Loss: 0.008799 \tValidation Loss: 0.016974\n","Epoch: 500 \tTraining Loss: 0.008730 \tValidation Loss: 0.016182\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nB5hs1EbYbSt","colab_type":"text"},"source":["Delete the print(\"Error\":)line as usual"]},{"cell_type":"code","metadata":{"id":"7NeVr4nS72Rd","colab_type":"code","colab":{}},"source":["# Evaluate this one\n","model.load_state_dict(torch.load(ROOT_FOLDER+'model.pt'))\n","predictions = predict(test_loader, model)\n","np.save(ROOT_FOLDER+'s1155132173.npy', predictions)\n","\n","# baseline: 0.069\n","# print('Error: ', np.linalg.norm(predictions-KPT_S_TEST.reshape((predictions.shape[0],-1)), axis=1).mean())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OE_hdVogZDhr","colab_type":"text"},"source":["Delete the draw_points() line \n","\n","The outcome seems pretty good"]},{"cell_type":"code","metadata":{"id":"OUsfnGUIIfcG","colab_type":"code","outputId":"7744fe7e-a9e2-4959-89b7-7728360f8966","executionInfo":{"status":"ok","timestamp":1584945878273,"user_tz":-480,"elapsed":684,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["idx = np.random.randint(predictions.shape[0])\n","print(idx)\n","draw_points(IMG_S_TEST[idx,:,:], predictions[idx,:].reshape((-1,2)))\n","#draw_points(IMG_S_TEST[idx,:,:], KPT_S_TEST[idx,:,:])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["60\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4v4fJT+OsdRe540AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"_RpnrVDpIiGt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7f0fcf49-12df-4184-f21f-4a385b40de1a","executionInfo":{"status":"ok","timestamp":1584950901917,"user_tz":-480,"elapsed":17493,"user":{"displayName":"Xiantao He","photoUrl":"","userId":"07478423814679491930"}}},"source":["# delete its output!\n","# Export the notebook as pdf\n","!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc\n","!jupyter nbconvert --to PDF \"./gdrive/My Drive/Colab Notebooks/Assignment2 3.23.ipynb\""],"execution_count":41,"outputs":[{"output_type":"stream","text":["\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 64%\r\rReading package lists... 64%\r\rReading package lists... 65%\r\rReading package lists... 65%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 77%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n","\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n","\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n","pandoc is already the newest version (1.19.2.4~dfsg-1build4).\n","texlive is already the newest version (2017.20180305-1).\n","texlive-latex-extra is already the newest version (2017.20180305-2).\n","texlive-xetex is already the newest version (2017.20180305-1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","Requirement already satisfied: pypandoc in /usr/local/lib/python3.6/dist-packages (1.4)\n","Requirement already satisfied: pip>=8.1.0 in /usr/local/lib/python3.6/dist-packages (from pypandoc) (19.3.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pypandoc) (46.0.0)\n","Requirement already satisfied: wheel>=0.25.0 in /usr/local/lib/python3.6/dist-packages (from pypandoc) (0.34.2)\n","[NbConvertApp] Converting notebook ./gdrive/My Drive/Colab Notebooks/Assignment2 3.23.ipynb to PDF\n","[NbConvertApp] Support files will be in Assignment2 3.23_files/\n","[NbConvertApp] Making directory ./Assignment2 3.23_files\n","[NbConvertApp] Making directory ./Assignment2 3.23_files\n","[NbConvertApp] Making directory ./Assignment2 3.23_files\n","[NbConvertApp] Making directory ./Assignment2 3.23_files\n","[NbConvertApp] Making directory ./Assignment2 3.23_files\n","[NbConvertApp] Making directory ./Assignment2 3.23_files\n","[NbConvertApp] Making directory ./Assignment2 3.23_files\n","[NbConvertApp] Writing 321269 bytes to ./notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n","[NbConvertApp] CRITICAL | xelatex failed: [u'xelatex', u'./notebook.tex', '-quiet']\n","This is XeTeX, Version 3.14159265-2.6-0.99998 (TeX Live 2017/Debian) (preloaded format=xelatex)\n"," restricted \\write18 enabled.\n","entering extended mode\n","(./notebook.tex\n","LaTeX2e <2017-04-15>\n","Babel <3.18> and hyphenation patterns for 3 language(s) loaded.\n","(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\n","Document Class: article 2014/09/29 v1.4h Standard LaTeX document class\n","(/usr/share/texlive/texmf-dist/tex/latex/base/size11.clo))\n","(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcolorbox.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-lists.t\n","ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.def\n","(/usr/share/texlive/texmf-dist/tex/latex/ms/everyshi.sty))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex))\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/xetex.def)))\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.code.t\n","ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.def\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfmx.def\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-pdf.de\n","f))))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol.code.\n","tex)) (/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/color.cfg))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basic.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trigonomet\n","ric.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.random.cod\n","e.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comparison\n",".code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.round.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.integerari\n","thmetics.code.tex)))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathconstruct.\n","code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusage.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphicstate.c\n","ode.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransformation\n","s.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.code.t\n","ex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathprocessing\n",".code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.code.tex\n","\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal.code.\n","tex))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransparency.c\n","ode.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns.code.\n","tex)))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.code.tex\n",") (/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-0-65\n",".sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-1-18\n",".sty)) (/usr/share/texlive/texmf-dist/tex/latex/tools/verbatim.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/environ/environ.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/trimspaces/trimspaces.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/etoolbox/etoolbox.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcbbreakable.code.tex\n","Library (tcolorbox): 'tcbbreakable.code.tex' version '4.12'\n",")) (/usr/share/texlive/texmf-dist/tex/latex/float/float.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def)\n","(/usr/share/texmf/tex/latex/lm/t1lmr.fd))\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/mathpazo.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/caption.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/caption3.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjustbox.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex)))\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjcalc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/trimclip.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/collectbox/collectbox.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/tc-xetex.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/ifoddpage/ifoddpage.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/varwidth/varwidth.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/enumerate.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifvtex.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty\n","For additional information on amsmath, use the `?' option.\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/base/textcomp.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/base/ts1enc.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/upquote/upquote.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/eurosym/eurosym.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucs.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/ucs/data/uni-global.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/base/inputenc.sty\n","\n","Package inputenc Warning: inputenc package ignored with utf8 based engines.\n","\n",") (/usr/share/texlive/texmf-dist/tex/latex/fancyvrb/fancyvrb.sty\n","Style option: `fancyvrb' v2.7a, with DG/SPQR fixes, and firstline=lastline fix \n","<2008/02/07> (tvz))\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/grffile.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ltxcmds.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvsetkeys.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/infwarerr.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/etexcmds.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifluatex.sty))))\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdftexcmds.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-hyperref.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-generic.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/auxhook.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def)\n","(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/hyperref.cfg)\n","(/usr/share/texlive/texmf-dist/tex/latex/url/url.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hxetex.def\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/puenc.def)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/stringenc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/rerunfilecheck.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/longtable.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/booktabs/booktabs.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/enumitem/enumitem.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/ulem/ulem.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/mathrsfs.sty)\n","No file notebook.aux.\n","(/usr/share/texlive/texmf-dist/tex/latex/base/ts1cmr.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/t1ppl.fd)\n","ABD: EveryShipout initializing macros\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/ltcaption.sty)\n","*geometry* driver: auto-detecting\n","*geometry* detected driver: xetex\n","*geometry* verbose mode - [ preamble ] result:\n","* driver: xetex\n","* paper: <default>\n","* layout: <same size as paper>\n","* layoutoffset:(h,v)=(0.0pt,0.0pt)\n","* modes: \n","* h-part:(L,W,R)=(72.26999pt, 469.75502pt, 72.26999pt)\n","* v-part:(T,H,B)=(72.26999pt, 650.43001pt, 72.26999pt)\n","* \\paperwidth=614.295pt\n","* \\paperheight=794.96999pt\n","* \\textwidth=469.75502pt\n","* \\textheight=650.43001pt\n","* \\oddsidemargin=0.0pt\n","* \\evensidemargin=0.0pt\n","* \\topmargin=-37.0pt\n","* \\headheight=12.0pt\n","* \\headsep=25.0pt\n","* \\topskip=11.0pt\n","* \\footskip=30.0pt\n","* \\marginparwidth=59.0pt\n","* \\marginparsep=10.0pt\n","* \\columnsep=10.0pt\n","* \\skip\\footins=10.0pt plus 4.0pt minus 2.0pt\n","* \\hoffset=0.0pt\n","* \\voffset=0.0pt\n","* \\mag=1000\n","* \\@twocolumnfalse\n","* \\@twosidefalse\n","* \\@mparswitchfalse\n","* \\@reversemarginfalse\n","* (1in=72.27pt=25.4mm, 1cm=28.453pt)\n","\n","(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucsencs.def)\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/nameref.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/gettitlestring.sty))\n","\n","Package hyperref Warning: Rerun to get /PageLabels entry.\n","\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/ot1ppl.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/omlzplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/omszplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/omxzplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/ot1zplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/ursfs.fd)\n","\n","LaTeX Warning: No \\author given.\n","\n","(/usr/share/texmf/tex/latex/lm/t1lmtt.fd)\n","(/usr/share/texmf/tex/latex/lm/ts1lmtt.fd)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/se-ascii-print.def)\n","[1]\n","! Missing $ inserted.\n","<inserted text> \n","                $\n","l.440 ...ent2 3.23_files/Assignment2 3.23_1_0.png}\n","                                                  \n","? \n","! Emergency stop.\n","<inserted text> \n","                $\n","l.440 ...ent2 3.23_files/Assignment2 3.23_1_0.png}\n","                                                  \n","Output written on notebook.pdf (1 page).\n","Transcript written on notebook.log.\n","\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 14876 bytes to ./gdrive/My Drive/Colab Notebooks/Assignment2 3.23.pdf\n"],"name":"stdout"}]}]}